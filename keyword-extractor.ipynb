{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"try:\n    %load_ext autotime\nexcept:\n    !pip install ipython-autotime\n    %load_ext autotime\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom tqdm.notebook import tqdm\nimport re\nimport pickle\nimport copy\n\n# !pip install -U spacy\n# import spacy\n# spacy.__version__","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-09T11:22:59.723910Z","iopub.execute_input":"2022-09-09T11:22:59.724346Z","iopub.status.idle":"2022-09-09T11:22:59.739635Z","shell.execute_reply.started":"2022-09-09T11:22:59.724315Z","shell.execute_reply":"2022-09-09T11:22:59.737920Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"The autotime extension is already loaded. To reload it, use:\n  %reload_ext autotime\ntime: 2.56 ms (started: 2022-09-09 11:22:59 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install openpyxl\nimport openpyxl\ndef build_gold_dict():\n    # read from excel\n    read_file = pd.read_excel('../input/gold-dict/Terms malantin 1er juin 2022.xlsx',\n                              sheet_name='categories 1 juin 2022')\n    read_file.dropna(0, how='all', inplace=True)\n    read_file.dropna(1, how='all', inplace=True)\n\n    gold_dict = {}\n    for i in range(1, len(read_file)):\n        if read_file.iloc[i]['Main form'] is None:\n            continue\n        gold_dict[read_file.iloc[i]['Main form'].strip()] = []\n\n    for i in range(1, len(read_file)):\n        index = read_file.iloc[i]['Main form'].strip()\n        if index is None:\n            continue\n        if not read_file.iloc[i].isna()['Sustainability preoccupations']:\n            gold_dict[index].append('Sustainability preoccupations')\n        if not read_file.iloc[i].isna()['Digital transformation']:\n            gold_dict[index].append('Digital transformation')\n        if not read_file.iloc[i].isna()['Change in management']:\n            gold_dict[index].append('Change in management')\n        if not read_file.iloc[i].isna()['Innovation activities']:\n            gold_dict[index].append('Innovation activities')\n        if not read_file.iloc[i].isna()['Business Model']:\n            gold_dict[index].append('Business Model')\n        if not read_file.iloc[i].isna()['Corporate social responsibility ou CSR']:\n            gold_dict[index].append('Corporate social responsibility ou CSR') \n\n    # Change the category for four keywords\n    # academic institutions, university & research institutions, service among university, \n    # worldwide research centres\n    category = 'Innovation activities'\n    changelist = ['academic institutions', 'worldwide research centers', 'university and research institutions', 'customer service among university']\n    for c in changelist:\n        gold_dict[c][0] = category\n\n    # Deal with the singular and plural cases in keywords\n    cortext3 = open(\"../input/cortext/Cortext3_min_delac_flex_utf8.txt\", \"r\")\n    lines = cortext3.readlines()\n    lefts, rights = [], []\n    for line in lines:\n        left, right = line.split(',')\n        if left != right.split('.')[0]:\n            lefts.append(left)\n            rights.append(right.split('.')[0])\n\n    tmp_gold_dict = gold_dict.copy()\n    for key, value in tmp_gold_dict.items():\n        if key in lefts:\n            index = lefts.index(key)\n            right = rights[index]\n            gold_dict[right] = gold_dict[key]\n        if key in rights:\n            indices = [i for i, word in enumerate(rights) if word == key]\n            for index in indices:\n                left = lefts[index]\n                gold_dict[left] = gold_dict[key]\n\n    tmp_dict = {}\n    for k in gold_dict.keys():\n        tmp_dict[k] = []\n    for k, v in gold_dict.items():\n        tmp_dict[k] = list(set(v))\n    gold_dict = tmp_dict\n\n    return gold_dict","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:00.067168Z","iopub.execute_input":"2022-09-09T11:23:00.067607Z","iopub.status.idle":"2022-09-09T11:23:11.897942Z","shell.execute_reply.started":"2022-09-09T11:23:00.067575Z","shell.execute_reply":"2022-09-09T11:23:11.895946Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.7/site-packages (3.0.10)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mtime: 11.8 s (started: 2022-09-09 11:23:00 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings  \nwarnings.filterwarnings('ignore')\nfrom sklearn.decomposition import PCA\n\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\n\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\nstops = set(stopwords.words('english'))\n\nfrom sklearn.metrics import accuracy_score\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n!pip install transformers\nfrom transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\nfrom transformers import RobertaForTokenClassification, RobertaTokenizerFast\nfrom transformers import DistilBertForTokenClassification, DistilBertTokenizerFast\nfrom transformers import AlbertForTokenClassification, AlbertTokenizerFast\n\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n\n# for TPU\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n\n# # for TPU\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:11.902504Z","iopub.execute_input":"2022-09-09T11:23:11.902874Z","iopub.status.idle":"2022-09-09T11:23:36.362847Z","shell.execute_reply.started":"2022-09-09T11:23:11.902840Z","shell.execute_reply":"2022-09-09T11:23:36.360953Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.53)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.7/site-packages (2.2.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.18.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.96)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.7.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.11.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.53)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (8.0.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mcuda\ntime: 24.4 s (started: 2022-09-09 11:23:11 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Test on Generalization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntag_dict = {}\ntag_dict['Sustainability preoccupations'] = 'I-sus'\ntag_dict['Digital transformation'] = 'I-dig'\ntag_dict['Change in management'] = 'I-mag'\ntag_dict['Innovation activities'] = 'I-inn'\ntag_dict['Business Model'] = 'I-bus'\ntag_dict['Corporate social responsibility ou CSR'] = 'I-cor'\ntag_dict['marco-label'] = 'I-mar'\ntag2cat = {v: k for k, v in tag_dict.items()}\n\nlabels_to_ids2 = {'O':0, 'I-sus':1, 'I-dig':2, 'I-mag':3, 'I-inn':4, 'I-bus':5, 'I-cor':6, 'I-mar':7}\nids_to_labels2 = {v: k for k, v in labels_to_ids2.items()}\n\ngold_dict = build_gold_dict()","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:36.365739Z","iopub.execute_input":"2022-09-09T11:23:36.366270Z","iopub.status.idle":"2022-09-09T11:23:37.200271Z","shell.execute_reply.started":"2022-09-09T11:23:36.366206Z","shell.execute_reply":"2022-09-09T11:23:37.198538Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"time: 826 ms (started: 2022-09-09 11:23:36 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Stored the semantic scores\ndef DatasetDistribution_sem_score(datasetdf):\n    TAGS2, TEXTS2 = [], []\n    for m in range(len(datasetdf)):\n        ts = datasetdf['Text_para'].iloc[m].split()\n        ks = datasetdf['Keyword'].iloc[m].split(',')\n        # deal with the special case\n        texts = []\n        for t in ts:\n            if t == '4.0' or t == 'R' or t == 'D' or t == '&':\n                texts.append(t)\n            else:\n                # text = re.findall(r\"([\\w']+(?:\\S-\\S)?[\\w'])+\", t)\n                text = re.findall(r\"([\\w']+[-]?[\\w']+)+\", t)\n                texts.extend(text)\n        # deal with the special case\n        keywords = []\n        for k in ks:\n            if k == 'Maintenance 4.0' or k == 'R & D' or k == 'R & D teams' or k == 'revolution 4.0':\n                keywords.append(k.split())\n            else:\n                keyword = re.findall(r\"([\\w']+[-]?[\\w']+)+\", k)\n                keywords.append(keyword)\n\n        doc_embedding = model.encode([' '.join(texts)])\n        scores = []\n        for key in keywords:\n            candidate_embeddings = [gold_emb_dict[' '.join(key)]]\n            distances = cosine_similarity(doc_embedding, candidate_embeddings)\n            scores.append([' '.join(key), distances[0][0]])\n        \n        TEXTS2.append(texts)\n        TAGS2.append(scores)\n#         print(texts)\n#         print(TAGS2)\n    \n    print(len(TAGS2), len(TEXTS2))\n#     datasetdf['similarity_score'] = TAGS2\n#     datasetdf['sentence'] = [' '.join(tx) for tx in TEXTS2]\n\n    return TAGS2, TEXTS2","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:37.204880Z","iopub.execute_input":"2022-09-09T11:23:37.205458Z","iopub.status.idle":"2022-09-09T11:23:37.220722Z","shell.execute_reply.started":"2022-09-09T11:23:37.205418Z","shell.execute_reply":"2022-09-09T11:23:37.219003Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"time: 5.5 ms (started: 2022-09-09 11:23:37 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import ast\n# Based on Sematic choice\ndef DatasetBuilder_tr_sem(datasetdf):\n    TAGS2, TEXTS2, KWS2 = [], [], []\n    for m in range(len(datasetdf)):\n        texts = ast.literal_eval(datasetdf['sentences'].iloc[m])\n        ks = datasetdf['keywords_0.1'].iloc[m]\n\n        # deal with the special case\n        keywords = []\n        for k in ks:\n            keywords.append(k.split())\n            \n        tags = ['O'] * len(texts)\n        # filter the keywords\n#         if len(keywords) > 2:\n#             doc_embedding = model.encode([' '.join(texts)])\n            \n#             candidate_embeddings = [gold_emb_dict[' '.join(key)] for key in keywords]\n#             top_n = 2\n#             distances = cosine_similarity(doc_embedding, candidate_embeddings)\n#             keywords = [keywords[index] for index in distances.argsort()[0][-top_n:]]\n        \n        # cases: for each of the keyword in each block\n        # 1. only one keyword in the block, and no marco lable overlapping\n        # 2. more keyword in the block, and no marco label overlapping\n        # 3. only one keyword in the block, but there's marco label overlapping\n        # 4. more keyword in the block, and maroc label overlapping exists\n        for kw in keywords:\n            tt = tag_dict[gold_dict[' '.join(kw)][0]]\n            length_kw = len(kw)\n            item = kw[0]\n\n            if length_kw == 1:\n                start = 0\n                end = len(texts)\n                while True:\n                    try: \n                        index = texts.index(item, start, end)\n                        tags[index] = tt\n                        start = index+1\n                    except:\n                        break\n            \n            if length_kw == 2:\n                start = 0\n                end = len(texts)\n                while True:\n                    try: \n                        index = texts.index(item, start, end)\n                        start = index+1\n                        if texts[index+1] == kw[1]:\n                            if (tags[index] != 'O' and tags[index] != tt) or (tags[index+1] != 'O' and tags[index+1] != tt):\n                                tt = 'I-mar'\n                            for i in range(index, index+length_kw):\n                                tags[i] = tt\n                    except:\n                        break\n\n            if length_kw >= 3:\n                start = 0\n                end = len(texts)\n                while True:\n                    try: \n                        index = texts.index(item, start, end)\n                        start = index+1\n                        if texts[index+1] == kw[1] and texts[index+2] == kw[2]:\n                            if (tags[index] != 'O' and tags[index] != tt) or (tags[index+1] != 'O' and tags[index+1] != tt) or (tags[index+2] != 'O' and tags[index+2] != tt):\n                                tt = 'I-mar'\n                            for i in range(index, index+length_kw):\n                                tags[i] = tt\n                    except:\n                        break\n        \n        KWS2.append(ks)\n        TAGS2.append(tags)\n        TEXTS2.append(texts)\n    \n    print(len(TAGS2), len(TEXTS2))\n    datasetdf['word_labels2'] = [','.join(bt) for bt in TAGS2]\n    datasetdf['sentence'] = [' '.join(tx) for tx in TEXTS2]\n\n    return datasetdf[[\"sentence\", \"word_labels2\", \"keywords_0.1\"]].drop_duplicates(subset=[\"sentence\", \"word_labels2\"]).reset_index(drop=True)\n#     data2 = datasetdf.drop_duplicates(subset=[\"sentence\", \"word_labels2\"])\n    \n#     data2 = data2[data2['keywords_0.1'].map(len) != 0].reset_index(drop=True)\n#     train_indices = []\n#     for i in range(len(data2)):\n#         kws = data2['keywords_0.1'].iloc[i]\n#         for kw in kws:\n#             if kw in train_terms:\n#                 train_indices.append(i)\n#                 break\n    \n#     return data2[[\"sentence\", \"word_labels2\"]], train_indices","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:37.224774Z","iopub.execute_input":"2022-09-09T11:23:37.225863Z","iopub.status.idle":"2022-09-09T11:23:37.247571Z","shell.execute_reply.started":"2022-09-09T11:23:37.225821Z","shell.execute_reply":"2022-09-09T11:23:37.245479Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"time: 3.63 ms (started: 2022-09-09 11:23:37 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# datasetdf = pd.read_csv(\"../input/annual-report/dataset_ap_07_18.csv\")\n# # datasetdf = datasetdf.drop_duplicates(subset=['Company', 'Sector', 'Text_para', 'Nb_company', 'Text_block', 'Catogory', 'Keyword'], keep='first', ignore_index=True)\n# datasetdf = datasetdf.drop_duplicates(subset=['Company', 'Year', 'Text_para', 'Text_block', 'Catogory', 'Keyword'], keep='first', ignore_index=True)\n# datasetdf.drop(datasetdf.columns[datasetdf.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n\n# def remove_back_slash_n(text):\n#     return text.replace('\\n', ' ')\n# datasetdf['Text_para'] = datasetdf['Text_para'].apply(remove_back_slash_n)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:37.249967Z","iopub.execute_input":"2022-09-09T11:23:37.251026Z","iopub.status.idle":"2022-09-09T11:23:37.267114Z","shell.execute_reply.started":"2022-09-09T11:23:37.250982Z","shell.execute_reply":"2022-09-09T11:23:37.265309Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"time: 9.75 ms (started: 2022-09-09 11:23:37 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nwith open(\"../input/dataset-semantic/dataset_semantic_07_22.pkl\", \"rb\") as f:\n    datasetdf = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:37.269844Z","iopub.execute_input":"2022-09-09T11:23:37.270471Z","iopub.status.idle":"2022-09-09T11:23:38.185756Z","shell.execute_reply.started":"2022-09-09T11:23:37.270427Z","shell.execute_reply":"2022-09-09T11:23:38.184372Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"time: 908 ms (started: 2022-09-09 11:23:37 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# from sklearn.metrics.pairwise import cosine_similarity\n# datasetdf = datasetdf[datasetdf['keywords_0.1'].map(len) != 0]\n# data2, train_indices = DatasetBuilder_tr_sem(datasetdf)\ndata2 = DatasetBuilder_tr_sem(datasetdf)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:38.188851Z","iopub.execute_input":"2022-09-09T11:23:38.189768Z","iopub.status.idle":"2022-09-09T11:23:51.561681Z","shell.execute_reply.started":"2022-09-09T11:23:38.189709Z","shell.execute_reply":"2022-09-09T11:23:51.560134Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"33166 33166\ntime: 13.4 s (started: 2022-09-09 11:23:38 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## There are three ways to divide the dataset\n\n- 75% terms into training + 25% terms into testing -> on web\n- Reduction: training is 70% of blocks plus 75% of terms and testing is 30% of blocks plus all terms\n- Annotate blocks: missing label, negative labels, ...","metadata":{}},{"cell_type":"code","source":"# Method 1\nkeywords = set(kw for kws in datasetdf['keywords_0.1'].values.tolist() for kw in kws)\nimport random\nrandom.seed(40)\n\ntrain_terms = random.sample(keywords, int(len(keywords)*0.75))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T18:57:57.512234Z","iopub.execute_input":"2022-09-05T18:57:57.512668Z","iopub.status.idle":"2022-09-05T18:57:57.529521Z","shell.execute_reply.started":"2022-09-05T18:57:57.512630Z","shell.execute_reply":"2022-09-05T18:57:57.528432Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"time: 11.5 ms (started: 2022-09-05 18:57:57 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Method 2\ndatasetdf = data2\nkeywords = set(kw for kws in datasetdf['keywords_0.1'].values.tolist() for kw in kws)\n\nkeywords2blocks = {kw:[] for kw in keywords}\nblocks2keywords = {i:datasetdf['keywords_0.1'].iloc[i] for i in range(len(datasetdf))}\n\n# Update keywords2blocks\nfor k in keywords2blocks.keys():\n    for kb, vb in blocks2keywords.items():\n        if k in vb:\n            keywords2blocks[k].append(kb)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:02:57.112872Z","iopub.execute_input":"2022-09-06T17:02:57.113173Z","iopub.status.idle":"2022-09-06T17:02:59.052887Z","shell.execute_reply.started":"2022-09-06T17:02:57.113145Z","shell.execute_reply":"2022-09-06T17:02:59.051828Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"time: 1.93 s (started: 2022-09-06 17:02:57 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"np.random.seed(422)\nkeyword_list, block_list = [], []\nfor k,v in keywords2blocks.items():\n    keyword_list.extend([k]*len(v))\n    block_list.extend(v)\n\nreductiondf = pd.DataFrame({'keyword':keyword_list, 'block': block_list})\n\nsample_size = int(len(datasetdf) * 0.2)\nsample = reductiondf.groupby('keyword').sample(1, random_state=1)\n\nsample = sample.append(\n    reductiondf[~reductiondf.index.isin(sample.index)] # only rows that have not been selected\n    .sample(n=sample_size-sample.shape[0]) # sample more rows as needed\n).sort_index()\n\ntest_indices = set(sample['block'].values.tolist())\n\n# the blocks remained\nb2k_remained = {}\nfor k,v in blocks2keywords.items():\n    if k not in test_indices:\n        b2k_remained[k] = v\n\nprint(f\"length of testing set: {len(test_indices)}\")\n\ntrain_size = int(len(test_indices) * (7/3))\n\n# build training set\ntrain_indices = np.random.choice(list(b2k_remained.keys()), size=train_size, replace=False)\ntrain_dict = {}\nfor k,v in b2k_remained.items():\n    if k in train_indices:\n        train_dict[k] = v\nprint(f\"length of training set: {len(train_dict)}, {train_size}\")\nprint(f\"the proportion of training: {len(train_dict)/(len(test_indices)+len(train_dict))}\")\nprint(f\"the proportaion of training terms: {len(set([v for vs in train_dict.values() for v in vs])) / len(keywords):.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:02:59.054415Z","iopub.execute_input":"2022-09-06T17:02:59.054784Z","iopub.status.idle":"2022-09-06T17:02:59.409616Z","shell.execute_reply.started":"2022-09-06T17:02:59.054747Z","shell.execute_reply":"2022-09-06T17:02:59.408559Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"length of testing set: 6096\nlength of training set: 14224, 14224\nthe proportion of training: 0.7\nthe proportaion of training terms: 0.861\ntime: 345 ms (started: 2022-09-06 17:02:59 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# firstly guarantee the proportion of terms\n# np.random.seed(422)\n\n# train_terms = np.random.choice(list(keywords), int(len(keywords)*0.8))\n# print(f\"train terms: {len(train_terms)}\")\n# block_list = []\n# for k,v in keywords2blocks.items():\n#     if k in train_terms:\n#         block_list.extend(v)\n# train_indices = list(set(block_list))\n# train_indices = np.random.choice(train_indices, int(len(train_indices)*0.8))\n\n# train_terms_final = []\n# for k,v in blocks2keywords.items():\n#     if k in train_indices:\n#         train_terms_final.extend(v)\n# train_terms_final = list(set(train_terms_final))\n# print(f\"train terms final: {len(train_terms_final)}\")\n\n# train_size = len(train_indices)\n# test_size = int(train_size * (3/7))\n# print(f\"train size: {train_size}\")\n# b2k_remained = {}\n# for k,v in blocks2keywords.items():\n#     if k not in train_indices:\n#         b2k_remained[k] = v\n\n# keyword_list, block_list = [], []\n# for k,v in keywords2blocks.items():\n#     keyword_list.extend([k]*len(v))\n#     block_list.extend(v)\n\n# reductiondf = pd.DataFrame({'keyword':keyword_list, 'block': block_list})\n\n# keyword_list, block_list = [], []\n# for i in range(len(reductiondf)):\n#     block = reductiondf['block'].iloc[i]\n#     keyword = reductiondf['keyword'].iloc[i]\n#     if block not in list(train_indices):\n#         keyword_list.extend(keyword)\n#         block_list.append(block)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:07:38.730680Z","iopub.execute_input":"2022-09-06T15:07:38.731092Z","iopub.status.idle":"2022-09-06T15:07:39.086909Z","shell.execute_reply.started":"2022-09-06T15:07:38.731064Z","shell.execute_reply":"2022-09-06T15:07:39.085092Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"train terms: 298\ntrain terms final: 336\ntrain size: 14937\ntime: 349 ms (started: 2022-09-06 15:07:38 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# with open(\"semantic_dataset_training_07_28.pkl\", \"wb\") as f:\n#     pickle.dump(data2, f)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T07:25:34.258249Z","iopub.execute_input":"2022-09-05T07:25:34.258537Z","iopub.status.idle":"2022-09-05T07:25:34.264399Z","shell.execute_reply.started":"2022-09-05T07:25:34.258511Z","shell.execute_reply":"2022-09-05T07:25:34.262850Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"time: 323 µs (started: 2022-09-05 07:25:34 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"class dataset2(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n  def __getitem__(self, index):\n        # step 1: get the sentence and word labels \n        sentence = self.data.sentence[index].strip().split()  \n        word_labels = self.data.word_labels2[index].split(\",\") \n\n        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n        encoding = self.tokenizer(sentence,\n                             is_split_into_words=True, \n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        # step 3: create token labels only for first word pieces of each tokenized word\n        labels = [labels_to_ids2[label] for label in word_labels] \n        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n        # create an empty array of -100 of length max_length\n        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n        \n        # set only labels whose first offset position is 0 and the second is not 0\n        i = 0\n        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n          if mapping[0] == 0 and mapping[1] != 0:\n            # overwrite label\n            encoded_labels[idx] = labels[i]\n            i += 1\n\n        # step 4: turn everything into PyTorch tensors\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        item['labels'] = torch.as_tensor(encoded_labels)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:51.568085Z","iopub.execute_input":"2022-09-09T11:23:51.568484Z","iopub.status.idle":"2022-09-09T11:23:51.584488Z","shell.execute_reply.started":"2022-09-09T11:23:51.568455Z","shell.execute_reply":"2022-09-09T11:23:51.582438Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"time: 2.99 ms (started: 2022-09-09 11:23:51 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Defining the training function on the 80% of the dataset for tuning the bert model\ndef train2(epoch, training_loader2):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels, tr_probs = [], [], []\n    # put model in training mode\n    model2.train()\n    \n    for idx, batch in enumerate(training_loader2):\n        ids = batch['input_ids'].to(device, dtype = torch.long)\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\n        labels = batch['labels'].to(device, dtype = torch.long)\n\n        results = model2(input_ids=ids, attention_mask=mask, labels=labels)\n        loss = results.loss\n        tr_logits = results.logits\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 100==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss per 100 training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model2.num_labels) # shape (batch_size * seq_len, num_labels)\n#         flattened_probabilities = F.softmax(active_logits, dim=1) # probabilities\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n#         probabilities = []\n#         for i, act in enumerate(active_accuracy):\n#             if act:\n#                 probabilities.append(flattened_probabilities[i])\n        \n        tr_labels.extend(labels)\n        tr_preds.extend(predictions)\n#         tr_probs.extend(probabilities)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model2.parameters(), max_norm=MAX_GRAD_NORM\n        )\n        \n        # backward pass\n        optimizer2.zero_grad()\n        loss.backward()\n        optimizer2.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")\n\n    return epoch_loss, tr_accuracy#, tr_labels, tr_preds, tr_probs","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:51.589565Z","iopub.execute_input":"2022-09-09T11:23:51.590311Z","iopub.status.idle":"2022-09-09T11:23:51.609921Z","shell.execute_reply.started":"2022-09-09T11:23:51.590278Z","shell.execute_reply":"2022-09-09T11:23:51.607837Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"time: 2.99 ms (started: 2022-09-09 11:23:51 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels, eval_probs = [], [], []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(testing_loader):\n            \n            ids = batch['input_ids'].to(device, dtype = torch.long)\n            mask = batch['attention_mask'].to(device, dtype = torch.long)\n            labels = batch['labels'].to(device, dtype = torch.long)\n            \n            results = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = results.loss\n            eval_logits = results.logits\n\n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += labels.size(0)\n        \n            # if idx % 100==0:\n            #     loss_step = eval_loss/nb_eval_steps\n            #     print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n              \n            # compute evaluation accuracy\n            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n            flattened_probabilities = F.softmax(active_logits, dim=1) # probabilities\n            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n            \n            # only compute accuracy at active labels\n            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        \n            labels = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            probabilities = []\n            for i, act in enumerate(active_accuracy):\n                if act:\n                    probabilities.append(flattened_probabilities[i])\n            \n            eval_labels.extend(labels)\n            eval_preds.extend(predictions)\n            eval_probs.extend(probabilities)\n            \n            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n            eval_accuracy += tmp_eval_accuracy\n\n    labels = [ids_to_labels2[id.item()] for id in eval_labels]\n    predictions = [ids_to_labels2[id.item()] for id in eval_preds]\n    \n    eval_loss = eval_loss / nb_eval_steps\n    eval_accuracy = eval_accuracy / nb_eval_steps\n    print(f\"Validation Loss: {eval_loss}\")\n    print(f\"Validation Accuracy: {eval_accuracy}\")\n\n    return labels, predictions, eval_loss, eval_accuracy, eval_probs","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:23:51.614035Z","iopub.execute_input":"2022-09-09T11:23:51.615855Z","iopub.status.idle":"2022-09-09T11:23:51.635650Z","shell.execute_reply.started":"2022-09-09T11:23:51.615810Z","shell.execute_reply":"2022-09-09T11:23:51.633913Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"time: 2.79 ms (started: 2022-09-09 11:23:51 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\nMAX_LEN = 300\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 10\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\n# tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n# tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v1', add_prefix_space=True)\n\n# K-fold\nk_fold = 5\ncur_fold = 1\ngap = int(len(data2) / k_fold)\nindices = [[i*gap, (i+1)*gap if i+1 != k_fold else len(data2)] \n          for i in range(k_fold)]\ndata3 = data2.sample(frac=1, random_state=200)\ntest_dataset2 = data3[indices[cur_fold][0]:indices[cur_fold][1]]\ntrain_dataset2 = data3.drop(test_dataset2.index).reset_index(drop=True)\ntest_dataset2 = test_dataset2.reset_index(drop=True)\n\n# New dividing method\n# Method 1\n# train_dataset2 = data2.iloc[train_indices]\n# test_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\n# train_dataset2 = train_dataset2.reset_index(drop=True)\n# Method 2\n# train_dataset2 = data2.iloc[train_indices].reset_index(drop=True)\n# test_dataset2 = data2.iloc[list(test_indices)].reset_index(drop=True)\n\n# train_size = 0.8\n# train_dataset2 = data2.sample(frac=train_size,random_state=202)\n# test_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\n# train_dataset2 = train_dataset2.reset_index(drop=True)\n\nprint(\"TRAIN Dataset: {}\".format(train_dataset2.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset2.shape))\n\ntraining_set2 = dataset2(train_dataset2, tokenizer, MAX_LEN)\ntesting_set2 = dataset2(test_dataset2, tokenizer, MAX_LEN)\n\ntrain_params2 = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params2 = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n#                 'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader2 = DataLoader(training_set2, **train_params2)\ntesting_loader2 = DataLoader(testing_set2, **test_params2)\n\n# model2 = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(labels_to_ids2))\n# model2 = BertForTokenClassification.from_pretrained('bert-large-cased', num_labels=len(labels_to_ids2))\n# model2 = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(labels_to_ids2))\nmodel2 = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(labels_to_ids2))\n# model2 = AlbertForTokenClassification.from_pretrained('albert-base-v1', num_labels=len(labels_to_ids2))\nmodel2.to(device)\n\noptimizer2 = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n\ntrLosslist, trAcclist, evalLosslist, evalAcclist, timelist = [], [], [], [], []\nlabel_pred_dict = {}\nfor epoch in range(EPOCHS):\n    print(f\"Training epoch: {epoch + 1}\")\n    tmp_dict = {}\n    start = time.time()\n    epoch_loss, tr_accuracy = train2(epoch, training_loader2)\n    labels, predictions, eval_loss, eval_accuracy, eval_probs = valid(model2, testing_loader2)\n    epoch_time = time.time() - start\n    trLosslist.append(epoch_loss)\n    trAcclist.append(tr_accuracy)\n    evalLosslist.append(eval_loss)\n    evalAcclist.append(eval_accuracy)\n    timelist.append(epoch_time)\n#     tmp_dict[\"predictions\"] = predictions\n#     tmp_dict[\"labels\"] = labels\n#     tmp_dict[\"probabilities\"] = eval_probs\n#     label_pred_dict[epoch] = tmp_dict\n\nresultdf = pd.DataFrame({\"Epoch\": list(range(1, EPOCHS+1)),\n                         \"Train_loss\": trLosslist,\n                         \"Eval_loss\": evalLosslist,\n                         \"Train_Acc\": trAcclist,\n                         \"Eval_Acc\": evalAcclist,\n                        \"Time\": timelist})\nresultdf    ","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:24:57.031348Z","iopub.execute_input":"2022-09-09T11:24:57.031766Z","iopub.status.idle":"2022-09-09T13:39:05.050959Z","shell.execute_reply.started":"2022-09-09T11:24:57.031735Z","shell.execute_reply":"2022-09-09T13:39:05.046747Z"},"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca77223adf984bd99b8d456b4685753a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522d8619c2744e1da6b371e607f8c9d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f480cf4ef236437aaeb269818412b9e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0373c159afb47bd91505eb1447c5bcf"}},"metadata":{}},{"name":"stdout","text":"TRAIN Dataset: (26362, 3)\nTEST Dataset: (6590, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33740541cfa94881a39bac954093fa29"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training epoch: 1\nTraining loss per 100 training steps: 2.112478494644165\nTraining loss per 100 training steps: 0.26645716170937117\nTraining loss per 100 training steps: 0.1907427158132566\nTraining loss per 100 training steps: 0.15235028500077732\nTraining loss per 100 training steps: 0.1269413362305955\nTraining loss per 100 training steps: 0.10919168517606968\nTraining loss per 100 training steps: 0.09581460379538267\nTraining loss per 100 training steps: 0.08616914597410343\nTraining loss per 100 training steps: 0.0788365905879463\nTraining loss per 100 training steps: 0.07276395568494932\nTraining loss per 100 training steps: 0.06737161724147601\nTraining loss per 100 training steps: 0.06324905563147144\nTraining loss per 100 training steps: 0.059688685438324306\nTraining loss per 100 training steps: 0.056565921461309156\nTraining loss per 100 training steps: 0.05388126501969416\nTraining loss per 100 training steps: 0.05174717426166338\nTraining loss per 100 training steps: 0.04965943842493684\nTraining loss per 100 training steps: 0.04778874891095339\nTraining loss per 100 training steps: 0.04603854941610384\nTraining loss per 100 training steps: 0.044422216815697614\nTraining loss per 100 training steps: 0.04302673518058678\nTraining loss per 100 training steps: 0.04160918475414402\nTraining loss per 100 training steps: 0.040499411968587105\nTraining loss per 100 training steps: 0.03937261810150647\nTraining loss per 100 training steps: 0.03838023259093064\nTraining loss per 100 training steps: 0.037584077397010245\nTraining loss per 100 training steps: 0.03670311541315698\nTraining loss per 100 training steps: 0.03590285536995684\nTraining loss per 100 training steps: 0.035093532819466304\nTraining loss per 100 training steps: 0.03430939435192632\nTraining loss per 100 training steps: 0.03358845321234003\nTraining loss per 100 training steps: 0.03292991189399563\nTraining loss per 100 training steps: 0.03236523833795663\nTraining loss per 100 training steps: 0.031759925442987585\nTraining loss per 100 training steps: 0.031204667231120045\nTraining loss per 100 training steps: 0.030687893400199114\nTraining loss per 100 training steps: 0.030244555667354896\nTraining loss per 100 training steps: 0.029751407145698683\nTraining loss per 100 training steps: 0.02934147508163054\nTraining loss per 100 training steps: 0.028926540042890365\nTraining loss per 100 training steps: 0.02854333416010274\nTraining loss per 100 training steps: 0.028152549973545042\nTraining loss per 100 training steps: 0.02777860090114913\nTraining loss per 100 training steps: 0.027358758986318128\nTraining loss per 100 training steps: 0.027027366239379354\nTraining loss per 100 training steps: 0.026741129715146462\nTraining loss per 100 training steps: 0.02644199099059937\nTraining loss per 100 training steps: 0.026143957402288098\nTraining loss per 100 training steps: 0.02584862001782927\nTraining loss per 100 training steps: 0.025555616407301255\nTraining loss per 100 training steps: 0.025261909280615173\nTraining loss per 100 training steps: 0.025002900149958592\nTraining loss per 100 training steps: 0.024749595951817183\nTraining loss per 100 training steps: 0.024511803082660245\nTraining loss per 100 training steps: 0.024238143383973728\nTraining loss per 100 training steps: 0.024007819017658725\nTraining loss per 100 training steps: 0.023755527310271\nTraining loss per 100 training steps: 0.023516198186404526\nTraining loss per 100 training steps: 0.02330092112258012\nTraining loss per 100 training steps: 0.023102971335428022\nTraining loss per 100 training steps: 0.022888677176100897\nTraining loss per 100 training steps: 0.022701636170961926\nTraining loss per 100 training steps: 0.022507785453469445\nTraining loss per 100 training steps: 0.022328654923341035\nTraining loss per 100 training steps: 0.022174883468186146\nTraining loss per 100 training steps: 0.0220085741882841\nTraining loss epoch: 0.021847956884377226\nTraining accuracy epoch: 0.9929274818411449\nValidation Loss: 0.011169996460139567\nValidation Accuracy: 0.9955599976222205\nTraining epoch: 2\nTraining loss per 100 training steps: 0.0014990158379077911\nTraining loss per 100 training steps: 0.009084698451639126\nTraining loss per 100 training steps: 0.008888108135561287\nTraining loss per 100 training steps: 0.009265479458549348\nTraining loss per 100 training steps: 0.009490496961256499\nTraining loss per 100 training steps: 0.009326580506512437\nTraining loss per 100 training steps: 0.009228898740153505\nTraining loss per 100 training steps: 0.009393466569695186\nTraining loss per 100 training steps: 0.009452855371768074\nTraining loss per 100 training steps: 0.009385523997803067\nTraining loss per 100 training steps: 0.009380219587725336\nTraining loss per 100 training steps: 0.009325974986764863\nTraining loss per 100 training steps: 0.009366849062992528\nTraining loss per 100 training steps: 0.009294217073352422\nTraining loss per 100 training steps: 0.00920095732968635\nTraining loss per 100 training steps: 0.00916538861727787\nTraining loss per 100 training steps: 0.009102368322508655\nTraining loss per 100 training steps: 0.009055337225386614\nTraining loss per 100 training steps: 0.009045516339346983\nTraining loss per 100 training steps: 0.008997765987538176\nTraining loss per 100 training steps: 0.008945440036328583\nTraining loss per 100 training steps: 0.008995520996133412\nTraining loss per 100 training steps: 0.008971477736614384\nTraining loss per 100 training steps: 0.009063851296192535\nTraining loss per 100 training steps: 0.009056124710827231\nTraining loss per 100 training steps: 0.00904569348390256\nTraining loss per 100 training steps: 0.009074895130258914\nTraining loss per 100 training steps: 0.009035236897822275\nTraining loss per 100 training steps: 0.009001643445661104\nTraining loss per 100 training steps: 0.008953197155157744\nTraining loss per 100 training steps: 0.00895589921727315\nTraining loss per 100 training steps: 0.00899334847274064\nTraining loss per 100 training steps: 0.008971154106357486\nTraining loss per 100 training steps: 0.008943351481769134\nTraining loss per 100 training steps: 0.008925426472377202\nTraining loss per 100 training steps: 0.008879020628396656\nTraining loss per 100 training steps: 0.008859399060409784\nTraining loss per 100 training steps: 0.00884765068989508\nTraining loss per 100 training steps: 0.008825665087969527\nTraining loss per 100 training steps: 0.008781921565619525\nTraining loss per 100 training steps: 0.00878720340872598\nTraining loss per 100 training steps: 0.00877652340011506\nTraining loss per 100 training steps: 0.008805401295098342\nTraining loss per 100 training steps: 0.00878382834198622\nTraining loss per 100 training steps: 0.008784591583869164\nTraining loss per 100 training steps: 0.00881471824089114\nTraining loss per 100 training steps: 0.0088126037972352\nTraining loss per 100 training steps: 0.00883548641162505\nTraining loss per 100 training steps: 0.00880784949849929\nTraining loss per 100 training steps: 0.00881614605669627\nTraining loss per 100 training steps: 0.008801794302167305\nTraining loss per 100 training steps: 0.00880014982505585\nTraining loss per 100 training steps: 0.008763693643759398\nTraining loss per 100 training steps: 0.008750002840057189\nTraining loss per 100 training steps: 0.008743832663823923\nTraining loss per 100 training steps: 0.008755230591852093\nTraining loss per 100 training steps: 0.008749150370320076\nTraining loss per 100 training steps: 0.008750153047387067\nTraining loss per 100 training steps: 0.008745995711915427\nTraining loss per 100 training steps: 0.008743282517165864\nTraining loss per 100 training steps: 0.00875350400636932\nTraining loss per 100 training steps: 0.008757709877633737\nTraining loss per 100 training steps: 0.008747459696884778\nTraining loss per 100 training steps: 0.008736884731673101\nTraining loss per 100 training steps: 0.008746308092091884\nTraining loss per 100 training steps: 0.008734563741537823\nTraining loss epoch: 0.008730639332123728\nTraining accuracy epoch: 0.9964705852676596\nValidation Loss: 0.009253446920809105\nValidation Accuracy: 0.996326875047106\nTraining epoch: 3\nTraining loss per 100 training steps: 0.009406466968357563\nTraining loss per 100 training steps: 0.007001027475240281\nTraining loss per 100 training steps: 0.0066319074520386\nTraining loss per 100 training steps: 0.006562809592572758\nTraining loss per 100 training steps: 0.006532785994779642\nTraining loss per 100 training steps: 0.006720070555334922\nTraining loss per 100 training steps: 0.006930299119240513\nTraining loss per 100 training steps: 0.006886154367470883\nTraining loss per 100 training steps: 0.006963397044197658\nTraining loss per 100 training steps: 0.006913167530724125\nTraining loss per 100 training steps: 0.006877315277884995\nTraining loss per 100 training steps: 0.007005343929430737\nTraining loss per 100 training steps: 0.006985194304621922\nTraining loss per 100 training steps: 0.006979687587201644\nTraining loss per 100 training steps: 0.006995374126687669\nTraining loss per 100 training steps: 0.006950932959004714\nTraining loss per 100 training steps: 0.006925397898564571\nTraining loss per 100 training steps: 0.007055675868646767\nTraining loss per 100 training steps: 0.007080867416021922\nTraining loss per 100 training steps: 0.007133549947158816\nTraining loss per 100 training steps: 0.007089135761107072\nTraining loss per 100 training steps: 0.007130029807678021\nTraining loss per 100 training steps: 0.007159081562809783\nTraining loss per 100 training steps: 0.0071242737898803985\nTraining loss per 100 training steps: 0.007101316879188293\nTraining loss per 100 training steps: 0.007025411940011705\nTraining loss per 100 training steps: 0.006974453360313892\nTraining loss per 100 training steps: 0.006988955180765126\nTraining loss per 100 training steps: 0.006996120595449604\nTraining loss per 100 training steps: 0.0069751900337585585\nTraining loss per 100 training steps: 0.0069564099914201675\nTraining loss per 100 training steps: 0.00698608492306579\nTraining loss per 100 training steps: 0.006957042357255792\nTraining loss per 100 training steps: 0.006958117278473376\nTraining loss per 100 training steps: 0.006924478365622159\nTraining loss per 100 training steps: 0.006911769437850665\nTraining loss per 100 training steps: 0.006910688749007559\nTraining loss per 100 training steps: 0.006906075720643347\nTraining loss per 100 training steps: 0.006899980271983079\nTraining loss per 100 training steps: 0.0069173064643439815\nTraining loss per 100 training steps: 0.006915062852338236\nTraining loss per 100 training steps: 0.006925723194158256\nTraining loss per 100 training steps: 0.006931992265438544\nTraining loss per 100 training steps: 0.006927243337445265\nTraining loss per 100 training steps: 0.006922014391292896\nTraining loss per 100 training steps: 0.0069073038038338285\nTraining loss per 100 training steps: 0.006907036784859799\nTraining loss per 100 training steps: 0.006891638117045404\nTraining loss per 100 training steps: 0.006895458049224703\nTraining loss per 100 training steps: 0.006880445362316514\nTraining loss per 100 training steps: 0.006870482565525667\nTraining loss per 100 training steps: 0.006880541939131396\nTraining loss per 100 training steps: 0.00686422195259734\nTraining loss per 100 training steps: 0.006858715328683434\nTraining loss per 100 training steps: 0.00684996347344321\nTraining loss per 100 training steps: 0.006840486267422392\nTraining loss per 100 training steps: 0.0068326069551898575\nTraining loss per 100 training steps: 0.006818935887149727\nTraining loss per 100 training steps: 0.006824418125313971\nTraining loss per 100 training steps: 0.006837989121415776\nTraining loss per 100 training steps: 0.006839044508369651\nTraining loss per 100 training steps: 0.0068471831214480345\nTraining loss per 100 training steps: 0.006846968968782119\nTraining loss per 100 training steps: 0.006833359641976487\nTraining loss per 100 training steps: 0.006823782278514068\nTraining loss per 100 training steps: 0.006797452643919839\nTraining loss epoch: 0.0067946064595183694\nTraining accuracy epoch: 0.9972399078535409\nValidation Loss: 0.008599785200825097\nValidation Accuracy: 0.996723697854325\nTraining epoch: 4\nTraining loss per 100 training steps: 9.350636537419632e-05\nTraining loss per 100 training steps: 0.005572460741922721\nTraining loss per 100 training steps: 0.00526511106365424\nTraining loss per 100 training steps: 0.0052264999253853256\nTraining loss per 100 training steps: 0.0050602990097440015\nTraining loss per 100 training steps: 0.0050481546140273455\nTraining loss per 100 training steps: 0.005118802949765024\nTraining loss per 100 training steps: 0.0051245226789974245\nTraining loss per 100 training steps: 0.005165231540378498\nTraining loss per 100 training steps: 0.005202295953515384\nTraining loss per 100 training steps: 0.0052320309341112635\nTraining loss per 100 training steps: 0.0052533234679128255\nTraining loss per 100 training steps: 0.005343711261588098\nTraining loss per 100 training steps: 0.005395337710992503\nTraining loss per 100 training steps: 0.0054646379612105994\nTraining loss per 100 training steps: 0.005493150289541232\nTraining loss per 100 training steps: 0.005420351240952608\nTraining loss per 100 training steps: 0.005406078896085669\nTraining loss per 100 training steps: 0.005481879472870339\nTraining loss per 100 training steps: 0.005519926936816245\nTraining loss per 100 training steps: 0.005500711848896159\nTraining loss per 100 training steps: 0.005474555041100934\nTraining loss per 100 training steps: 0.0054619612547809025\nTraining loss per 100 training steps: 0.005450883352837637\nTraining loss per 100 training steps: 0.0054148454131585306\nTraining loss per 100 training steps: 0.005398706050898943\nTraining loss per 100 training steps: 0.005397237320646086\nTraining loss per 100 training steps: 0.005383671564177705\nTraining loss per 100 training steps: 0.005393925704258179\nTraining loss per 100 training steps: 0.005373450509724653\nTraining loss per 100 training steps: 0.005396036759355769\nTraining loss per 100 training steps: 0.0054056264448083015\nTraining loss per 100 training steps: 0.005372934274053705\nTraining loss per 100 training steps: 0.0053965027583398875\nTraining loss per 100 training steps: 0.005414199745753447\nTraining loss per 100 training steps: 0.005389925008773782\nTraining loss per 100 training steps: 0.005401465429117619\nTraining loss per 100 training steps: 0.0053901584376271966\nTraining loss per 100 training steps: 0.005381617122731222\nTraining loss per 100 training steps: 0.005372173233177421\nTraining loss per 100 training steps: 0.0053947227078547455\nTraining loss per 100 training steps: 0.005391286670593641\nTraining loss per 100 training steps: 0.005383159438726698\nTraining loss per 100 training steps: 0.005365696141436862\nTraining loss per 100 training steps: 0.005351082344610097\nTraining loss per 100 training steps: 0.005381957574899126\nTraining loss per 100 training steps: 0.0053787535853801974\nTraining loss per 100 training steps: 0.005379709178580923\nTraining loss per 100 training steps: 0.00538854331705089\nTraining loss per 100 training steps: 0.0053856452519603585\nTraining loss per 100 training steps: 0.005426541336598494\nTraining loss per 100 training steps: 0.005407485863334151\nTraining loss per 100 training steps: 0.005425113718238346\nTraining loss per 100 training steps: 0.005410611930583218\nTraining loss per 100 training steps: 0.005405227153010534\nTraining loss per 100 training steps: 0.00540572446486713\nTraining loss per 100 training steps: 0.005401512070756363\nTraining loss per 100 training steps: 0.005397194606571121\nTraining loss per 100 training steps: 0.005409900536133358\nTraining loss per 100 training steps: 0.005424157424534798\nTraining loss per 100 training steps: 0.0054237137456728465\nTraining loss per 100 training steps: 0.005429303876187033\nTraining loss per 100 training steps: 0.0054309039742637625\nTraining loss per 100 training steps: 0.005438303873579227\nTraining loss per 100 training steps: 0.005429598225247956\nTraining loss per 100 training steps: 0.005425825709819844\nTraining loss epoch: 0.005421633003724691\nTraining accuracy epoch: 0.9978553780733053\nValidation Loss: 0.008939548026843687\nValidation Accuracy: 0.9967953206794257\nTraining epoch: 5\nTraining loss per 100 training steps: 0.0007395331049337983\nTraining loss per 100 training steps: 0.003805828024287441\nTraining loss per 100 training steps: 0.004094536034588699\nTraining loss per 100 training steps: 0.0038234909304666743\nTraining loss per 100 training steps: 0.0038159082828145043\nTraining loss per 100 training steps: 0.00390770919919581\nTraining loss per 100 training steps: 0.003805486437904892\nTraining loss per 100 training steps: 0.003888008670632119\nTraining loss per 100 training steps: 0.003906669044662886\nTraining loss per 100 training steps: 0.003819848931137299\nTraining loss per 100 training steps: 0.003756446581007424\nTraining loss per 100 training steps: 0.003822408431181022\nTraining loss per 100 training steps: 0.0037832800275726117\nTraining loss per 100 training steps: 0.0038261519265081053\nTraining loss per 100 training steps: 0.003840760504575958\nTraining loss per 100 training steps: 0.003865596223355856\nTraining loss per 100 training steps: 0.003914651746932551\nTraining loss per 100 training steps: 0.0039176254542752885\nTraining loss per 100 training steps: 0.003945939718520187\nTraining loss per 100 training steps: 0.003949905827863999\nTraining loss per 100 training steps: 0.003996836819279851\nTraining loss per 100 training steps: 0.00399400562370497\nTraining loss per 100 training steps: 0.003985956554210149\nTraining loss per 100 training steps: 0.0040141467263353605\nTraining loss per 100 training steps: 0.003993593395251883\nTraining loss per 100 training steps: 0.004021675824283758\nTraining loss per 100 training steps: 0.004040663725822408\nTraining loss per 100 training steps: 0.004058979222052872\nTraining loss per 100 training steps: 0.00412106774378924\nTraining loss per 100 training steps: 0.004104650992755587\nTraining loss per 100 training steps: 0.004103857105498953\nTraining loss per 100 training steps: 0.0041049189494890715\nTraining loss per 100 training steps: 0.004104755031600457\nTraining loss per 100 training steps: 0.004104959703171271\nTraining loss per 100 training steps: 0.004114908069400228\nTraining loss per 100 training steps: 0.004157094307256144\nTraining loss per 100 training steps: 0.00417746872346972\nTraining loss per 100 training steps: 0.004184252082544082\nTraining loss per 100 training steps: 0.004174293364336662\nTraining loss per 100 training steps: 0.004186943940043298\nTraining loss per 100 training steps: 0.0041955554200850745\nTraining loss per 100 training steps: 0.004181323486058359\nTraining loss per 100 training steps: 0.004175717460125549\nTraining loss per 100 training steps: 0.004158816024981703\nTraining loss per 100 training steps: 0.0041762762133316\nTraining loss per 100 training steps: 0.004167477291662549\nTraining loss per 100 training steps: 0.004176404724728076\nTraining loss per 100 training steps: 0.00418436501493125\nTraining loss per 100 training steps: 0.004177870230591551\nTraining loss per 100 training steps: 0.004172566879272542\nTraining loss per 100 training steps: 0.0041776164094214354\nTraining loss per 100 training steps: 0.004185703709080824\nTraining loss per 100 training steps: 0.004177454938542542\nTraining loss per 100 training steps: 0.004165387307653915\nTraining loss per 100 training steps: 0.004173065193624492\nTraining loss per 100 training steps: 0.004176334636280597\nTraining loss per 100 training steps: 0.004184044959291201\nTraining loss per 100 training steps: 0.004191786638499829\nTraining loss per 100 training steps: 0.004199302415543439\nTraining loss per 100 training steps: 0.004210824324115772\nTraining loss per 100 training steps: 0.004215436068295694\nTraining loss per 100 training steps: 0.004209372762244899\nTraining loss per 100 training steps: 0.004218678056354027\nTraining loss per 100 training steps: 0.004221403164285616\nTraining loss per 100 training steps: 0.004227981033740972\nTraining loss per 100 training steps: 0.00422168386327807\nTraining loss epoch: 0.004213801804610741\nTraining accuracy epoch: 0.9983718737818075\nValidation Loss: 0.00887207226323922\nValidation Accuracy: 0.9969417503163949\nTraining epoch: 6\nTraining loss per 100 training steps: 0.000784390140324831\nTraining loss per 100 training steps: 0.002413341164680274\nTraining loss per 100 training steps: 0.0025713321686892036\nTraining loss per 100 training steps: 0.003037996160245624\nTraining loss per 100 training steps: 0.002908753032376293\nTraining loss per 100 training steps: 0.0030273469974980583\nTraining loss per 100 training steps: 0.0029722957567874238\nTraining loss per 100 training steps: 0.0030098551667810465\nTraining loss per 100 training steps: 0.0030671131355663197\nTraining loss per 100 training steps: 0.0030810360626285452\nTraining loss per 100 training steps: 0.0030289221819723867\nTraining loss per 100 training steps: 0.0030989637433595897\nTraining loss per 100 training steps: 0.003090727446042963\nTraining loss per 100 training steps: 0.0031190340130504983\nTraining loss per 100 training steps: 0.0031381391386142857\nTraining loss per 100 training steps: 0.003114708251888514\nTraining loss per 100 training steps: 0.003116069927038017\nTraining loss per 100 training steps: 0.003121989919588333\nTraining loss per 100 training steps: 0.0031475914329045602\nTraining loss per 100 training steps: 0.003149419498873538\nTraining loss per 100 training steps: 0.0031330590474148534\nTraining loss per 100 training steps: 0.003104397846036608\nTraining loss per 100 training steps: 0.0031569557754803283\nTraining loss per 100 training steps: 0.00319961824621194\nTraining loss per 100 training steps: 0.003196989224654248\nTraining loss per 100 training steps: 0.003188431235820772\nTraining loss per 100 training steps: 0.0032034488854334854\nTraining loss per 100 training steps: 0.003196995180504184\nTraining loss per 100 training steps: 0.0031961650391603247\nTraining loss per 100 training steps: 0.0032212849541032556\nTraining loss per 100 training steps: 0.0032492194048573354\nTraining loss per 100 training steps: 0.0032594438037076077\nTraining loss per 100 training steps: 0.0032225560433574198\nTraining loss per 100 training steps: 0.003202329445900201\nTraining loss per 100 training steps: 0.0032154986416944556\nTraining loss per 100 training steps: 0.0032057570306605974\nTraining loss per 100 training steps: 0.0032111376296311976\nTraining loss per 100 training steps: 0.0032194780580452873\nTraining loss per 100 training steps: 0.003224222660850897\nTraining loss per 100 training steps: 0.0032134245050863116\nTraining loss per 100 training steps: 0.0032121683211206696\nTraining loss per 100 training steps: 0.0031974736099524286\nTraining loss per 100 training steps: 0.00321978107619258\nTraining loss per 100 training steps: 0.0032265100057999657\nTraining loss per 100 training steps: 0.0032373898656797794\nTraining loss per 100 training steps: 0.0032160283738494793\nTraining loss per 100 training steps: 0.0032142764707051184\nTraining loss per 100 training steps: 0.003214650995153821\nTraining loss per 100 training steps: 0.003210841675268454\nTraining loss per 100 training steps: 0.003211482794873933\nTraining loss per 100 training steps: 0.0032305132129250894\nTraining loss per 100 training steps: 0.003256075975679888\nTraining loss per 100 training steps: 0.003249704839745891\nTraining loss per 100 training steps: 0.003240030125527661\nTraining loss per 100 training steps: 0.0032264661366795257\nTraining loss per 100 training steps: 0.003215122961701768\nTraining loss per 100 training steps: 0.0032047157364560805\nTraining loss per 100 training steps: 0.003203314735515589\nTraining loss per 100 training steps: 0.0032169938856668875\nTraining loss per 100 training steps: 0.003213190388113911\nTraining loss per 100 training steps: 0.0032201607075666777\nTraining loss per 100 training steps: 0.003215066792152081\nTraining loss per 100 training steps: 0.0032142286884728013\nTraining loss per 100 training steps: 0.003205770131402077\nTraining loss per 100 training steps: 0.0032027781147830024\nTraining loss per 100 training steps: 0.00319842446854836\nTraining loss epoch: 0.003203148902234101\nTraining accuracy epoch: 0.9988095223296118\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3511441975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_loader2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mtrLosslist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3788492471.py\u001b[0m in \u001b[0;36mvalid\u001b[0;34m(model, testing_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3438723534.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m                              \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                              \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                              max_length=self.max_len)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# step 3: create token labels only for first word pieces of each tokenized word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m             )\n\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2566\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2567\u001b[0m         )\n\u001b[1;32m   2568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/tokenization_roberta_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m         )\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/tokenization_roberta_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stdout","text":"time: 2h 14min 8s (started: 2022-09-09 11:24:57 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# NEW ONE newly generated words\ndef lengthOfTokens(pair):\n        return len(pair.split())\n\ndef tags_to_keywords(sample, words):\n    indices = [i for i, l in enumerate(sample) if l != 'O']\n    keywords, key_cats = [], []\n    for j, id in enumerate(indices):\n        if j == 0:\n            start = end = id\n            continue\n        if j == len(indices):\n            pos = pos_tag(words[start:end+1])\n            if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n                keywords.append(' '.join(words[start:end+1]))\n                key_cats.append((' '.join(words[start:end+1]), sample[start:end+1]))\n            continue\n        if end+1 == id:\n            end = id\n        else:\n            pos = pos_tag(words[start:end+1])\n            if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n                keywords.append(' '.join(words[start:end+1]))\n                key_cats.append((' '.join(words[start:end+1]), sample[start:end+1]))\n            start = end = id\n    return list(set(keywords)), key_cats\n\ndef countKeywords(test_dataset, model):\n    kws_pairs = []\n    Preds, Preds_cats, Lbs, Lbs_cats = [], [], [], []\n    for tmp_num in range(len(test_dataset)):\n        sentence = test_dataset[\"sentence\"].iloc[tmp_num]\n\n        inputs = tokenizer(sentence.split(),\n                            is_split_into_words=True, \n                            return_offsets_mapping=True, \n                            padding='max_length', \n                            truncation=True, \n                            max_length=MAX_LEN,\n                            return_tensors=\"pt\")\n\n        # move to gpu\n        ids = inputs[\"input_ids\"].to(device)\n        mask = inputs[\"attention_mask\"].to(device)\n        # forward pass\n        outputs = model(ids, attention_mask=mask)\n        logits = outputs.logits\n\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n\n        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n        token_predictions = [ids_to_labels2[i] for i in flattened_predictions.cpu().numpy()]\n        wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n        prediction = []\n        for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n          #only predictions on first word pieces are important\n          if mapping[0] == 0 and mapping[1] != 0:\n            prediction.append(token_pred[1])\n          else:\n            continue\n        \n        # predictions\n        preds, preds_cats = tags_to_keywords(prediction, sentence.split())\n        lbs, lbs_cats = tags_to_keywords(test_dataset[\"word_labels2\"].iloc[tmp_num].split(','), sentence.split())\n\n        Preds.append(preds)\n        Preds_cats.append(preds_cats)\n        Lbs.append(lbs)\n        Lbs_cats.append(lbs_cats)\n        \n    return Preds, Preds_cats, Lbs, Lbs_cats","metadata":{"execution":{"iopub.status.busy":"2022-09-09T13:39:12.579703Z","iopub.execute_input":"2022-09-09T13:39:12.580141Z","iopub.status.idle":"2022-09-09T13:39:12.607505Z","shell.execute_reply.started":"2022-09-09T13:39:12.580110Z","shell.execute_reply":"2022-09-09T13:39:12.606068Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"time: 5.78 ms (started: 2022-09-09 13:39:12 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"Preds, Preds_cats, Lbs, Lbs_cats = countKeywords(test_dataset2, model2)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T13:39:13.276235Z","iopub.execute_input":"2022-09-09T13:39:13.277059Z","iopub.status.idle":"2022-09-09T13:41:26.658545Z","shell.execute_reply.started":"2022-09-09T13:39:13.277008Z","shell.execute_reply":"2022-09-09T13:41:26.656919Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"time: 2min 13s (started: 2022-09-09 13:39:13 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_lable_keywords = {}\npred_lable_keywords['tag_preds'] = predictions\npred_lable_keywords['tag_labels'] = labels\npred_lable_keywords['predicitons'] = Preds\npred_lable_keywords['preds_cats'] = Preds_cats\npred_lable_keywords['labels'] = Lbs\npred_lable_keywords['labels_cats'] = Lbs_cats","metadata":{"execution":{"iopub.status.busy":"2022-09-09T13:41:26.662246Z","iopub.execute_input":"2022-09-09T13:41:26.662562Z","iopub.status.idle":"2022-09-09T13:41:26.670674Z","shell.execute_reply.started":"2022-09-09T13:41:26.662533Z","shell.execute_reply":"2022-09-09T13:41:26.669070Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"time: 833 µs (started: 2022-09-09 13:41:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\ndef get_recall(Labels, Predictions, ignore=True):\n    recalls = []\n    for i, lbs in enumerate(Labels):\n        preds = Predictions[i]\n        if not lbs:\n            if not ignore:\n                recalls.append(1.0 if not preds else 0.0)\n            continue\n        recalls.append(len(intersection(preds, lbs)) / len(lbs))\n    return np.mean(recalls)\n\ndef get_precision(Labels, Predictions, ignore=True):\n    precisions = []\n    for i, preds in enumerate(Predictions):\n        lbs = Labels[i]\n        if not preds:\n            if not ignore:\n                precisions.append(1.0 if not preds else 0.0)\n            continue\n        precisions.append(len(intersection(preds, lbs)) / len(preds))\n    return np.mean(precisions)\n\ndef get_f1score(Labels, Predictions, ignore=True):\n    precision = get_precision(Labels, Predictions, ignore)\n    recall = get_recall(Labels, Predictions, ignore)\n    return 2 * precision * recall / (precision + recall)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T13:41:26.672617Z","iopub.execute_input":"2022-09-09T13:41:26.673575Z","iopub.status.idle":"2022-09-09T13:41:26.691286Z","shell.execute_reply.started":"2022-09-09T13:41:26.673531Z","shell.execute_reply":"2022-09-09T13:41:26.689497Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"time: 2.56 ms (started: 2022-09-09 13:41:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = pred_lable_keywords['predicitons']\nlabels = pred_lable_keywords['labels']\n\nprint(get_recall(labels, predictions, False))\nprint(get_precision(labels, predictions, False))\nprint(get_f1score(labels, predictions, False))\nprint(get_recall(labels, predictions))\nprint(get_precision(labels, predictions))\nprint(get_f1score(labels, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-09-09T13:41:26.694767Z","iopub.execute_input":"2022-09-09T13:41:26.695228Z","iopub.status.idle":"2022-09-09T13:41:26.740581Z","shell.execute_reply.started":"2022-09-09T13:41:26.695186Z","shell.execute_reply":"2022-09-09T13:41:26.739070Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"0.9264653909570448\n0.9651591516728086\n0.9454165249380461\n0.842757306888265\n0.8856567776512996\n0.8636746569437744\ntime: 38.4 ms (started: 2022-09-09 13:41:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# import pickle\n\n# with open(f\"./albert-noblock-label-pred-keywords-kfold{cur_fold}.pkl\", \"wb\") as f:\n#     pickle.dump(pred_lable_keywords, f)\n\n# resultdf.to_csv(f\"./albert-noblock-loss-acc-kfold{cur_fold}.csv\")\n\nimport pickle\n\nwith open(f\"./albert-label-pred-keywords-kfold{cur_fold}.pkl\", \"wb\") as f:\n    pickle.dump(pred_lable_keywords, f)\n\nresultdf.to_csv(f\"./albert-loss-acc-kfold{cur_fold}.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:37:02.834645Z","iopub.execute_input":"2022-09-05T08:37:02.835598Z","iopub.status.idle":"2022-09-05T08:37:02.906890Z","shell.execute_reply.started":"2022-09-05T08:37:02.835562Z","shell.execute_reply":"2022-09-05T08:37:02.905813Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"time: 66 ms (started: 2022-09-05 08:37:02 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"model2.save_pretrained(f\"./albert_epoch4-kfold{cur_fold}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:37:02.909738Z","iopub.execute_input":"2022-09-05T08:37:02.910148Z","iopub.status.idle":"2022-09-05T08:37:02.990941Z","shell.execute_reply.started":"2022-09-05T08:37:02.910101Z","shell.execute_reply":"2022-09-05T08:37:02.989815Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"time: 76.4 ms (started: 2022-09-05 08:37:02 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT + CRF","metadata":{}},{"cell_type":"code","source":"!pip install pytorch_pretrained_bert==0.6.1","metadata":{"execution":{"iopub.status.busy":"2022-09-09T10:46:34.588900Z","iopub.execute_input":"2022-09-09T10:46:34.589249Z","iopub.status.idle":"2022-09-09T10:47:03.104843Z","shell.execute_reply.started":"2022-09-09T10:46:34.589212Z","shell.execute_reply":"2022-09-09T10:47:03.102886Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Collecting pytorch_pretrained_bert==0.6.1\n  Downloading pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.3/114.3 kB\u001b[0m \u001b[31m571.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert==0.6.1) (2.27.1)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert==0.6.1) (1.24.10)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert==0.6.1) (2021.11.10)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert==0.6.1) (1.11.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert==0.6.1) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert==0.6.1) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (4.1.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert==0.6.1) (0.6.0)\nRequirement already satisfied: botocore<1.28.0,>=1.27.10 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert==0.6.1) (1.27.10)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert==0.6.1) (1.0.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert==0.6.1) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert==0.6.1) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert==0.6.1) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert==0.6.1) (1.26.9)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.10->boto3->pytorch_pretrained_bert==0.6.1) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.10->boto3->pytorch_pretrained_bert==0.6.1) (1.16.0)\nInstalling collected packages: pytorch_pretrained_bert\n  Attempting uninstall: pytorch_pretrained_bert\n    Found existing installation: pytorch-pretrained-bert 0.6.2\n    Uninstalling pytorch-pretrained-bert-0.6.2:\n      Successfully uninstalled pytorch-pretrained-bert-0.6.2\nSuccessfully installed pytorch_pretrained_bert-0.6.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mtime: 28.5 s (started: 2022-09-09 10:46:34 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nimport os\nimport time\nimport importlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport torch.optim as optim\n\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils import data\n\nfrom tqdm import tqdm, trange\nimport collections\n\nfrom pytorch_pretrained_bert.modeling import BertModel, BertForTokenClassification, BertLayerNorm\nimport pickle\nfrom pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-09-09T10:47:20.725525Z","iopub.execute_input":"2022-09-09T10:47:20.725964Z","iopub.status.idle":"2022-09-09T10:47:20.738970Z","shell.execute_reply.started":"2022-09-09T10:47:20.725933Z","shell.execute_reply":"2022-09-09T10:47:20.737220Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"time: 1.6 ms (started: 2022-09-09 10:47:20 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def log_sum_exp_1vec(vec):  # shape(1,m)\n    max_score = vec[0, np.argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n\ndef log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n\ndef log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))\n\n\nclass BERT_CRF_NER(nn.Module):\n\n    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n        super(BERT_CRF_NER, self).__init__()\n        self.hidden_size = 768\n        self.start_label_id = start_label_id\n        self.stop_label_id = stop_label_id\n        self.num_labels = num_labels\n        # self.max_seq_length = max_seq_length\n        self.batch_size = batch_size\n        self.device=device\n\n        # use pretrainded BertModel\n        self.bert = bert_model\n        self.dropout = torch.nn.Dropout(0.2)\n        # Maps the output of the bert into label space.\n        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n\n        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n        self.transitions = nn.Parameter(\n            torch.randn(self.num_labels, self.num_labels))\n\n        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n        # so this enforcement is likely unimportant)\n        self.transitions.data[start_label_id, :] = -10000\n        self.transitions.data[:, stop_label_id] = -10000\n\n        nn.init.xavier_uniform_(self.hidden2label.weight)\n        nn.init.constant_(self.hidden2label.bias, 0.0)\n        # self.apply(self.init_bert_weights)\n\n    def init_bert_weights(self, module):\n        \"\"\" Initialize the weights.\n        \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def _forward_alg(self, feats):\n        '''\n        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX\n        '''\n\n        # T = self.max_seq_length\n        T = feats.shape[1]\n        batch_size = feats.shape[0]\n\n        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n        # self.start_label has all of the score. it is log,0 is p=1\n        log_alpha[:, 0, self.start_label_id] = 0\n\n        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n        # feats is the probability of emission, feat.shape=(1,tag_size)\n        for t in range(1, T):\n            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n\n        # log_prob of all barX\n        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n        return log_prob_all_barX\n\n    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n        '''\n        sentances -> word embedding -> lstm -> MLP -> feats\n        '''\n        bert_seq_out, _ = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, output_all_encoded_layers=False)\n        bert_seq_out = self.dropout(bert_seq_out)\n        bert_feats = self.hidden2label(bert_seq_out)\n        return bert_feats\n\n    def _score_sentence(self, feats, label_ids):\n        '''\n        Gives the score of a provided label sequence\n        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n        '''\n\n        # T = self.max_seq_length\n        T = feats.shape[1]\n        batch_size = feats.shape[0]\n\n        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n        batch_transitions = batch_transitions.flatten(1)\n\n        score = torch.zeros((feats.shape[0],1)).to(device)\n        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n        for t in range(1, T):\n            score = score + \\\n                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n        return score\n\n    def _viterbi_decode(self, feats):\n        '''\n        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n        '''\n\n        # T = self.max_seq_length\n        T = feats.shape[1]\n        batch_size = feats.shape[0]\n\n        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n\n        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n        log_delta[:, 0, self.start_label_id] = 0\n\n        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n        for t in range(1, T):\n            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n\n        # trace back\n        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n\n        # max p(z1:t,all_x|theta)\n        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n\n        for t in range(T-2, -1, -1):\n            # choose the state of z_t according the state choosed of z_t+1.\n            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n\n        return max_logLL_allz_allx, path\n\n    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n        forward_score = self._forward_alg(bert_feats)\n        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n        gold_score = self._score_sentence(bert_feats, label_ids)\n        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n        return torch.mean(forward_score - gold_score)\n\n    # this forward is just for predict, not for train\n    # dont confuse this with _forward_alg above.\n    def forward(self, input_ids, segment_ids, input_mask):\n        # Get the emission scores from the BiLSTM\n        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n\n        # Find the best path, given the features.\n        score, label_seq_ids = self._viterbi_decode(bert_feats)\n        return score, label_seq_ids","metadata":{"execution":{"iopub.status.busy":"2022-09-09T10:27:54.718750Z","iopub.execute_input":"2022-09-09T10:27:54.720217Z","iopub.status.idle":"2022-09-09T10:27:54.804046Z","shell.execute_reply.started":"2022-09-09T10:27:54.720153Z","shell.execute_reply":"2022-09-09T10:27:54.802316Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"time: 14.4 ms (started: 2022-09-09 10:27:54 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\nMAX_LEN = 300\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 4\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n# tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v1', add_prefix_space=True)\n\n# K-fold\n# k_fold = 5\n# cur_fold = 4\n# gap = int(len(data2) / k_fold)\n# indices = [[i*gap, (i+1)*gap if i+1 != k_fold else len(data2)] \n#           for i in range(k_fold)]\n# data3 = data2.sample(frac=1, random_state=200)\n# test_dataset2 = data3[indices[cur_fold][0]:indices[cur_fold][1]]\n# train_dataset2 = data3.drop(test_dataset2.index).reset_index(drop=True)\n# test_dataset2 = test_dataset2.reset_index(drop=True)\n\n# New dividing method\n# Method 1\n# train_dataset2 = data2.iloc[train_indices]\n# test_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\n# train_dataset2 = train_dataset2.reset_index(drop=True)\n# Method 2\n# train_dataset2 = data2.iloc[train_indices].reset_index(drop=True)\n# test_dataset2 = data2.iloc[list(test_indices)].reset_index(drop=True)\n\ntrain_size = 0.8\ntrain_dataset2 = data2.sample(frac=train_size,random_state=202)\ntest_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\ntrain_dataset2 = train_dataset2.reset_index(drop=True)\n\nprint(\"TRAIN Dataset: {}\".format(train_dataset2.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset2.shape))\n\ntraining_set2 = dataset2(train_dataset2, tokenizer, MAX_LEN)\ntesting_set2 = dataset2(test_dataset2, tokenizer, MAX_LEN)\n\ntrain_params2 = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params2 = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\n\ntraining_loader2 = DataLoader(training_set2, **train_params2)\ntesting_loader2 = DataLoader(testing_set2, **test_params2)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T10:40:50.445618Z","iopub.execute_input":"2022-09-09T10:40:50.447049Z","iopub.status.idle":"2022-09-09T10:40:56.277280Z","shell.execute_reply.started":"2022-09-09T10:40:50.446987Z","shell.execute_reply":"2022-09-09T10:40:56.275834Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea8cfb7c2dc948ddb850a1a6c6ae6d19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7040be91ef406f985df34f739eb0db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e56f0633d9844e0a2bf068e6849b94a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f876ff3c4112409dbc9bdc82c3426443"}},"metadata":{}},{"name":"stdout","text":"TRAIN Dataset: (26362, 3)\nTEST Dataset: (6590, 3)\ntime: 5.82 s (started: 2022-09-09 10:40:50 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"counter = 0\nfor batch in training_loader2:\n    print(batch['input_ids'])\n    print(batch['labels'])\n    counter += 1\n    if counter > 5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:09:43.171807Z","iopub.execute_input":"2022-09-09T11:09:43.172303Z","iopub.status.idle":"2022-09-09T11:09:43.241970Z","shell.execute_reply.started":"2022-09-09T11:09:43.172257Z","shell.execute_reply":"2022-09-09T11:09:43.240428Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"tensor([[  101,  5514,  1329,  ...,     0,     0,     0],\n        [  101,  1857,  1203,  ...,     0,     0,     0],\n        [  101,  1109,  6986,  ...,  4679, 11478,   102],\n        [  101, 24819, 12880,  ...,  1559,  8653,   102]])\ntensor([[-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0, -100,  ..., -100,    0, -100]])\ntensor([[ 101, 1130, 1103,  ...,    0,    0,    0],\n        [ 101, 1130, 1142,  ...,    0,    0,    0],\n        [ 101, 8094, 1112,  ...,    0,    0,    0],\n        [ 101, 4602, 1106,  ...,    0,    0,    0]])\ntensor([[-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100]])\ntensor([[  101,  1130, 10351,  ...,     0,     0,     0],\n        [  101,  1130,  1901,  ...,     0,     0,     0],\n        [  101, 22439,  1106,  ...,     0,     0,     0],\n        [  101, 22531,  1880,  ...,     0,     0,     0]])\ntensor([[-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0, -100,  ..., -100, -100, -100]])\ntensor([[ 101, 1130, 6796,  ...,    0,    0,    0],\n        [ 101, 1135, 1209,  ...,    0,    0,    0],\n        [ 101, 1109, 4644,  ...,    0,    0,    0],\n        [ 101, 1247, 1110,  ..., 1137, 2653,  102]])\ntensor([[-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ...,    0,    0, -100]])\ntensor([[  101, 21161,  7792,  ...,     0,     0,     0],\n        [  101,  1409,   157,  ...,     0,     0,     0],\n        [  101,  6291,  4290,  ...,     0,     0,     0],\n        [  101,   143,  7535,  ...,     0,     0,     0]])\ntensor([[-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0, -100,  ..., -100, -100, -100]])\ntensor([[  101,  1124,  1110,  ...,     0,     0,     0],\n        [  101,  3518, 19547,  ...,     0,     0,     0],\n        [  101, 18821,  1105,  ...,     0,     0,     0],\n        [  101,  5533,  1191,  ...,     0,     0,     0]])\ntensor([[-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100],\n        [-100,    0,    0,  ..., -100, -100, -100]])\ntime: 64.3 ms (started: 2022-09-09 11:09:43 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"start_label_id = -100\nstop_label_id = -100\nbert_model = BertModel.from_pretrained('bert-base-cased')\nmodel2 = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(labels_to_ids2), MAX_LEN, batch_size, device)\nmodel2.to(device)\n\noptimizer2 = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n\ntrLosslist, trAcclist, evalLosslist, evalAcclist, timelist = [], [], [], [], []\nlabel_pred_dict = {}\nfor epoch in range(EPOCHS):\n    print(f\"Training epoch: {epoch + 1}\")\n    tmp_dict = {}\n    start = time.time()\n    epoch_loss, tr_accuracy = train2(epoch, training_loader2)\n    labels, predictions, eval_loss, eval_accuracy, eval_probs = valid(model2, testing_loader2)\n    epoch_time = time.time() - start\n    trLosslist.append(epoch_loss)\n    trAcclist.append(tr_accuracy)\n    evalLosslist.append(eval_loss)\n    evalAcclist.append(eval_accuracy)\n    timelist.append(epoch_time)\n\nresultdf = pd.DataFrame({\"Epoch\": list(range(1, EPOCHS+1)),\n                         \"Train_loss\": trLosslist,\n                         \"Eval_loss\": evalLosslist,\n                         \"Train_Acc\": trAcclist,\n                         \"Eval_Acc\": evalAcclist,\n                        \"Time\": timelist})\nresultdf  ","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:10:38.946034Z","iopub.execute_input":"2022-09-09T11:10:38.946507Z","iopub.status.idle":"2022-09-09T11:10:47.854207Z","shell.execute_reply.started":"2022-09-09T11:10:38.946473Z","shell.execute_reply":"2022-09-09T11:10:47.849468Z"},"trusted":true},"execution_count":66,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/2525770496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstop_label_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_CRF_NER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_label_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_label_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_to_ids2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/2051178190.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# and we never transfer *from* the stop label (the model would probably learn this anyway,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# so this enforcement is likely unimportant)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_label_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_label_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index -100 is out of bounds for dimension 0 with size 8"],"ename":"IndexError","evalue":"index -100 is out of bounds for dimension 0 with size 8","output_type":"error"},{"name":"stdout","text":"time: 8.89 s (started: 2022-09-09 11:10:38 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Version 1 only keywords","metadata":{}},{"cell_type":"code","source":"def intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\ndef get_recall(Labels, Predictions):\n    recalls = []\n    for i, lbs in enumerate(Labels):\n        preds = Predictions[i]\n        recalls.append(len(intersection(preds, lbs)) / len(lbs))\n    return np.mean(recalls)\n\ndef get_precision(Labels, Predictions):\n    precisions = []\n    for i, preds in enumerate(Predictions):\n        lbs = Labels[i]\n        precisions.append(len(intersection(preds, lbs)) / len(preds))\n    return np.mean(precisions)\n\ndef get_f1score(Labels, Predictions):\n    precision = get_precision(Labels, Predictions)\n    recall = get_recall(Labels, Predictions)\n    return [(precision[i][0], \n             np.nan_to_num(2 * precision[i][1] * recall[i][1] / (precision[i][1] + recall[i][1])))\n            for i in range(len(matrix)-1)]","metadata":{"execution":{"iopub.status.busy":"2022-08-09T14:32:00.948730Z","iopub.execute_input":"2022-08-09T14:32:00.949415Z","iopub.status.idle":"2022-08-09T14:32:00.963268Z","shell.execute_reply.started":"2022-08-09T14:32:00.949377Z","shell.execute_reply":"2022-08-09T14:32:00.962044Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"[('research and development', ['I-inn', 'I-inn', 'I-bus']),\n ('artificial intelligence research project',\n  ['I-dig', 'I-dig', 'I-inn', 'I-inn']),\n ('research centers', ['I-inn', 'I-dig']),\n ('product portfolio product development',\n  ['I-bus', 'I-bus', 'I-inn', 'I-inn']),\n ('research and development', ['I-inn', 'I-inn', 'I-bus']),\n ('research and development', ['I-inn', 'I-inn', 'I-bus'])]"},"metadata":{}},{"name":"stdout","text":"time: 8.97 ms (started: 2022-08-09 14:32:00 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# OLD ONE newly generated words\ndef lengthOfTokens(pair):\n        return len(pair.split())\n\ndef countKeywords(test_dataset, model):\n    kws_pairs = []\n    \n    for tmp_num in range(len(test_dataset)):\n        sentence = test_dataset[\"sentence\"].iloc[tmp_num]\n\n        inputs = tokenizer(sentence.split(),\n                            is_split_into_words=True, \n                            return_offsets_mapping=True, \n                            padding='max_length', \n                            truncation=True, \n                            max_length=MAX_LEN,\n                            return_tensors=\"pt\")\n\n        # move to gpu\n        ids = inputs[\"input_ids\"].to(device)\n        mask = inputs[\"attention_mask\"].to(device)\n        # forward pass\n        outputs = model(ids, attention_mask=mask)\n        logits = outputs.logits\n\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n\n        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n        token_predictions = [ids_to_labels2[i] for i in flattened_predictions.cpu().numpy()]\n        wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n        prediction = []\n        for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n          #only predictions on first word pieces are important\n          if mapping[0] == 0 and mapping[1] != 0:\n            prediction.append(token_pred[1])\n          else:\n            continue\n\n        for i in zip(sentence.split(), prediction, test_dataset[\"word_labels2\"].iloc[tmp_num].split(',')):\n            kws_pairs.append(i)\n\n    predict_kws = []\n    tmp_wd = []\n    tmp_pred, tmp_gold = '', ''\n    for word, pred, gold in kws_pairs:\n        if pred != 'O':\n            if len(tmp_wd) == 0: \n                tmp_pred = pred\n            else:\n                if tmp_pred != pred:\n                    predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                    tmp_wd = []\n                    tmp_pred = pred\n            tmp_wd.append(word)\n        else:\n            if len(tmp_wd) > 0:\n                predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                tmp_wd = []\n                tmp_pred = pred\n\n    # check whether there are new keywords\n    gold_dict = build_gold_dict()\n    new_kws, kw_tags = [], []\n    gold_lists = list(gold_dict.keys())\n    gold_lists = [g.lower() for g in gold_lists]\n    for word, label in predict_kws:\n        if not word.lower() in gold_lists:\n            new_kws.append(word.lower())\n            kw_tags.append(tag2cat[label])\n    new_keywords = list(zip(new_kws, kw_tags))\n\n    new_kws_dict, new_kws_pos = {}, []\n    for kw, cat in new_keywords:\n        pos = pos_tag(kw.split())\n        if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n            key = ' '.join([p for p, t in pos])\n            tag = [t for p, t in pos]\n            new_kws_pos.append(key)\n            if not key in new_kws_dict.keys():\n                new_kws_dict[key] = (tag, cat)\n\n    new_kws = list(new_kws_dict.keys())\n    cats = [value[1] for value in new_kws_dict.values()]\n\n    df = pd.DataFrame({\"keyword\": new_kws, \"category\": cats})\n\n    s = df['keyword'].apply(lengthOfTokens)\n    s.sort_values()\n\n    predict_kws = Counter(predict_kws)\n    new_kws_pos = Counter(new_kws_pos)\n\n    predict_kws = sorted(predict_kws.items(), key=lambda pair: pair[1], reverse=True)\n    new_kws_pos = sorted(new_kws_pos.items(), key=lambda pair: pair[1], reverse=True)\n\n    return predict_kws, new_kws_pos, df","metadata":{"execution":{"iopub.status.busy":"2022-07-29T14:50:38.716652Z","iopub.execute_input":"2022-07-29T14:50:38.716993Z","iopub.status.idle":"2022-07-29T14:50:38.741148Z","shell.execute_reply.started":"2022-07-29T14:50:38.716964Z","shell.execute_reply":"2022-07-29T14:50:38.739788Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"time: 5.12 ms (started: 2022-07-29 14:50:38 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"new_kws_dict = {}\nnew_kw_df = pd.DataFrame({})\nfor i in range(4):\n    labels = label_pred_dict[i][\"labels\"]\n    predictions = label_pred_dict[i][\"predictions\"]\n\n    predict_kws, new_kws_pos, tmp_kws_df = countKeywords(test_dataset2, model2)\n    tmp_kws_df['epoch'] = [i+1]*len(tmp_kws_df)\n\n    new_kws_dict['predicted_keywords_epoch_' + str(i+1)] = predict_kws\n    new_kws_dict['new_keywords_epoch_' + str(i+1)] = new_kws_pos\n\n    new_kw_df = pd.concat([new_kw_df, tmp_kws_df])\n    new_kw_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T14:50:40.241741Z","iopub.execute_input":"2022-07-29T14:50:40.242440Z","iopub.status.idle":"2022-07-29T14:55:23.345530Z","shell.execute_reply.started":"2022-07-29T14:50:40.242403Z","shell.execute_reply":"2022-07-29T14:55:23.344462Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n  import sys\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n  \n","output_type":"stream"},{"name":"stdout","text":"time: 4min 43s (started: 2022-07-29 14:50:40 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(4):\n    predict_kws = new_kws_dict['new_keywords_epoch_' + str(i+1)]\n    print(f\"In Epoch {i+1}:\")\n    print(predict_kws)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T14:55:23.347817Z","iopub.execute_input":"2022-07-29T14:55:23.349393Z","iopub.status.idle":"2022-07-29T14:55:23.356909Z","shell.execute_reply.started":"2022-07-29T14:55:23.349352Z","shell.execute_reply":"2022-07-29T14:55:23.355851Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"In Epoch 1:\n[('development', 52), ('research and development costs', 22), ('software development costs', 11), ('product development costs', 11), ('property', 10), ('programs', 8), ('energy', 7), ('data', 7), ('research and development services', 6), ('storage', 5), ('costs', 5), ('business', 5), ('product', 5), ('research', 5), ('solutions', 4), ('design', 4), ('chain', 4), ('carbon', 4), ('new industry', 4), ('artificial intelligence machine learning', 4), ('personal data data', 4), ('efforts', 3), ('co2', 3), ('product development projects', 3), ('product development efforts', 3), ('innovation', 3), ('technology', 3), ('cutting-edge', 3), ('design kit', 3), ('customer', 2), ('transformation', 2), ('climate', 2), ('efficiency', 2), ('science', 2), ('development problems', 2), ('quality', 2), ('iot', 2), ('innovative skills', 2), ('personal data protection', 2), ('research and development and commercialization', 2), ('innovation programs', 2), ('process', 2), ('center', 2), ('system designs', 2), ('cutting-edge technology platforms', 2), ('research and development product development', 2), ('experience', 2), ('circular', 2), ('production', 2), ('routers', 1), ('grant process', 1), ('partnerships', 1), ('style', 1), ('software development software development costs', 1), ('reality', 1), ('video design', 1), ('greenhouse gas', 1), ('product development activities', 1), ('development step', 1), ('agreement', 1), ('fuel', 1), ('user requirements', 1), ('digital technology data analytics', 1), ('collaborations', 1), ('product development project', 1), ('product development and commercialization', 1), ('climate change energy efficiency', 1), ('application process', 1), ('create', 1), ('lifecycle', 1), ('car design', 1), ('innovation strategy', 1), ('machine vision', 1), ('focus', 1), ('innovative tool', 1), ('system design', 1), ('development period', 1), ('research and development new product', 1), ('product development services', 1), ('transition', 1), ('gas emissions', 1), ('cutting edge technology', 1), ('drive', 1), ('scientific research projects', 1), ('scientific research teams', 1), ('innovative treatment', 1), ('product design process', 1), ('product development cycle', 1), ('storage technology', 1), ('design expertise', 1), ('products', 1), ('groups', 1), ('new work', 1), ('innovative concepts', 1), ('research programme', 1), ('information', 1), ('system design process', 1), ('activities', 1), ('cutting-edge solutions', 1), ('needs', 1), ('system development project', 1), (\"product portfolio's\", 1), ('of things', 1), ('protection', 1), ('network intelligence', 1), ('product design efforts', 1), ('design rules', 1), ('business concept', 1), ('data research', 1), ('development efforts research and development', 1), ('product development efforts research and development', 1), ('design costs', 1), ('innovative energy', 1), ('development and commercializationoftheseproduct', 1), ('cutting-edge strategies', 1), ('scientific research scientific research', 1), ('research universities', 1), ('nations', 1), ('engineering', 1), ('research and development cost', 1), ('development coststhe', 1), ('data centers data center', 1), ('interfaces', 1), ('design cycle', 1), ('structure', 1), ('product development strategy', 1), ('additive manufacturing big data', 1), ('artificial intelligence big data', 1), ('innovative models', 1), ('new product development projects', 1), ('science and innovation', 1), ('new product design', 1), ('new process', 1), ('innovation solutions', 1), ('research programmes', 1), ('innovative strategy', 1), ('product development effort', 1), ('cyber security data protection', 1), ('change', 1), ('climate change carbon emissions', 1), ('innovative development', 1), ('digital', 1), ('gas', 1), ('climate changing', 1), ('production phase', 1), ('data structures', 1), ('technology designs', 1), ('sources', 1), ('research and product development', 1), ('innovation organization', 1), ('innovative software', 1), ('software development process', 1), ('technology suite', 1), ('collaboration', 1), ('industrial designers', 1), ('software', 1), ('research bodies', 1), ('research firms', 1), ('product design technology development', 1), ('research and development efforts', 1), ('network', 1), ('cyber protection', 1), ('user', 1), ('innovative businesses', 1), ('organizations', 1), ('technologies', 1), ('interface', 1), ('test systems', 1), ('entire life cycle', 1), ('new product designs', 1), ('design authority', 1), ('whole value chain', 1), ('development approach', 1), ('new paradigm', 1), ('discovery and commercialization', 1), ('co2 emission', 1), ('production tools', 1), ('skill', 1), ('decision maker', 1), ('development concepts', 1), ('developments', 1), ('requirements', 1), ('cutting edge technologies', 1), ('production and commercialization', 1), ('design studio innovation centre', 1), ('research centres', 1), ('product development requirements', 1), ('technology development programs', 1), ('foundation', 1), ('standards', 1), ('project', 1), ('development areas', 1), ('systems', 1), ('greenhouse emission', 1), ('materials', 1)]\nIn Epoch 2:\n[('development', 52), ('research and development costs', 22), ('software development costs', 11), ('product development costs', 11), ('property', 10), ('programs', 8), ('energy', 7), ('data', 7), ('research and development services', 6), ('storage', 5), ('costs', 5), ('business', 5), ('product', 5), ('research', 5), ('solutions', 4), ('design', 4), ('chain', 4), ('carbon', 4), ('new industry', 4), ('artificial intelligence machine learning', 4), ('personal data data', 4), ('efforts', 3), ('co2', 3), ('product development projects', 3), ('product development efforts', 3), ('innovation', 3), ('technology', 3), ('cutting-edge', 3), ('design kit', 3), ('customer', 2), ('transformation', 2), ('climate', 2), ('efficiency', 2), ('science', 2), ('development problems', 2), ('quality', 2), ('iot', 2), ('innovative skills', 2), ('personal data protection', 2), ('research and development and commercialization', 2), ('innovation programs', 2), ('process', 2), ('center', 2), ('system designs', 2), ('cutting-edge technology platforms', 2), ('research and development product development', 2), ('experience', 2), ('circular', 2), ('production', 2), ('routers', 1), ('grant process', 1), ('partnerships', 1), ('style', 1), ('software development software development costs', 1), ('reality', 1), ('video design', 1), ('greenhouse gas', 1), ('product development activities', 1), ('development step', 1), ('agreement', 1), ('fuel', 1), ('user requirements', 1), ('digital technology data analytics', 1), ('collaborations', 1), ('product development project', 1), ('product development and commercialization', 1), ('climate change energy efficiency', 1), ('application process', 1), ('create', 1), ('lifecycle', 1), ('car design', 1), ('innovation strategy', 1), ('machine vision', 1), ('focus', 1), ('innovative tool', 1), ('system design', 1), ('development period', 1), ('research and development new product', 1), ('product development services', 1), ('transition', 1), ('gas emissions', 1), ('cutting edge technology', 1), ('drive', 1), ('scientific research projects', 1), ('scientific research teams', 1), ('innovative treatment', 1), ('product design process', 1), ('product development cycle', 1), ('storage technology', 1), ('design expertise', 1), ('products', 1), ('groups', 1), ('new work', 1), ('innovative concepts', 1), ('research programme', 1), ('information', 1), ('system design process', 1), ('activities', 1), ('cutting-edge solutions', 1), ('needs', 1), ('system development project', 1), (\"product portfolio's\", 1), ('of things', 1), ('protection', 1), ('network intelligence', 1), ('product design efforts', 1), ('design rules', 1), ('business concept', 1), ('data research', 1), ('development efforts research and development', 1), ('product development efforts research and development', 1), ('design costs', 1), ('innovative energy', 1), ('development and commercializationoftheseproduct', 1), ('cutting-edge strategies', 1), ('scientific research scientific research', 1), ('research universities', 1), ('nations', 1), ('engineering', 1), ('research and development cost', 1), ('development coststhe', 1), ('data centers data center', 1), ('interfaces', 1), ('design cycle', 1), ('structure', 1), ('product development strategy', 1), ('additive manufacturing big data', 1), ('artificial intelligence big data', 1), ('innovative models', 1), ('new product development projects', 1), ('science and innovation', 1), ('new product design', 1), ('new process', 1), ('innovation solutions', 1), ('research programmes', 1), ('innovative strategy', 1), ('product development effort', 1), ('cyber security data protection', 1), ('change', 1), ('climate change carbon emissions', 1), ('innovative development', 1), ('digital', 1), ('gas', 1), ('climate changing', 1), ('production phase', 1), ('data structures', 1), ('technology designs', 1), ('sources', 1), ('research and product development', 1), ('innovation organization', 1), ('innovative software', 1), ('software development process', 1), ('technology suite', 1), ('collaboration', 1), ('industrial designers', 1), ('software', 1), ('research bodies', 1), ('research firms', 1), ('product design technology development', 1), ('research and development efforts', 1), ('network', 1), ('cyber protection', 1), ('user', 1), ('innovative businesses', 1), ('organizations', 1), ('technologies', 1), ('interface', 1), ('test systems', 1), ('entire life cycle', 1), ('new product designs', 1), ('design authority', 1), ('whole value chain', 1), ('development approach', 1), ('new paradigm', 1), ('discovery and commercialization', 1), ('co2 emission', 1), ('production tools', 1), ('skill', 1), ('decision maker', 1), ('development concepts', 1), ('developments', 1), ('requirements', 1), ('cutting edge technologies', 1), ('production and commercialization', 1), ('design studio innovation centre', 1), ('research centres', 1), ('product development requirements', 1), ('technology development programs', 1), ('foundation', 1), ('standards', 1), ('project', 1), ('development areas', 1), ('systems', 1), ('greenhouse emission', 1), ('materials', 1)]\nIn Epoch 3:\n[('development', 52), ('research and development costs', 22), ('software development costs', 11), ('product development costs', 11), ('property', 10), ('programs', 8), ('energy', 7), ('data', 7), ('research and development services', 6), ('storage', 5), ('costs', 5), ('business', 5), ('product', 5), ('research', 5), ('solutions', 4), ('design', 4), ('chain', 4), ('carbon', 4), ('new industry', 4), ('artificial intelligence machine learning', 4), ('personal data data', 4), ('efforts', 3), ('co2', 3), ('product development projects', 3), ('product development efforts', 3), ('innovation', 3), ('technology', 3), ('cutting-edge', 3), ('design kit', 3), ('customer', 2), ('transformation', 2), ('climate', 2), ('efficiency', 2), ('science', 2), ('development problems', 2), ('quality', 2), ('iot', 2), ('innovative skills', 2), ('personal data protection', 2), ('research and development and commercialization', 2), ('innovation programs', 2), ('process', 2), ('center', 2), ('system designs', 2), ('cutting-edge technology platforms', 2), ('research and development product development', 2), ('experience', 2), ('circular', 2), ('production', 2), ('routers', 1), ('grant process', 1), ('partnerships', 1), ('style', 1), ('software development software development costs', 1), ('reality', 1), ('video design', 1), ('greenhouse gas', 1), ('product development activities', 1), ('development step', 1), ('agreement', 1), ('fuel', 1), ('user requirements', 1), ('digital technology data analytics', 1), ('collaborations', 1), ('product development project', 1), ('product development and commercialization', 1), ('climate change energy efficiency', 1), ('application process', 1), ('create', 1), ('lifecycle', 1), ('car design', 1), ('innovation strategy', 1), ('machine vision', 1), ('focus', 1), ('innovative tool', 1), ('system design', 1), ('development period', 1), ('research and development new product', 1), ('product development services', 1), ('transition', 1), ('gas emissions', 1), ('cutting edge technology', 1), ('drive', 1), ('scientific research projects', 1), ('scientific research teams', 1), ('innovative treatment', 1), ('product design process', 1), ('product development cycle', 1), ('storage technology', 1), ('design expertise', 1), ('products', 1), ('groups', 1), ('new work', 1), ('innovative concepts', 1), ('research programme', 1), ('information', 1), ('system design process', 1), ('activities', 1), ('cutting-edge solutions', 1), ('needs', 1), ('system development project', 1), (\"product portfolio's\", 1), ('of things', 1), ('protection', 1), ('network intelligence', 1), ('product design efforts', 1), ('design rules', 1), ('business concept', 1), ('data research', 1), ('development efforts research and development', 1), ('product development efforts research and development', 1), ('design costs', 1), ('innovative energy', 1), ('development and commercializationoftheseproduct', 1), ('cutting-edge strategies', 1), ('scientific research scientific research', 1), ('research universities', 1), ('nations', 1), ('engineering', 1), ('research and development cost', 1), ('development coststhe', 1), ('data centers data center', 1), ('interfaces', 1), ('design cycle', 1), ('structure', 1), ('product development strategy', 1), ('additive manufacturing big data', 1), ('artificial intelligence big data', 1), ('innovative models', 1), ('new product development projects', 1), ('science and innovation', 1), ('new product design', 1), ('new process', 1), ('innovation solutions', 1), ('research programmes', 1), ('innovative strategy', 1), ('product development effort', 1), ('cyber security data protection', 1), ('change', 1), ('climate change carbon emissions', 1), ('innovative development', 1), ('digital', 1), ('gas', 1), ('climate changing', 1), ('production phase', 1), ('data structures', 1), ('technology designs', 1), ('sources', 1), ('research and product development', 1), ('innovation organization', 1), ('innovative software', 1), ('software development process', 1), ('technology suite', 1), ('collaboration', 1), ('industrial designers', 1), ('software', 1), ('research bodies', 1), ('research firms', 1), ('product design technology development', 1), ('research and development efforts', 1), ('network', 1), ('cyber protection', 1), ('user', 1), ('innovative businesses', 1), ('organizations', 1), ('technologies', 1), ('interface', 1), ('test systems', 1), ('entire life cycle', 1), ('new product designs', 1), ('design authority', 1), ('whole value chain', 1), ('development approach', 1), ('new paradigm', 1), ('discovery and commercialization', 1), ('co2 emission', 1), ('production tools', 1), ('skill', 1), ('decision maker', 1), ('development concepts', 1), ('developments', 1), ('requirements', 1), ('cutting edge technologies', 1), ('production and commercialization', 1), ('design studio innovation centre', 1), ('research centres', 1), ('product development requirements', 1), ('technology development programs', 1), ('foundation', 1), ('standards', 1), ('project', 1), ('development areas', 1), ('systems', 1), ('greenhouse emission', 1), ('materials', 1)]\nIn Epoch 4:\n[('development', 52), ('research and development costs', 22), ('software development costs', 11), ('product development costs', 11), ('property', 10), ('programs', 8), ('energy', 7), ('data', 7), ('research and development services', 6), ('storage', 5), ('costs', 5), ('business', 5), ('product', 5), ('research', 5), ('solutions', 4), ('design', 4), ('chain', 4), ('carbon', 4), ('new industry', 4), ('artificial intelligence machine learning', 4), ('personal data data', 4), ('efforts', 3), ('co2', 3), ('product development projects', 3), ('product development efforts', 3), ('innovation', 3), ('technology', 3), ('cutting-edge', 3), ('design kit', 3), ('customer', 2), ('transformation', 2), ('climate', 2), ('efficiency', 2), ('science', 2), ('development problems', 2), ('quality', 2), ('iot', 2), ('innovative skills', 2), ('personal data protection', 2), ('research and development and commercialization', 2), ('innovation programs', 2), ('process', 2), ('center', 2), ('system designs', 2), ('cutting-edge technology platforms', 2), ('research and development product development', 2), ('experience', 2), ('circular', 2), ('production', 2), ('routers', 1), ('grant process', 1), ('partnerships', 1), ('style', 1), ('software development software development costs', 1), ('reality', 1), ('video design', 1), ('greenhouse gas', 1), ('product development activities', 1), ('development step', 1), ('agreement', 1), ('fuel', 1), ('user requirements', 1), ('digital technology data analytics', 1), ('collaborations', 1), ('product development project', 1), ('product development and commercialization', 1), ('climate change energy efficiency', 1), ('application process', 1), ('create', 1), ('lifecycle', 1), ('car design', 1), ('innovation strategy', 1), ('machine vision', 1), ('focus', 1), ('innovative tool', 1), ('system design', 1), ('development period', 1), ('research and development new product', 1), ('product development services', 1), ('transition', 1), ('gas emissions', 1), ('cutting edge technology', 1), ('drive', 1), ('scientific research projects', 1), ('scientific research teams', 1), ('innovative treatment', 1), ('product design process', 1), ('product development cycle', 1), ('storage technology', 1), ('design expertise', 1), ('products', 1), ('groups', 1), ('new work', 1), ('innovative concepts', 1), ('research programme', 1), ('information', 1), ('system design process', 1), ('activities', 1), ('cutting-edge solutions', 1), ('needs', 1), ('system development project', 1), (\"product portfolio's\", 1), ('of things', 1), ('protection', 1), ('network intelligence', 1), ('product design efforts', 1), ('design rules', 1), ('business concept', 1), ('data research', 1), ('development efforts research and development', 1), ('product development efforts research and development', 1), ('design costs', 1), ('innovative energy', 1), ('development and commercializationoftheseproduct', 1), ('cutting-edge strategies', 1), ('scientific research scientific research', 1), ('research universities', 1), ('nations', 1), ('engineering', 1), ('research and development cost', 1), ('development coststhe', 1), ('data centers data center', 1), ('interfaces', 1), ('design cycle', 1), ('structure', 1), ('product development strategy', 1), ('additive manufacturing big data', 1), ('artificial intelligence big data', 1), ('innovative models', 1), ('new product development projects', 1), ('science and innovation', 1), ('new product design', 1), ('new process', 1), ('innovation solutions', 1), ('research programmes', 1), ('innovative strategy', 1), ('product development effort', 1), ('cyber security data protection', 1), ('change', 1), ('climate change carbon emissions', 1), ('innovative development', 1), ('digital', 1), ('gas', 1), ('climate changing', 1), ('production phase', 1), ('data structures', 1), ('technology designs', 1), ('sources', 1), ('research and product development', 1), ('innovation organization', 1), ('innovative software', 1), ('software development process', 1), ('technology suite', 1), ('collaboration', 1), ('industrial designers', 1), ('software', 1), ('research bodies', 1), ('research firms', 1), ('product design technology development', 1), ('research and development efforts', 1), ('network', 1), ('cyber protection', 1), ('user', 1), ('innovative businesses', 1), ('organizations', 1), ('technologies', 1), ('interface', 1), ('test systems', 1), ('entire life cycle', 1), ('new product designs', 1), ('design authority', 1), ('whole value chain', 1), ('development approach', 1), ('new paradigm', 1), ('discovery and commercialization', 1), ('co2 emission', 1), ('production tools', 1), ('skill', 1), ('decision maker', 1), ('development concepts', 1), ('developments', 1), ('requirements', 1), ('cutting edge technologies', 1), ('production and commercialization', 1), ('design studio innovation centre', 1), ('research centres', 1), ('product development requirements', 1), ('technology development programs', 1), ('foundation', 1), ('standards', 1), ('project', 1), ('development areas', 1), ('systems', 1), ('greenhouse emission', 1), ('materials', 1)]\ntime: 1.42 ms (started: 2022-07-29 14:55:23 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_index_positions(list_of_elems, element):\n    ''' Returns the indexes of all occurrences of give element in\n    the list- listOfElements '''\n    index_pos_list = []\n    index_pos = 0\n    while True:\n        try:\n            # Search for item in list from indexPos to the end of list\n            index_pos = list_of_elems.index(element, index_pos)\n            # Add the index position in list\n            index_pos_list.append(index_pos)\n            index_pos += 1\n        except ValueError as e:\n            break\n    return index_pos_list\n\ndef get_distribution(inputs, name):\n    true_res = {}\n    for k, v in tag2cat.items():\n        indexlist = get_index_positions(inputs, k)\n        left, right = 0, 0\n        new_labels = []\n        for i in indexlist:\n            if left == 0:\n                left = i\n                right = i\n                continue\n            if right+1 == i:\n                right = i\n            else:\n                new_labels.append((left, right))\n                left = i\n                right = i\n        true_res[v] = new_labels\n    print(f\"The distribution of {name} is:\")\n    for k,v in true_res.items():\n        print(f\"There are {len(v)} occurances in {k}\")\n    return true_res\n\ndef get_confusion_matrix(true_labels, true_predictions):\n    # strict mode\n    tp_dict = {}\n    for true_k, true_v in true_labels.items():\n        true_total = len(true_v)\n        tmp_dict = {}\n        tmp_total = 0\n        for pred_k, pred_v in true_predictions.items():\n            num = 0\n            for v in pred_v:\n                if v in true_v:\n                    num += 1\n            tmp_dict[pred_k] = num\n            tmp_total += num\n        tmp_dict[\"O\"] = true_total - tmp_total\n        tp_dict[true_k] = tmp_dict\n    tp_dict\n    tmp_dict = {}\n    for pred_k, pred_v in true_predictions.items():\n        true_total = len(pred_v)\n        num = 0\n        for tp_k, tp_v in tp_dict.items():\n            num += tp_v[pred_k]\n        tmp_dict[pred_k] = true_total - num\n    tmp_dict[\"O\"] = 0\n    tp_dict[\"O\"] = tmp_dict\n    return pd.DataFrame(tp_dict).T","metadata":{"execution":{"iopub.status.busy":"2022-08-02T11:10:59.754966Z","iopub.execute_input":"2022-08-02T11:10:59.755330Z","iopub.status.idle":"2022-08-02T11:10:59.768308Z","shell.execute_reply.started":"2022-08-02T11:10:59.755298Z","shell.execute_reply":"2022-08-02T11:10:59.767269Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"time: 2.3 ms (started: 2022-08-02 11:10:59 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(4):\n    print(f\"In Epoch {i+1}:\")\n    labels = label_pred_dict[i][\"labels\"]\n    predictions = label_pred_dict[i][\"predictions\"]\n\n    true_labels = get_distribution(labels, \"labels\")\n    true_predictions = get_distribution(predictions, \"predictions\")\n    df = get_confusion_matrix(true_labels, true_predictions)\n    display(df)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T14:55:23.378735Z","iopub.execute_input":"2022-07-29T14:55:23.379198Z","iopub.status.idle":"2022-07-29T14:55:30.127079Z","shell.execute_reply.started":"2022-07-29T14:55:23.379163Z","shell.execute_reply":"2022-07-29T14:55:30.126155Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"In Epoch 1:\nThe distribution of labels is:\nThere are 1525 occurances in Sustainability preoccupations\nThere are 1112 occurances in Digital transformation\nThere are 200 occurances in Change in management\nThere are 5219 occurances in Innovation activities\nThere are 1293 occurances in Business Model\nThere are 35 occurances in Corporate social responsibility ou CSR\nThere are 1 occurances in marco-label\nThe distribution of predictions is:\nThere are 1737 occurances in Sustainability preoccupations\nThere are 1146 occurances in Digital transformation\nThere are 221 occurances in Change in management\nThere are 5274 occurances in Innovation activities\nThere are 1392 occurances in Business Model\nThere are 0 occurances in Corporate social responsibility ou CSR\nThere are 0 occurances in marco-label\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                        Sustainability preoccupations  \\\nSustainability preoccupations                                    1373   \nDigital transformation                                              0   \nChange in management                                                0   \nInnovation activities                                               0   \nBusiness Model                                                      0   \nCorporate social responsibility ou CSR                              0   \nmarco-label                                                         0   \nO                                                                 364   \n\n                                        Digital transformation  \\\nSustainability preoccupations                                0   \nDigital transformation                                     931   \nChange in management                                         0   \nInnovation activities                                        0   \nBusiness Model                                               0   \nCorporate social responsibility ou CSR                       0   \nmarco-label                                                  0   \nO                                                          215   \n\n                                        Change in management  \\\nSustainability preoccupations                              0   \nDigital transformation                                     0   \nChange in management                                     164   \nInnovation activities                                      3   \nBusiness Model                                             0   \nCorporate social responsibility ou CSR                     0   \nmarco-label                                                0   \nO                                                         54   \n\n                                        Innovation activities  Business Model  \\\nSustainability preoccupations                               0               0   \nDigital transformation                                      0               0   \nChange in management                                        1               0   \nInnovation activities                                    4237               1   \nBusiness Model                                              0            1252   \nCorporate social responsibility ou CSR                      0               0   \nmarco-label                                                 0               1   \nO                                                        1036             138   \n\n                                        Corporate social responsibility ou CSR  \\\nSustainability preoccupations                                                0   \nDigital transformation                                                       0   \nChange in management                                                         0   \nInnovation activities                                                        0   \nBusiness Model                                                               0   \nCorporate social responsibility ou CSR                                       0   \nmarco-label                                                                  0   \nO                                                                            0   \n\n                                        marco-label    O  \nSustainability preoccupations                     0  152  \nDigital transformation                            0  181  \nChange in management                              0   35  \nInnovation activities                             0  978  \nBusiness Model                                    0   41  \nCorporate social responsibility ou CSR            0   35  \nmarco-label                                       0    0  \nO                                                 0    0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sustainability preoccupations</th>\n      <th>Digital transformation</th>\n      <th>Change in management</th>\n      <th>Innovation activities</th>\n      <th>Business Model</th>\n      <th>Corporate social responsibility ou CSR</th>\n      <th>marco-label</th>\n      <th>O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sustainability preoccupations</th>\n      <td>1373</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>Digital transformation</th>\n      <td>0</td>\n      <td>931</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>181</td>\n    </tr>\n    <tr>\n      <th>Change in management</th>\n      <td>0</td>\n      <td>0</td>\n      <td>164</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>Innovation activities</th>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>4237</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>978</td>\n    </tr>\n    <tr>\n      <th>Business Model</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1252</td>\n      <td>0</td>\n      <td>0</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>Corporate social responsibility ou CSR</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>marco-label</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>O</th>\n      <td>364</td>\n      <td>215</td>\n      <td>54</td>\n      <td>1036</td>\n      <td>138</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"In Epoch 2:\nThe distribution of labels is:\nThere are 1525 occurances in Sustainability preoccupations\nThere are 1112 occurances in Digital transformation\nThere are 200 occurances in Change in management\nThere are 5219 occurances in Innovation activities\nThere are 1293 occurances in Business Model\nThere are 35 occurances in Corporate social responsibility ou CSR\nThere are 1 occurances in marco-label\nThe distribution of predictions is:\nThere are 1890 occurances in Sustainability preoccupations\nThere are 1341 occurances in Digital transformation\nThere are 209 occurances in Change in management\nThere are 5800 occurances in Innovation activities\nThere are 1349 occurances in Business Model\nThere are 22 occurances in Corporate social responsibility ou CSR\nThere are 0 occurances in marco-label\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                        Sustainability preoccupations  \\\nSustainability preoccupations                                    1447   \nDigital transformation                                              0   \nChange in management                                                0   \nInnovation activities                                               0   \nBusiness Model                                                      0   \nCorporate social responsibility ou CSR                              0   \nmarco-label                                                         0   \nO                                                                 443   \n\n                                        Digital transformation  \\\nSustainability preoccupations                                0   \nDigital transformation                                    1049   \nChange in management                                         0   \nInnovation activities                                        0   \nBusiness Model                                               0   \nCorporate social responsibility ou CSR                       0   \nmarco-label                                                  0   \nO                                                          292   \n\n                                        Change in management  \\\nSustainability preoccupations                              0   \nDigital transformation                                     0   \nChange in management                                     172   \nInnovation activities                                      1   \nBusiness Model                                             0   \nCorporate social responsibility ou CSR                     0   \nmarco-label                                                0   \nO                                                         36   \n\n                                        Innovation activities  Business Model  \\\nSustainability preoccupations                               0               0   \nDigital transformation                                      0               0   \nChange in management                                        0               0   \nInnovation activities                                    4736               0   \nBusiness Model                                              0            1223   \nCorporate social responsibility ou CSR                      0               0   \nmarco-label                                                 0               1   \nO                                                        1064             125   \n\n                                        Corporate social responsibility ou CSR  \\\nSustainability preoccupations                                                0   \nDigital transformation                                                       0   \nChange in management                                                         0   \nInnovation activities                                                        0   \nBusiness Model                                                               0   \nCorporate social responsibility ou CSR                                      15   \nmarco-label                                                                  0   \nO                                                                            7   \n\n                                        marco-label    O  \nSustainability preoccupations                     0   78  \nDigital transformation                            0   63  \nChange in management                              0   28  \nInnovation activities                             0  482  \nBusiness Model                                    0   70  \nCorporate social responsibility ou CSR            0   20  \nmarco-label                                       0    0  \nO                                                 0    0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sustainability preoccupations</th>\n      <th>Digital transformation</th>\n      <th>Change in management</th>\n      <th>Innovation activities</th>\n      <th>Business Model</th>\n      <th>Corporate social responsibility ou CSR</th>\n      <th>marco-label</th>\n      <th>O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sustainability preoccupations</th>\n      <td>1447</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>Digital transformation</th>\n      <td>0</td>\n      <td>1049</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>Change in management</th>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>Innovation activities</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4736</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>482</td>\n    </tr>\n    <tr>\n      <th>Business Model</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1223</td>\n      <td>0</td>\n      <td>0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>Corporate social responsibility ou CSR</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>15</td>\n      <td>0</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>marco-label</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>O</th>\n      <td>443</td>\n      <td>292</td>\n      <td>36</td>\n      <td>1064</td>\n      <td>125</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"In Epoch 3:\nThe distribution of labels is:\nThere are 1525 occurances in Sustainability preoccupations\nThere are 1112 occurances in Digital transformation\nThere are 200 occurances in Change in management\nThere are 5219 occurances in Innovation activities\nThere are 1293 occurances in Business Model\nThere are 35 occurances in Corporate social responsibility ou CSR\nThere are 1 occurances in marco-label\nThe distribution of predictions is:\nThere are 1685 occurances in Sustainability preoccupations\nThere are 1346 occurances in Digital transformation\nThere are 276 occurances in Change in management\nThere are 5444 occurances in Innovation activities\nThere are 1320 occurances in Business Model\nThere are 26 occurances in Corporate social responsibility ou CSR\nThere are 0 occurances in marco-label\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                        Sustainability preoccupations  \\\nSustainability preoccupations                                    1390   \nDigital transformation                                              0   \nChange in management                                                0   \nInnovation activities                                               0   \nBusiness Model                                                      0   \nCorporate social responsibility ou CSR                              0   \nmarco-label                                                         0   \nO                                                                 295   \n\n                                        Digital transformation  \\\nSustainability preoccupations                                0   \nDigital transformation                                    1054   \nChange in management                                         0   \nInnovation activities                                        0   \nBusiness Model                                               0   \nCorporate social responsibility ou CSR                       0   \nmarco-label                                                  0   \nO                                                          292   \n\n                                        Change in management  \\\nSustainability preoccupations                              0   \nDigital transformation                                     0   \nChange in management                                     193   \nInnovation activities                                      3   \nBusiness Model                                             0   \nCorporate social responsibility ou CSR                     0   \nmarco-label                                                0   \nO                                                         80   \n\n                                        Innovation activities  Business Model  \\\nSustainability preoccupations                               0               0   \nDigital transformation                                      0               0   \nChange in management                                        0               0   \nInnovation activities                                    4632               0   \nBusiness Model                                              0            1226   \nCorporate social responsibility ou CSR                      0               0   \nmarco-label                                                 0               0   \nO                                                         812              94   \n\n                                        Corporate social responsibility ou CSR  \\\nSustainability preoccupations                                                0   \nDigital transformation                                                       0   \nChange in management                                                         0   \nInnovation activities                                                        0   \nBusiness Model                                                               0   \nCorporate social responsibility ou CSR                                      14   \nmarco-label                                                                  0   \nO                                                                           12   \n\n                                        marco-label    O  \nSustainability preoccupations                     0  135  \nDigital transformation                            0   58  \nChange in management                              0    7  \nInnovation activities                             0  584  \nBusiness Model                                    0   67  \nCorporate social responsibility ou CSR            0   21  \nmarco-label                                       0    1  \nO                                                 0    0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sustainability preoccupations</th>\n      <th>Digital transformation</th>\n      <th>Change in management</th>\n      <th>Innovation activities</th>\n      <th>Business Model</th>\n      <th>Corporate social responsibility ou CSR</th>\n      <th>marco-label</th>\n      <th>O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sustainability preoccupations</th>\n      <td>1390</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>135</td>\n    </tr>\n    <tr>\n      <th>Digital transformation</th>\n      <td>0</td>\n      <td>1054</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>Change in management</th>\n      <td>0</td>\n      <td>0</td>\n      <td>193</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>Innovation activities</th>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>4632</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>584</td>\n    </tr>\n    <tr>\n      <th>Business Model</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1226</td>\n      <td>0</td>\n      <td>0</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>Corporate social responsibility ou CSR</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>marco-label</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>O</th>\n      <td>295</td>\n      <td>292</td>\n      <td>80</td>\n      <td>812</td>\n      <td>94</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"In Epoch 4:\nThe distribution of labels is:\nThere are 1525 occurances in Sustainability preoccupations\nThere are 1112 occurances in Digital transformation\nThere are 200 occurances in Change in management\nThere are 5219 occurances in Innovation activities\nThere are 1293 occurances in Business Model\nThere are 35 occurances in Corporate social responsibility ou CSR\nThere are 1 occurances in marco-label\nThe distribution of predictions is:\nThere are 1624 occurances in Sustainability preoccupations\nThere are 1247 occurances in Digital transformation\nThere are 236 occurances in Change in management\nThere are 5877 occurances in Innovation activities\nThere are 1363 occurances in Business Model\nThere are 5 occurances in Corporate social responsibility ou CSR\nThere are 0 occurances in marco-label\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                        Sustainability preoccupations  \\\nSustainability preoccupations                                    1401   \nDigital transformation                                              0   \nChange in management                                                0   \nInnovation activities                                               0   \nBusiness Model                                                      0   \nCorporate social responsibility ou CSR                              0   \nmarco-label                                                         0   \nO                                                                 223   \n\n                                        Digital transformation  \\\nSustainability preoccupations                                0   \nDigital transformation                                    1031   \nChange in management                                         0   \nInnovation activities                                        0   \nBusiness Model                                               0   \nCorporate social responsibility ou CSR                       0   \nmarco-label                                                  0   \nO                                                          216   \n\n                                        Change in management  \\\nSustainability preoccupations                              0   \nDigital transformation                                     0   \nChange in management                                     192   \nInnovation activities                                      1   \nBusiness Model                                             0   \nCorporate social responsibility ou CSR                     0   \nmarco-label                                                0   \nO                                                         43   \n\n                                        Innovation activities  Business Model  \\\nSustainability preoccupations                               0               0   \nDigital transformation                                      0               0   \nChange in management                                        0               0   \nInnovation activities                                    4892               0   \nBusiness Model                                              0            1253   \nCorporate social responsibility ou CSR                      0               0   \nmarco-label                                                 0               0   \nO                                                         985             110   \n\n                                        Corporate social responsibility ou CSR  \\\nSustainability preoccupations                                                0   \nDigital transformation                                                       0   \nChange in management                                                         0   \nInnovation activities                                                        0   \nBusiness Model                                                               0   \nCorporate social responsibility ou CSR                                       3   \nmarco-label                                                                  0   \nO                                                                            2   \n\n                                        marco-label    O  \nSustainability preoccupations                     0  124  \nDigital transformation                            0   81  \nChange in management                              0    8  \nInnovation activities                             0  326  \nBusiness Model                                    0   40  \nCorporate social responsibility ou CSR            0   32  \nmarco-label                                       0    1  \nO                                                 0    0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sustainability preoccupations</th>\n      <th>Digital transformation</th>\n      <th>Change in management</th>\n      <th>Innovation activities</th>\n      <th>Business Model</th>\n      <th>Corporate social responsibility ou CSR</th>\n      <th>marco-label</th>\n      <th>O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sustainability preoccupations</th>\n      <td>1401</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>Digital transformation</th>\n      <td>0</td>\n      <td>1031</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>Change in management</th>\n      <td>0</td>\n      <td>0</td>\n      <td>192</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>Innovation activities</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4892</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>326</td>\n    </tr>\n    <tr>\n      <th>Business Model</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1253</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>Corporate social responsibility ou CSR</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>marco-label</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>O</th>\n      <td>223</td>\n      <td>216</td>\n      <td>43</td>\n      <td>985</td>\n      <td>110</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"time: 6.74 s (started: 2022-07-29 14:55:23 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# ./bert-base-cased-keywords-epoch4-9vs1.pkl\n# ./bert-base-cased-new-keywords-epoch4-9vs1.csv\n\n# ./bert-base-cased-keywords-epoch4-dropout5.pkl\n# ./bert-base-cased-new-keywords-epoch4-dropout5.csv\n# ./bert-base-cased-loss-acc-epoch4-dropout5.csv\n# ./model_epoch4_dropout5\n\n# Best\n# ./bert-base-cased-keywords-epoch4-dropout2.pkl\n# ./bert-base-cased-new-keywords-epoch4-dropout2.csv\n# ./bert-base-cased-loss-acc-epoch4-dropout2.csv\n# ./model_epoch4_dropout2\n\n# semantic\n# ./bert-base-cased-keywords-epoch4-semantic2.pkl\n# ./bert-base-cased-new-keywords-epoch4-semantic2.csv\n# ./bert-base-cased-loss-acc-epoch4-semantic2.csv\n# ./model_epoch4_semantic2\n\nimport pickle\n\n# store\n# label_pred_dict\n\nwith open(\"./distilbert-label-pred-dict.pkl\", \"wb\") as f:\n    pickle.dump(label_pred_dict, f)\n    \nwith open(\"./distilbert-keywords-epoch4-semantic2.pkl\", \"wb\") as f:\n    pickle.dump(new_kws_dict, f)\n\nnew_kw_df.to_csv(\"./distilbert-new-keywords-epoch4-semantic2.csv\")  \n\nresultdf.to_csv(\"./distilbert-loss-acc-epoch4-semantic2.csv\")\n\nmodel2.save_pretrained(\"./distilbert_epoch4_semantic2\")","metadata":{"execution":{"iopub.status.busy":"2022-07-29T14:58:30.215895Z","iopub.execute_input":"2022-07-29T14:58:30.216404Z","iopub.status.idle":"2022-07-29T14:58:30.313949Z","shell.execute_reply.started":"2022-07-29T14:58:30.216301Z","shell.execute_reply":"2022-07-29T14:58:30.311933Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_2110/3213756645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./distilbert-label-pred-dict.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_pred_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./distilbert-keywords-epoch4-semantic2.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'label_pred_dict' is not defined"],"ename":"NameError","evalue":"name 'label_pred_dict' is not defined","output_type":"error"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/bert-base-cased-new-keywords-epoch4-9vs1.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2022-07-21T10:02:01.872719Z","iopub.execute_input":"2022-07-21T10:02:01.873237Z","iopub.status.idle":"2022-07-21T10:02:01.920184Z","shell.execute_reply.started":"2022-07-21T10:02:01.873187Z","shell.execute_reply":"2022-07-21T10:02:01.918530Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/4045858675.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/bert-base-cased-new-keywords-epoch4-9vs1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/bert-base-cased-new-keywords-epoch4-9vs1.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/bert-base-cased-new-keywords-epoch4-9vs1.csv'","output_type":"error"},{"name":"stdout","text":"time: 40.4 ms (started: 2022-07-21 10:02:01 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Test on Generalizability","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test on Annual Report","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN - from notebook","metadata":{}},{"cell_type":"code","source":"import functools\nimport sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchtext\nimport tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:14.196295Z","iopub.execute_input":"2022-08-11T20:50:14.196648Z","iopub.status.idle":"2022-08-11T20:50:14.285985Z","shell.execute_reply.started":"2022-08-11T20:50:14.196613Z","shell.execute_reply":"2022-08-11T20:50:14.284957Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"cuda\ntime: 84.4 ms (started: 2022-08-11 20:50:14 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"seed = 0\n\ntorch.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:14.288700Z","iopub.execute_input":"2022-08-11T20:50:14.289144Z","iopub.status.idle":"2022-08-11T20:50:14.305342Z","shell.execute_reply.started":"2022-08-11T20:50:14.289116Z","shell.execute_reply":"2022-08-11T20:50:14.304177Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f71b0aa4b50>"},"metadata":{}},{"name":"stdout","text":"time: 8.02 ms (started: 2022-08-11 20:50:14 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from allennlp.modules import ConditionalRandomField","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:14.306971Z","iopub.execute_input":"2022-08-11T20:50:14.307611Z","iopub.status.idle":"2022-08-11T20:50:14.854578Z","shell.execute_reply.started":"2022-08-11T20:50:14.307577Z","shell.execute_reply":"2022-08-11T20:50:14.853509Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"time: 542 ms (started: 2022-08-11 20:50:14 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"class CNN_n(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, output_dim, dropout_rate, pad_index, crf=False):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n        self.conv1=torch.nn.Conv1d(embedding_dim, 128, 5, padding=2)\n        self.conv2=torch.nn.Conv1d(embedding_dim, 128, 3, padding=1)\n        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n        self.conv4=torch.nn.Conv1d(256, 256, 5, padding=2)\n        self.conv5=torch.nn.Conv1d(256, 256, 5, padding=2)\n        self.fc = nn.Linear(256, output_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.crf_flag=crf\n        if self.crf_flag:\n            self.crf = ConditionalRandomField(output_dim) \n        \n    def forward(self, ids, ids_len, tag, is_training=True):\n        # ids = [batch size, seq len]\n#         print(f\"ids: {ids.size()}\")\n        embedded = self.dropout(self.embedding(ids))\n#         print(f\"embedding: {embedded.size()}\")\n        # embedded = [batch size, seq len, embedding dim]\n        conved = embedded.transpose(1, 2)\n#         print(f\"after permute: {conved.size()}\")\n        # embedded = [batch size, embedding dim, seq len]\n        x_conv=torch.nn.functional.relu(torch.cat((self.conv1(conved), self.conv2(conved)), dim=1))\n#         x_conv=torch.nn.functional.relu(self.conv1(conved))\n        x_conv=self.dropout(x_conv)\n        x_conv=torch.nn.functional.relu(self.conv3(x_conv))\n        x_conv=self.dropout(x_conv)\n        x_conv=torch.nn.functional.relu(self.conv4(x_conv))\n        x_conv=self.dropout(x_conv)\n        x_conv=torch.nn.functional.relu(self.conv5(x_conv))\n        conved=x_conv.transpose(1, 2)\n        logit = self.fc(conved)\n        if not is_training:\n            if self.crf_flag:\n                score = self.crf.viterbi_tags(logit)\n            else:\n                x_logit = logit.transpose(2, 0)\n                score = torch.nn.functional.log_softmax(x_logit).transpose(2, 0)\n        else:\n            if self.crf_flag:\n                score = - self.crf(logit, tag)\n            else:\n                x_logit = torch.nn.utils.rnn.pack_padded_sequence(logit, ids_len, batch_first=True)\n                score = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(x_logit.data), tag.data)\n        return score","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:14.856178Z","iopub.execute_input":"2022-08-11T20:50:14.857512Z","iopub.status.idle":"2022-08-11T20:50:14.873537Z","shell.execute_reply.started":"2022-08-11T20:50:14.857467Z","shell.execute_reply":"2022-08-11T20:50:14.872441Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"time: 2.29 ms (started: 2022-08-11 20:50:14 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchtext\ndef tokenize_example(example, max_length=300):\n    return example.split(' ')[:max_length]\ndef split_tags(example, max_length=300):\n    return example.split(',')[:max_length]\n\n# data2 = data2.sample(200)\n\ndata2['tokens'] = data2['sentence'].map(tokenize_example)\ndata2['labels'] = data2['word_labels2'].map(split_tags)\n\n# K-fold\nk_fold = 5\ncur_fold = 3 # [3]\ngap = int(len(data2) / k_fold)\nindices = [[i*gap, (i+1)*gap if i+1 != k_fold else len(data2)] \n          for i in range(k_fold)]\ndata3 = data2.sample(frac=1, random_state=200)\ntest_dataset = data3[indices[cur_fold][0]:indices[cur_fold][1]]\ntrain_dataset = data3.drop(test_dataset.index).reset_index(drop=True)\ntest_dataset = test_dataset.reset_index(drop=True)\n\nmin_freq = 3\nspecial_tokens = ['<unk>', '<pad>']\n\nvocab = torchtext.vocab.build_vocab_from_iterator(train_dataset['tokens'],\n                                                  min_freq=min_freq,\n                                                  specials=special_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:14.874976Z","iopub.execute_input":"2022-08-11T20:50:14.875419Z","iopub.status.idle":"2022-08-11T20:50:17.085235Z","shell.execute_reply.started":"2022-08-11T20:50:14.875376Z","shell.execute_reply":"2022-08-11T20:50:17.084061Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"time: 2.2 s (started: 2022-08-11 20:50:14 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"unk_index = vocab['<unk>']\npad_index = vocab['<pad>']\nvocab.set_default_index(unk_index)\n\ndef numericalize_data(example, vocab=vocab):\n    return [vocab[token] for token in example]\n\ndef numericalize_label(example, vocab=labels_to_ids2):\n    return [vocab[token] for token in example]\n\ntrain_dataset['ids'] = train_dataset['tokens'].map(numericalize_data)\ntrain_dataset['labels2'] = train_dataset['labels'].map(numericalize_label)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:17.086580Z","iopub.execute_input":"2022-08-11T20:50:17.088174Z","iopub.status.idle":"2022-08-11T20:50:20.185380Z","shell.execute_reply.started":"2022-08-11T20:50:17.088124Z","shell.execute_reply":"2022-08-11T20:50:20.184305Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"time: 3.09 s (started: 2022-08-11 20:50:17 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 300\n# n_filters = 100\n# filter_sizes = [5,3,5,5]\noutput_dim = len(labels_to_ids2)\ndropout_rate = 0.5\n\nmodel = CNN_n(vocab_size, embedding_dim, output_dim, dropout_rate, pad_index, crf=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:20.187019Z","iopub.execute_input":"2022-08-11T20:50:20.187395Z","iopub.status.idle":"2022-08-11T20:50:20.303421Z","shell.execute_reply.started":"2022-08-11T20:50:20.187357Z","shell.execute_reply":"2022-08-11T20:50:20.302299Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"time: 111 ms (started: 2022-08-11 20:50:20 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:20.305020Z","iopub.execute_input":"2022-08-11T20:50:20.305373Z","iopub.status.idle":"2022-08-11T20:50:20.313390Z","shell.execute_reply.started":"2022-08-11T20:50:20.305337Z","shell.execute_reply":"2022-08-11T20:50:20.311315Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"The model has 11,042,800 trainable parameters\ntime: 793 µs (started: 2022-08-11 20:50:20 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def initialize_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Conv1d):\n        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n        nn.init.zeros_(m.bias)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:20.315202Z","iopub.execute_input":"2022-08-11T20:50:20.316375Z","iopub.status.idle":"2022-08-11T20:50:20.324507Z","shell.execute_reply.started":"2022-08-11T20:50:20.316331Z","shell.execute_reply":"2022-08-11T20:50:20.323253Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"time: 1.12 ms (started: 2022-08-11 20:50:20 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.apply(initialize_weights)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:20.326729Z","iopub.execute_input":"2022-08-11T20:50:20.327182Z","iopub.status.idle":"2022-08-11T20:50:20.349765Z","shell.execute_reply.started":"2022-08-11T20:50:20.327146Z","shell.execute_reply":"2022-08-11T20:50:20.348903Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CNN_n(\n  (embedding): Embedding(32498, 300, padding_idx=1)\n  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n  (conv2): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n  (fc): Linear(in_features=256, out_features=8, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (crf): ConditionalRandomField()\n)"},"metadata":{}},{"name":"stdout","text":"time: 16.5 ms (started: 2022-08-11 20:50:20 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"vectors = torchtext.vocab.FastText()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:50:20.350929Z","iopub.execute_input":"2022-08-11T20:50:20.351949Z","iopub.status.idle":"2022-08-11T20:59:39.690402Z","shell.execute_reply.started":"2022-08-11T20:50:20.351912Z","shell.execute_reply":"2022-08-11T20:59:39.689022Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":".vector_cache/wiki.en.vec: 6.60GB [03:50, 28.6MB/s]                                \n100%|██████████| 2519370/2519370 [04:33<00:00, 9218.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"time: 9min 19s (started: 2022-08-11 20:50:20 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:59:39.693119Z","iopub.execute_input":"2022-08-11T20:59:39.694663Z","iopub.status.idle":"2022-08-11T20:59:39.976994Z","shell.execute_reply.started":"2022-08-11T20:59:39.694619Z","shell.execute_reply":"2022-08-11T20:59:39.975954Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"time: 275 ms (started: 2022-08-11 20:59:39 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.embedding.weight.data = pretrained_embedding\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n# criterion = nn.CrossEntropyLoss()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n# criterion = criterion.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:59:39.978711Z","iopub.execute_input":"2022-08-11T20:59:39.979096Z","iopub.status.idle":"2022-08-11T20:59:40.091134Z","shell.execute_reply.started":"2022-08-11T20:59:39.979059Z","shell.execute_reply":"2022-08-11T20:59:40.089928Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"time: 107 ms (started: 2022-08-11 20:59:39 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\ndef sent2ids(text):\n    return [vocab[w] for w in text]\n\ndef labels2ids(labels):\n    return [labels_to_ids2[w] for w in labels]\n\ndef data_generator(sents, labels, batch_size=32, is_training=True, index=0):\n    if is_training:\n        select_indices = np.random.choice(len(sents), batch_size, replace=False)\n    else:\n        start = index\n        end = min(start + batch_size, len(sents)) \n        select_indices = list(range(start, end))\n    #select_indices = list(range(batch_size))\n    batch_sents = np.array(sents)[select_indices]\n    batch_labels = np.array(labels)[select_indices]\n    \n    batch_sents = list(map(sent2ids, batch_sents))\n    batch_labels = list(map(labels2ids, batch_labels))\n    \n    seq_lens = [len(s) for s in batch_sents]\n    seq_lens = torch.LongTensor(seq_lens)\n    max_len = max(seq_lens)\n    \n    batch_sents = [torch.LongTensor(s) for s in batch_sents]\n    \n    batch_sents = pad_sequence(batch_sents, batch_first=True)\n\n    if not is_training:\n        return batch_sents, batch_labels, seq_lens, end\n    batch_labels = [torch.LongTensor(s) for s in batch_labels]\n    batch_labels = pad_sequence(batch_labels, batch_first=True)\n  \n    return batch_sents, batch_labels, seq_lens","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:59:40.093677Z","iopub.execute_input":"2022-08-11T20:59:40.094827Z","iopub.status.idle":"2022-08-11T20:59:40.105819Z","shell.execute_reply.started":"2022-08-11T20:59:40.094789Z","shell.execute_reply":"2022-08-11T20:59:40.104639Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"time: 1.64 ms (started: 2022-08-11 20:59:40 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\ntrain_sent_words = train_dataset['tokens']\ntrain_sent_tags = train_dataset['labels']\nepoch = 40\nloop_num = int(len(train_sent_words)/100)\nprint(f\"The loop numbers: {loop_num}\")\nfor i in range(epoch):\n    print(f\"Epoch {i}:\")\n    start = time.time()\n    model.train()\n    for j in range(loop_num):\n        optimizer.zero_grad()\n        batch_sents, batch_tags, seq_lens, = data_generator(train_sent_words, train_sent_tags, batch_size=100)\n        loss = model(batch_sents.to(device), seq_lens.to(device), batch_tags.to(device))\n        loss.backward()\n        optimizer.step()\n        if j % 100 == 0:\n            print(f'Loss: {loss.item()} \\t Cost time per step: {time.time() - start}')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:59:40.107406Z","iopub.execute_input":"2022-08-11T20:59:40.108616Z","iopub.status.idle":"2022-08-11T21:49:34.241521Z","shell.execute_reply.started":"2022-08-11T20:59:40.108579Z","shell.execute_reply":"2022-08-11T21:49:34.240471Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"The loop numbers: 263\nEpoch 0:\nLoss: 61739.8125 \t Cost time per step: 7.18489146232605\nLoss: 1653.6947021484375 \t Cost time per step: 35.791404485702515\nLoss: 734.48193359375 \t Cost time per step: 64.74219727516174\nEpoch 1:\nLoss: 742.416748046875 \t Cost time per step: 0.279463529586792\nLoss: 594.3251953125 \t Cost time per step: 28.63482689857483\nLoss: 442.1259765625 \t Cost time per step: 57.19734048843384\nEpoch 2:\nLoss: 452.62451171875 \t Cost time per step: 0.2781364917755127\nLoss: 365.93505859375 \t Cost time per step: 28.850520372390747\nLoss: 306.5361328125 \t Cost time per step: 57.527618169784546\nEpoch 3:\nLoss: 361.11669921875 \t Cost time per step: 0.2790236473083496\nLoss: 325.17822265625 \t Cost time per step: 28.39827275276184\nLoss: 209.03857421875 \t Cost time per step: 56.48990345001221\nEpoch 4:\nLoss: 243.12548828125 \t Cost time per step: 0.2745969295501709\nLoss: 267.0947265625 \t Cost time per step: 28.796079635620117\nLoss: 200.029296875 \t Cost time per step: 57.55385661125183\nEpoch 5:\nLoss: 229.01708984375 \t Cost time per step: 0.2755155563354492\nLoss: 206.7216796875 \t Cost time per step: 28.70398783683777\nLoss: 165.43408203125 \t Cost time per step: 57.239503383636475\nEpoch 6:\nLoss: 213.80517578125 \t Cost time per step: 0.2807145118713379\nLoss: 194.33837890625 \t Cost time per step: 28.82231307029724\nLoss: 186.59521484375 \t Cost time per step: 57.33422017097473\nEpoch 7:\nLoss: 151.6845703125 \t Cost time per step: 0.34940242767333984\nLoss: 130.796875 \t Cost time per step: 29.41402578353882\nLoss: 181.4189453125 \t Cost time per step: 57.9014413356781\nEpoch 8:\nLoss: 127.54931640625 \t Cost time per step: 0.2754848003387451\nLoss: 152.28125 \t Cost time per step: 29.38334560394287\nLoss: 121.7568359375 \t Cost time per step: 57.829728841781616\nEpoch 9:\nLoss: 178.48876953125 \t Cost time per step: 0.2766444683074951\nLoss: 164.138671875 \t Cost time per step: 28.351929187774658\nLoss: 137.32373046875 \t Cost time per step: 56.83839201927185\nEpoch 10:\nLoss: 166.43017578125 \t Cost time per step: 0.2765793800354004\nLoss: 133.89990234375 \t Cost time per step: 29.096745014190674\nLoss: 101.38623046875 \t Cost time per step: 57.31359267234802\nEpoch 11:\nLoss: 90.93603515625 \t Cost time per step: 0.2741577625274658\nLoss: 116.3798828125 \t Cost time per step: 28.98332905769348\nLoss: 158.83251953125 \t Cost time per step: 57.21928834915161\nEpoch 12:\nLoss: 95.78125 \t Cost time per step: 0.27306342124938965\nLoss: 109.11181640625 \t Cost time per step: 28.43183183670044\nLoss: 152.87060546875 \t Cost time per step: 56.80880880355835\nEpoch 13:\nLoss: 138.24853515625 \t Cost time per step: 0.7743105888366699\nLoss: 145.14208984375 \t Cost time per step: 29.58352541923523\nLoss: 105.0224609375 \t Cost time per step: 57.6673219203949\nEpoch 14:\nLoss: 130.8134765625 \t Cost time per step: 0.27417564392089844\nLoss: 93.59375 \t Cost time per step: 28.952290534973145\nLoss: 61.98095703125 \t Cost time per step: 57.103296518325806\nEpoch 15:\nLoss: 104.0615234375 \t Cost time per step: 0.28374743461608887\nLoss: 105.14794921875 \t Cost time per step: 29.71083116531372\nLoss: 77.54296875 \t Cost time per step: 58.74275851249695\nEpoch 16:\nLoss: 90.96435546875 \t Cost time per step: 0.28618860244750977\nLoss: 106.8779296875 \t Cost time per step: 29.627843856811523\nLoss: 106.232421875 \t Cost time per step: 57.845083475112915\nEpoch 17:\nLoss: 108.0419921875 \t Cost time per step: 0.2730083465576172\nLoss: 90.462890625 \t Cost time per step: 28.291178703308105\nLoss: 80.1708984375 \t Cost time per step: 56.5916063785553\nEpoch 18:\nLoss: 81.57373046875 \t Cost time per step: 0.27554845809936523\nLoss: 85.23974609375 \t Cost time per step: 28.718992710113525\nLoss: 87.11376953125 \t Cost time per step: 57.405003786087036\nEpoch 19:\nLoss: 76.75732421875 \t Cost time per step: 0.27504611015319824\nLoss: 88.34814453125 \t Cost time per step: 28.47652506828308\nLoss: 67.82177734375 \t Cost time per step: 56.809170722961426\nEpoch 20:\nLoss: 68.341796875 \t Cost time per step: 0.27433061599731445\nLoss: 75.86328125 \t Cost time per step: 28.662689447402954\nLoss: 79.68896484375 \t Cost time per step: 56.76159858703613\nEpoch 21:\nLoss: 83.0830078125 \t Cost time per step: 0.276822566986084\nLoss: 92.09521484375 \t Cost time per step: 28.607691287994385\nLoss: 68.5048828125 \t Cost time per step: 56.997284173965454\nEpoch 22:\nLoss: 75.69140625 \t Cost time per step: 0.3029208183288574\nLoss: 60.5107421875 \t Cost time per step: 28.43920636177063\nLoss: 79.10009765625 \t Cost time per step: 56.272026777267456\nEpoch 23:\nLoss: 34.59228515625 \t Cost time per step: 0.27790236473083496\nLoss: 52.77880859375 \t Cost time per step: 28.714763879776\nLoss: 44.9736328125 \t Cost time per step: 56.86537575721741\nEpoch 24:\nLoss: 63.28466796875 \t Cost time per step: 0.27694153785705566\nLoss: 57.830078125 \t Cost time per step: 28.604610681533813\nLoss: 93.630859375 \t Cost time per step: 56.54666519165039\nEpoch 25:\nLoss: 56.166015625 \t Cost time per step: 0.275235652923584\nLoss: 52.0986328125 \t Cost time per step: 29.017441511154175\nLoss: 69.39111328125 \t Cost time per step: 57.080304861068726\nEpoch 26:\nLoss: 47.5703125 \t Cost time per step: 0.27695274353027344\nLoss: 61.89453125 \t Cost time per step: 28.311084270477295\nLoss: 42.8525390625 \t Cost time per step: 56.73066186904907\nEpoch 27:\nLoss: 48.423828125 \t Cost time per step: 0.3278162479400635\nLoss: 47.0244140625 \t Cost time per step: 28.304293632507324\nLoss: 35.775390625 \t Cost time per step: 57.281359910964966\nEpoch 28:\nLoss: 55.85400390625 \t Cost time per step: 0.2725865840911865\nLoss: 50.3916015625 \t Cost time per step: 28.55891251564026\nLoss: 37.05517578125 \t Cost time per step: 57.38156294822693\nEpoch 29:\nLoss: 34.99365234375 \t Cost time per step: 0.39331674575805664\nLoss: 86.79052734375 \t Cost time per step: 28.730916023254395\nLoss: 54.9716796875 \t Cost time per step: 56.92900347709656\nEpoch 30:\nLoss: 50.83056640625 \t Cost time per step: 0.2789914608001709\nLoss: 34.0068359375 \t Cost time per step: 28.443423748016357\nLoss: 51.71484375 \t Cost time per step: 56.48994755744934\nEpoch 31:\nLoss: 52.42138671875 \t Cost time per step: 0.274677038192749\nLoss: 59.83935546875 \t Cost time per step: 28.27757740020752\nLoss: 28.9755859375 \t Cost time per step: 56.43492841720581\nEpoch 32:\nLoss: 52.7255859375 \t Cost time per step: 0.2750117778778076\nLoss: 55.9228515625 \t Cost time per step: 28.40640115737915\nLoss: 53.39453125 \t Cost time per step: 56.56344747543335\nEpoch 33:\nLoss: 38.125 \t Cost time per step: 0.29558277130126953\nLoss: 46.517578125 \t Cost time per step: 28.139970541000366\nLoss: 33.9482421875 \t Cost time per step: 56.519572019577026\nEpoch 34:\nLoss: 33.3701171875 \t Cost time per step: 0.27744174003601074\nLoss: 40.77197265625 \t Cost time per step: 28.35417127609253\nLoss: 48.6220703125 \t Cost time per step: 56.91278433799744\nEpoch 35:\nLoss: 53.27978515625 \t Cost time per step: 0.2736380100250244\nLoss: 65.5458984375 \t Cost time per step: 28.45748805999756\nLoss: 26.77685546875 \t Cost time per step: 57.119892597198486\nEpoch 36:\nLoss: 53.25830078125 \t Cost time per step: 0.27538371086120605\nLoss: 23.8046875 \t Cost time per step: 28.62916898727417\nLoss: 48.22607421875 \t Cost time per step: 56.946579456329346\nEpoch 37:\nLoss: 23.44970703125 \t Cost time per step: 0.2767984867095947\nLoss: 44.73095703125 \t Cost time per step: 28.469220876693726\nLoss: 29.4609375 \t Cost time per step: 56.421138048172\nEpoch 38:\nLoss: 33.998046875 \t Cost time per step: 0.2739288806915283\nLoss: 36.84228515625 \t Cost time per step: 28.374417543411255\nLoss: 28.91162109375 \t Cost time per step: 56.58255958557129\nEpoch 39:\nLoss: 27.837890625 \t Cost time per step: 0.2725512981414795\nLoss: 24.505859375 \t Cost time per step: 28.064003229141235\nLoss: 47.2021484375 \t Cost time per step: 56.31438732147217\ntime: 49min 54s (started: 2022-08-11 20:59:40 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def tags_to_keywords(sample, words):\n    indices = [i for i, l in enumerate(sample) if l != 'O']\n    keywords, key_cats = [], []\n    for j, id in enumerate(indices):\n        if j == 0:\n            start = end = id\n            continue\n        if j == len(indices):\n            pos = pos_tag(words[start:end+1])\n            if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n                keywords.append(' '.join(words[start:end+1]))\n                key_cats.append((' '.join(words[start:end+1]), sample[start:end+1]))\n            continue\n        if end+1 == id:\n            end = id\n        else:\n            try:\n                pos = pos_tag(words[start:end+1])\n                if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n                    keywords.append(' '.join(words[start:end+1]))\n                    key_cats.append((' '.join(words[start:end+1]), sample[start:end+1]))\n                start = end = id\n            except:\n                continue\n    return list(set(keywords)), key_cats","metadata":{"execution":{"iopub.status.busy":"2022-08-11T21:49:34.243158Z","iopub.execute_input":"2022-08-11T21:49:34.243513Z","iopub.status.idle":"2022-08-11T21:49:34.257539Z","shell.execute_reply.started":"2022-08-11T21:49:34.243477Z","shell.execute_reply":"2022-08-11T21:49:34.256553Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"time: 2.05 ms (started: 2022-08-11 21:49:34 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"dev_sent_words = test_dataset['tokens']\ndev_sent_tags = test_dataset['labels']\nmodel.eval()\nindex = 0\npred_label_list = []\nPreds, Preds_cats = [], []\npbar = tqdm.tqdm(total=len(dev_sent_words))\nwhile index < len(dev_sent_words):\n    batch_sents, batch_tags, seq_lens, index = data_generator(dev_sent_words, \n                                                              dev_sent_tags, batch_size=32, \n                                                              is_training=False, index=index)\n    pred_labels = model(batch_sents.to(device), seq_lens.to(device), batch_tags, is_training=False)\n    for i, label_seq in enumerate(pred_labels):\n        pred_labels = [ids_to_labels2[t] for t in label_seq[0]]\n        \n        preds, preds_cats = tags_to_keywords(pred_labels, list(dev_sent_words[index-32:min(index, len(dev_sent_words))])[i])\n        \n        pred_label_list.append(pred_labels)\n        Preds.append(preds)\n        Preds_cats.append(preds_cats)\n    \n    pbar.update(1000)\npbar.close()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T21:49:34.263305Z","iopub.execute_input":"2022-08-11T21:49:34.263691Z","iopub.status.idle":"2022-08-11T21:51:43.295200Z","shell.execute_reply.started":"2022-08-11T21:49:34.263658Z","shell.execute_reply":"2022-08-11T21:51:43.294215Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"206000it [02:09, 1596.72it/s]                        ","output_type":"stream"},{"name":"stdout","text":"time: 2min 9s (started: 2022-08-11 21:49:34 +00:00)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = [p for pl in pred_label_list for p in pl]\nlabels = [d for dev in dev_sent_tags for d in dev]","metadata":{"execution":{"iopub.status.busy":"2022-08-11T21:51:43.296531Z","iopub.execute_input":"2022-08-11T21:51:43.297313Z","iopub.status.idle":"2022-08-11T21:51:43.430021Z","shell.execute_reply.started":"2022-08-11T21:51:43.297273Z","shell.execute_reply":"2022-08-11T21:51:43.428927Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"time: 128 ms (started: 2022-08-11 21:51:43 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_lable_keywords = {}\npred_lable_keywords['tag_preds'] = predictions\npred_lable_keywords['tag_labels'] = labels\npred_lable_keywords['predicitons'] = Preds\npred_lable_keywords['preds_cats'] = Preds_cats\npred_lable_keywords['cnn_pred_label_list'] = pred_label_list","metadata":{"execution":{"iopub.status.busy":"2022-08-11T21:51:43.431361Z","iopub.execute_input":"2022-08-11T21:51:43.431709Z","iopub.status.idle":"2022-08-11T21:51:43.439378Z","shell.execute_reply.started":"2022-08-11T21:51:43.431673Z","shell.execute_reply":"2022-08-11T21:51:43.437743Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"time: 692 µs (started: 2022-08-11 21:51:43 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Saving results and models ... \")\nwith open(f\"./CNN-label-pred-keywords-kfold{cur_fold}.pkl\", \"wb\") as f:\n    pickle.dump(pred_lable_keywords, f)\n\ntorch.save(model.state_dict, f\"./Model_state_dict-kfold{cur_fold}.pt\")\n\ntorch.save(model, f\"./CNN-kfold{cur_fold}.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-08-11T21:51:43.441093Z","iopub.execute_input":"2022-08-11T21:51:43.441822Z","iopub.status.idle":"2022-08-11T21:51:43.817037Z","shell.execute_reply.started":"2022-08-11T21:51:43.441786Z","shell.execute_reply":"2022-08-11T21:51:43.815962Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Saving results and models ... \ntime: 369 ms (started: 2022-08-11 21:51:43 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"import functools\nimport sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchtext\nimport tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:23.563289Z","iopub.execute_input":"2022-08-03T14:14:23.564169Z","iopub.status.idle":"2022-08-03T14:14:23.636030Z","shell.execute_reply.started":"2022-08-03T14:14:23.564126Z","shell.execute_reply":"2022-08-03T14:14:23.634958Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"cuda\ntime: 67.2 ms (started: 2022-08-03 14:14:23 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"seed = 0\n\ntorch.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:24.572015Z","iopub.execute_input":"2022-08-03T14:14:24.573069Z","iopub.status.idle":"2022-08-03T14:14:24.589597Z","shell.execute_reply.started":"2022-08-03T14:14:24.573024Z","shell.execute_reply":"2022-08-03T14:14:24.588454Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f389016ab70>"},"metadata":{}},{"name":"stdout","text":"time: 5.23 ms (started: 2022-08-03 14:14:24 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"class CNN_n(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, output_dim, dropout_rate, pad_index, crf=False):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n        self.conv1=torch.nn.Conv1d(embedding_dim, 128, 5, padding=2)\n        self.conv2=torch.nn.Conv1d(embedding_dim, 128, 3, padding=1)\n        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n        self.conv4=torch.nn.Conv1d(256, 256, 5, padding=2)\n        self.conv5=torch.nn.Conv1d(256, 256, 5, padding=2)\n        self.fc = nn.Linear(256, output_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.crf_flag=crf\n        if self.crf_flag:\n            self.crf = ConditionalRandomField(output_dim) \n        \n    def forward(self, ids, ids_len, mask, tag=None, is_training=True):\n        # ids = [batch size, seq len]\n#         print(f\"ids: {ids.size()}\")\n        embedded = self.dropout(self.embedding(ids))\n#         print(f\"embedding: {embedded.size()}\")\n        # embedded = [batch size, seq len, embedding dim]\n        conved = embedded.transpose(1, 2)\n#         print(f\"after permute: {conved.size()}\")\n        # embedded = [batch size, embedding dim, seq len]\n        x_conv=torch.nn.functional.relu(torch.cat((self.conv1(conved), self.conv2(conved)), dim=1))\n#         x_conv=torch.nn.functional.relu(self.conv1(conved))\n        x_conv=self.dropout(x_conv)\n        x_conv=torch.nn.functional.relu(self.conv3(x_conv))\n        x_conv=self.dropout(x_conv)\n        x_conv=torch.nn.functional.relu(self.conv4(x_conv))\n        x_conv=self.dropout(x_conv)\n        x_conv=torch.nn.functional.relu(self.conv5(x_conv))\n        conved=x_conv.transpose(1, 2)\n        logit = self.fc(conved)\n        if not is_training:\n            if self.crf_flag:\n                score = self.crf.viterbi_tags(logit, mask)\n            else:\n                x_logit = logit.transpose(2, 0)\n                score = torch.nn.functional.log_softmax(x_logit).transpose(2, 0)\n        else:\n            if self.crf_flag:\n                score = - self.crf(logit, tag, mask)\n            else:\n                x_logit = torch.nn.utils.rnn.pack_padded_sequence(logit, ids_len, batch_first=True)\n                score = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(x_logit.data), tag.data)\n        return score","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:26.621140Z","iopub.execute_input":"2022-08-03T14:14:26.621889Z","iopub.status.idle":"2022-08-03T14:14:26.637043Z","shell.execute_reply.started":"2022-08-03T14:14:26.621853Z","shell.execute_reply":"2022-08-03T14:14:26.635878Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"time: 2.89 ms (started: 2022-08-03 14:14:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchtext\ndef tokenize_example(example, max_length=300):\n    return example.split(' ')[:max_length]\ndef split_tags(example, max_length=300):\n    return example.split(',')[:max_length]\n\n# data2 = data2.sample(200)\n\ndata2['tokens'] = data2['sentence'].map(tokenize_example)\ndata2['labels'] = data2['word_labels2'].map(split_tags)\n\ntrain_size = 0.8\ntrain_dataset = data2.sample(frac=train_size,random_state=200)\ntest_dataset = data2.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\nmin_freq = 3\nspecial_tokens = ['<unk>', '<pad>']\n\nvocab = torchtext.vocab.build_vocab_from_iterator(train_dataset['tokens'],\n                                                  min_freq=min_freq,\n                                                  specials=special_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:28.753628Z","iopub.execute_input":"2022-08-03T14:14:28.753990Z","iopub.status.idle":"2022-08-03T14:14:31.153686Z","shell.execute_reply.started":"2022-08-03T14:14:28.753957Z","shell.execute_reply":"2022-08-03T14:14:31.152631Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"time: 2.39 s (started: 2022-08-03 14:14:28 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"unk_index = vocab['<unk>']\npad_index = vocab['<pad>']\nvocab.set_default_index(unk_index)\n\ndef numericalize_data(example, vocab=vocab):\n    return [vocab[token] for token in example]\n\ndef numericalize_label(example, vocab=labels_to_ids2):\n    return [vocab[token] for token in example]\n\ntrain_dataset['ids'] = train_dataset['tokens'].map(numericalize_data)\ntrain_dataset['labels2'] = train_dataset['labels'].map(numericalize_label)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:31.155721Z","iopub.execute_input":"2022-08-03T14:14:31.156140Z","iopub.status.idle":"2022-08-03T14:14:34.257245Z","shell.execute_reply.started":"2022-08-03T14:14:31.156095Z","shell.execute_reply":"2022-08-03T14:14:34.256092Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"time: 3.1 s (started: 2022-08-03 14:14:31 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from allennlp.modules import ConditionalRandomField","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:34.259238Z","iopub.execute_input":"2022-08-03T14:14:34.259712Z","iopub.status.idle":"2022-08-03T14:14:34.807627Z","shell.execute_reply.started":"2022-08-03T14:14:34.259672Z","shell.execute_reply":"2022-08-03T14:14:34.806559Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"time: 543 ms (started: 2022-08-03 14:14:34 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 300\noutput_dim = len(labels_to_ids2)\ndropout_rate = 0.5\n\nmodel = CNN_n(vocab_size, embedding_dim, output_dim, dropout_rate, pad_index, crf=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:34.808952Z","iopub.execute_input":"2022-08-03T14:14:34.809917Z","iopub.status.idle":"2022-08-03T14:14:34.928646Z","shell.execute_reply.started":"2022-08-03T14:14:34.809879Z","shell.execute_reply":"2022-08-03T14:14:34.927518Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"time: 113 ms (started: 2022-08-03 14:14:34 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:34.931001Z","iopub.execute_input":"2022-08-03T14:14:34.931421Z","iopub.status.idle":"2022-08-03T14:14:34.939820Z","shell.execute_reply.started":"2022-08-03T14:14:34.931383Z","shell.execute_reply":"2022-08-03T14:14:34.938587Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"The model has 11,112,700 trainable parameters\ntime: 833 µs (started: 2022-08-03 14:14:34 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def initialize_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Conv1d):\n        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n        nn.init.zeros_(m.bias)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:37.341378Z","iopub.execute_input":"2022-08-03T14:14:37.341887Z","iopub.status.idle":"2022-08-03T14:14:37.356924Z","shell.execute_reply.started":"2022-08-03T14:14:37.341843Z","shell.execute_reply":"2022-08-03T14:14:37.354248Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"time: 3.39 ms (started: 2022-08-03 14:14:37 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.apply(initialize_weights)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:14:38.343204Z","iopub.execute_input":"2022-08-03T14:14:38.344129Z","iopub.status.idle":"2022-08-03T14:14:38.363504Z","shell.execute_reply.started":"2022-08-03T14:14:38.344062Z","shell.execute_reply":"2022-08-03T14:14:38.362517Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CNN_n(\n  (embedding): Embedding(32731, 300, padding_idx=1)\n  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n  (conv2): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n  (fc): Linear(in_features=256, out_features=8, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (crf): ConditionalRandomField()\n)"},"metadata":{}},{"name":"stdout","text":"time: 15.1 ms (started: 2022-08-03 14:14:38 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"vectors = torchtext.vocab.GloVe()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:18:48.187172Z","iopub.execute_input":"2022-08-03T14:18:48.188050Z","iopub.status.idle":"2022-08-03T14:31:37.594067Z","shell.execute_reply.started":"2022-08-03T14:18:48.188009Z","shell.execute_reply":"2022-08-03T14:31:37.590301Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":".vector_cache/glove.840B.300d.zip: 2.18GB [06:51, 5.30MB/s]                               \n100%|█████████▉| 2196016/2196017 [04:03<00:00, 9002.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"time: 12min 49s (started: 2022-08-03 14:18:48 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\nmodel.embedding.weight.data = pretrained_embedding\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:32:39.325243Z","iopub.execute_input":"2022-08-03T14:32:39.325617Z","iopub.status.idle":"2022-08-03T14:32:39.795252Z","shell.execute_reply.started":"2022-08-03T14:32:39.325584Z","shell.execute_reply":"2022-08-03T14:32:39.794337Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"time: 460 ms (started: 2022-08-03 14:32:39 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\ndef sent2ids(text):\n    return [vocab[w] for w in text]\n\ndef labels2ids(labels):\n    return [labels_to_ids2[w] for w in labels]\n\ndef data_generator(sents, labels, batch_size=32, is_training=True, index=0):\n    if is_training:\n        select_indices = np.random.choice(len(sents), batch_size, replace=False)\n    else:\n        start = index\n        end = min(start + batch_size, len(sents)) \n        select_indices = list(range(start, end))\n    #select_indices = list(range(batch_size))\n    batch_sents = np.array(sents)[select_indices]\n    batch_labels = np.array(labels)[select_indices]\n    \n    batch_sents = list(map(sent2ids, batch_sents))\n    batch_labels = list(map(labels2ids, batch_labels))\n    \n    seq_lens = [len(s) for s in batch_sents]\n    seq_lens = torch.LongTensor(seq_lens)\n    max_len = max(seq_lens)\n    \n    batch_sents = [torch.LongTensor(s) for s in batch_sents]\n    \n    batch_sents = pad_sequence(batch_sents, batch_first=True)\n    \n    batch_labels = [torch.LongTensor(s) for s in batch_labels]\n    batch_labels = pad_sequence(batch_labels, batch_first=True)\n\n    batch_mask = (batch_sents!=0).numpy().astype(np.uint8)\n    batch_mask = torch.from_numpy(batch_mask)\n    \n    if not is_training:\n        return batch_sents, batch_labels, seq_lens, batch_mask, end\n    return batch_sents, batch_labels, seq_lens, batch_mask","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:33:32.521603Z","iopub.execute_input":"2022-08-03T14:33:32.522039Z","iopub.status.idle":"2022-08-03T14:33:32.537129Z","shell.execute_reply.started":"2022-08-03T14:33:32.522000Z","shell.execute_reply":"2022-08-03T14:33:32.535425Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"time: 4.46 ms (started: 2022-08-03 14:33:32 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\ntrain_sent_words = train_dataset['tokens']\ntrain_sent_tags = train_dataset['labels']\nepoch = 10\nloop_num = int(len(train_sent_words)/100)\nprint(f\"The loop numbers: {loop_num}\")\nfor i in range(epoch):\n    start = time.time()\n    model.train()\n    for j in range(loop_num):\n        optimizer.zero_grad()\n        batch_sents, batch_tags, seq_lens, batch_mask = data_generator(train_sent_words, train_sent_tags, batch_size=100)\n        loss = model(batch_sents.to(device), seq_lens.to(device), batch_mask.to(device), batch_tags.to(device))\n        loss.backward()\n        optimizer.step()\n        if j % 100 == 0:\n            print(f'Loss: {loss.item()} \\t Cost time per step: {time.time() - start}')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:34:01.095172Z","iopub.execute_input":"2022-08-03T14:34:01.096115Z","iopub.status.idle":"2022-08-03T14:34:01.159445Z","shell.execute_reply.started":"2022-08-03T14:34:01.096043Z","shell.execute_reply":"2022-08-03T14:34:01.158144Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"The loop numbers: 263\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/810164795.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sent_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sent_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.","output_type":"error"},{"name":"stdout","text":"time: 56.7 ms (started: 2022-08-03 14:34:01 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2022-08-03T14:33:57.995985Z","iopub.execute_input":"2022-08-03T14:33:57.996688Z","iopub.status.idle":"2022-08-03T14:33:58.003927Z","shell.execute_reply.started":"2022-08-03T14:33:57.996651Z","shell.execute_reply":"2022-08-03T14:33:58.002874Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"time: 402 µs (started: 2022-08-03 14:33:57 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"dev_sent_words = test_dataset['tokens']\ndev_sent_tags = test_dataset['labels']\nmodel.eval()\nindex = 0\npred_label_list = []\nwhile index < len(dev_sent_words):\n    batch_sents, batch_tags, seq_lens, batch_mask, index = data_generator(dev_sent_words, \n                                                              dev_sent_tags, batch_size=32, \n                                                              is_training=False, index=index)\n    pred_labels = model(batch_sents.to(device), seq_lens.to(device), batch_mask.to(device), batch_tags, is_training=False)\n    for label_seq in pred_labels:\n        pred_labels = [ids_to_labels2[t] for t in label_seq[0]]\n        pred_label_list.append(pred_labels)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T13:58:55.638757Z","iopub.execute_input":"2022-08-03T13:58:55.639152Z","iopub.status.idle":"2022-08-03T13:58:56.026581Z","shell.execute_reply.started":"2022-08-03T13:58:55.639119Z","shell.execute_reply":"2022-08-03T13:58:56.025269Z"},"trusted":true},"execution_count":39,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3756451921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids_to_labels2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpred_label_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3756451921.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids_to_labels2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpred_label_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 9"],"ename":"KeyError","evalue":"9","output_type":"error"},{"name":"stdout","text":"time: 380 ms (started: 2022-08-03 13:58:55 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Other Model - 1","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport torch.optim as optim\nimport os\nimport sys\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:13:51.738240Z","iopub.execute_input":"2022-09-09T09:13:51.739052Z","iopub.status.idle":"2022-09-09T09:13:51.747984Z","shell.execute_reply.started":"2022-09-09T09:13:51.739018Z","shell.execute_reply":"2022-09-09T09:13:51.745690Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"time: 961 µs (started: 2022-09-09 09:13:51 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# emb_file = '../input/glove6b100dtxt/glove.6B.100d.txt'  # path to pre-trained word embeddings\nemb_file = '../input/glove6b100dtxt/glove.6B.100d.txt'\nmin_word_freq = 3  # threshold for word frequency\nmin_char_freq = 1  # threshold for character frequency\ncaseless = True  # lowercase everything?\nexpand_vocab = True  # expand model's input vocabulary to the pre-trained embeddings' vocabulary?\n\n# Model parameters\nchar_emb_dim = 30  # character embedding size\nwith open(emb_file, 'r') as f:\n    word_emb_dim = len(f.readline().split(' ')) - 1  # word embedding size\nword_rnn_dim = 300  # word RNN size\nchar_rnn_dim = 300  # character RNN size\nchar_rnn_layers = 1  # number of layers in character RNN\nword_rnn_layers = 1  # number of layers in word RNN\nhighway_layers = 1  # number of layers in highway network\ndropout = 0.5  # dropout\nfine_tune_word_embeddings = False  # fine-tune pre-trained word embeddings?\n\n# Training parameters\nstart_epoch = 0  # start at this epoch\nbatch_size = 32  # batch size\nlr = 0.015  # learning rate\nlr_decay = 0.05  # decay learning rate by this amount\nmomentum = 0.9  # momentum\nworkers = 1  # number of workers for loading data in the DataLoader\nepochs = 12  # number of epochs to run without early-stopping\ngrad_clip = 5.  # clip gradients at this value\nprint_freq = 100  # print training or validation status every __ batches\nbest_f1 = 0.  # F1 score to start with\ncheckpoint = None  # path to model checkpoint, None if none\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:13:57.877522Z","iopub.execute_input":"2022-09-09T09:13:57.877935Z","iopub.status.idle":"2022-09-09T09:13:57.899024Z","shell.execute_reply.started":"2022-09-09T09:13:57.877902Z","shell.execute_reply":"2022-09-09T09:13:57.897463Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"time: 12.4 ms (started: 2022-09-09 09:13:57 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_example(example, max_length=300):\n    return example.split(' ')[:max_length]\n\ndef split_tags(example, max_length=300):\n    return example.split(',')[:max_length]\n\n# data2 = data2.sample(200)\n\ndata2['tokens'] = data2['sentence'].map(tokenize_example)\ndata2['labels'] = data2['word_labels2'].map(split_tags)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:13:59.590969Z","iopub.execute_input":"2022-09-09T09:13:59.591374Z","iopub.status.idle":"2022-09-09T09:14:00.808860Z","shell.execute_reply.started":"2022-09-09T09:13:59.591341Z","shell.execute_reply":"2022-09-09T09:14:00.807435Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"time: 1.21 s (started: 2022-09-09 09:13:59 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# train_size = 0.8\n# train_dataset2 = data2.sample(frac=train_size, random_state=200)\n# test_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\n# train_dataset2 = train_dataset2.reset_index(drop=True)\nk_fold = 5\ncur_fold = 3\ngap = int(len(data2) / k_fold)\nindices = [[i*gap, (i+1)*gap if i+1 != k_fold else len(data2)] \n          for i in range(k_fold)]\ndata3 = data2.sample(frac=1, random_state=200)\ntest_dataset2 = data3[indices[cur_fold][0]:indices[cur_fold][1]]\ntrain_dataset2 = data3.drop(test_dataset2.index).reset_index(drop=True)\ntest_dataset2 = test_dataset2.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:14:04.313499Z","iopub.execute_input":"2022-09-09T09:14:04.313911Z","iopub.status.idle":"2022-09-09T09:14:04.873100Z","shell.execute_reply.started":"2022-09-09T09:14:04.313881Z","shell.execute_reply":"2022-09-09T09:14:04.871566Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"time: 550 ms (started: 2022-09-09 09:14:04 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"word_map, char_map, tag_map = create_maps(data2.tokens, data2.labels, min_word_freq,\n                                                  min_char_freq)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:14:05.170417Z","iopub.execute_input":"2022-09-09T09:14:05.171131Z","iopub.status.idle":"2022-09-09T09:15:25.107173Z","shell.execute_reply.started":"2022-09-09T09:14:05.171058Z","shell.execute_reply":"2022-09-09T09:15:25.105423Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"time: 1min 19s (started: 2022-09-09 09:14:05 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map,\n                                                              expand_vocab)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:15:25.109668Z","iopub.execute_input":"2022-09-09T09:15:25.110895Z","iopub.status.idle":"2022-09-09T09:15:53.677286Z","shell.execute_reply.started":"2022-09-09T09:15:25.110846Z","shell.execute_reply":"2022-09-09T09:15:53.675721Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Embedding length is 100.\nYou have elected to include embeddings that are out-of-corpus.\n\nLoading embeddings...\n'word_map' is being updated accordingly.\n\nDone.\n Embedding vocabulary: 415143\n Language Model vocabulary: 30184.\n\ntime: 28.6 s (started: 2022-09-09 09:15:25 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = LM_LSTM_CRF(tagset_size=len(tag_map),\n                    charset_size=len(char_map),\n                    char_emb_dim=char_emb_dim,\n                    char_rnn_dim=char_rnn_dim,\n                    char_rnn_layers=char_rnn_layers,\n                    vocab_size=len(word_map),\n                    lm_vocab_size=lm_vocab_size,\n                    word_emb_dim=word_emb_dim,\n                    word_rnn_dim=word_rnn_dim,\n                    word_rnn_layers=word_rnn_layers,\n                    dropout=dropout,\n                    highway_layers=highway_layers).to(device)\nmodel.init_word_embeddings(embeddings.to(device))  # initialize embedding layer with pre-trained embeddings\nmodel.fine_tune_word_embeddings(fine_tune_word_embeddings)  # fine-tune\noptimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:15:53.679060Z","iopub.execute_input":"2022-09-09T09:15:53.680107Z","iopub.status.idle":"2022-09-09T09:15:54.269599Z","shell.execute_reply.started":"2022-09-09T09:15:53.680063Z","shell.execute_reply":"2022-09-09T09:15:54.268097Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"time: 580 ms (started: 2022-09-09 09:15:53 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss functions\nlm_criterion = nn.CrossEntropyLoss().to(device)\ncrf_criterion = ViterbiLoss(tag_map).to(device)\n\n# Since the language model's vocab is restricted to in-corpus indices, encode training/val with only these!\n# word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n# they were in the pre-trained embeddings\ntemp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\ntrain_inputs = create_input_tensors(train_dataset2.tokens, train_dataset2.labels, temp_word_map, char_map,\n                                    tag_map)\nval_inputs = create_input_tensors(test_dataset2.tokens, test_dataset2.labels, temp_word_map, char_map, tag_map)\n\n# DataLoaders\ntrain_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n                                           num_workers=workers, pin_memory=False)\nval_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n                                         num_workers=workers, pin_memory=False)\n\n# Viterbi decoder (to find accuracy during validation)\nvb_decoder = ViterbiDecoder(tag_map)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:15:54.273706Z","iopub.execute_input":"2022-09-09T09:15:54.274858Z","iopub.status.idle":"2022-09-09T09:19:10.324310Z","shell.execute_reply.started":"2022-09-09T09:15:54.274784Z","shell.execute_reply":"2022-09-09T09:19:10.322625Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"time: 3min 16s (started: 2022-09-09 09:15:54 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder):\n    \"\"\"\n    Performs one epoch's training.\n\n    :param train_loader: DataLoader for training data\n    :param model: model\n    :param lm_criterion: cross entropy loss layer\n    :param crf_criterion: viterbi loss layer\n    :param optimizer: optimizer\n    :param epoch: epoch number\n    :param vb_decoder: viterbi decoder (to decode and find F1 score)\n    \"\"\"\n\n    model.train()  # training mode enables dropout\n\n    ce_losses = AverageMeter()  # cross entropy loss\n    vb_losses = AverageMeter()  # viterbi loss\n    f1s = AverageMeter()  # f1 score\n\n    start = time.time()\n\n    # Batches\n    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n            train_loader):\n\n        max_word_len = max(wmap_lengths.tolist())\n        max_char_len = max(cmap_lengths.tolist())\n\n        # Reduce batch's padded length to maximum in-batch sequence\n        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n        wmaps = wmaps[:, :max_word_len].to(device)\n        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n        tmaps = tmaps[:, :max_word_len].to(device)\n        wmap_lengths = wmap_lengths.to(device)\n        cmap_lengths = cmap_lengths.to(device)\n\n        # Forward prop.\n        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n                                                                                                             cmaps_b,\n                                                                                                             cmarkers_f,\n                                                                                                             cmarkers_b,\n                                                                                                             wmaps,\n                                                                                                             tmaps,\n                                                                                                             wmap_lengths,\n                                                                                                             cmap_lengths)\n\n        # LM loss\n\n        # We don't predict the next word at the pads or <end> tokens\n        # We will only predict at [dunston, checks, in] among [dunston, checks, in, <end>, <pad>, <pad>, ...]\n        # So, prediction lengths are word sequence lengths - 1\n        lm_lengths = wmap_lengths_sorted - 1\n        lm_lengths = lm_lengths.tolist()\n\n        # Remove scores at timesteps we won't predict at\n        # pack_padded_sequence is a good trick to do this (see dynamic_rnn.py, where we explore this)\n        lm_f_scores = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n        lm_b_scores = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n\n        # For the forward sequence, targets are from the second word onwards, up to <end>\n        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n        lm_f_targets = wmaps_sorted[:, 1:]\n        lm_f_targets = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n\n        # For the backward sequence, targets are <end> followed by all words except the last word\n        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n        lm_b_targets = torch.cat(\n            [torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), wmaps_sorted], dim=1)\n        lm_b_targets = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n\n        # Calculate loss\n        ce_loss = lm_criterion(lm_f_scores.data, lm_f_targets.data) + lm_criterion(lm_b_scores.data, lm_b_targets.data)\n        wmap_lengths_sorted = wmap_lengths_sorted.to('cpu')\n        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n        loss = ce_loss + vb_loss\n\n        # Back prop.\n        optimizer.zero_grad()\n        loss.backward()\n\n        if grad_clip is not None:\n            clip_gradient(optimizer, grad_clip)\n\n        optimizer.step()\n\n        # Viterbi decode to find accuracy / f1\n        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n\n        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n        decoded = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n        tmaps_sorted = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n\n        # F1\n        f1 = f1_score(tmaps_sorted.data.to(\"cpu\"), decoded.data.numpy(), average='macro')\n\n        # Keep track of metrics\n        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n        f1s.update(f1, sum(lm_lengths))\n\n    # Print training status\n    print(f'Epoch: [{epoch}]\\t Epoch Time {time.time() - start} \\t'\n          f'CE Loss {ce_losses.val:.4f} ({ce_losses.avg:.4f})\\t'\n          f'VB Loss {vb_losses.val:.4f} ({vb_losses.avg:.4f})\\t'\n          f'F1 {f1s.val:.3f} ({f1s.avg:.3f})')\n\n\ndef validate(val_loader, model, crf_criterion, vb_decoder):\n    \"\"\"\n    Performs one epoch's validation.\n\n    :param val_loader: DataLoader for validation data\n    :param model: model\n    :param crf_criterion: viterbi loss layer\n    :param vb_decoder: viterbi decoder\n    :return: validation F1 score\n    \"\"\"\n    model.eval()\n\n    batch_time = AverageMeter()\n    vb_losses = AverageMeter()\n    f1s = AverageMeter()\n    labels, predictions = [], []\n\n    start = time.time()\n\n    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n            val_loader):\n\n        max_word_len = max(wmap_lengths.tolist())\n        max_char_len = max(cmap_lengths.tolist())\n\n        # Reduce batch's padded length to maximum in-batch sequence\n        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n        wmaps = wmaps[:, :max_word_len].to(device)\n        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n        tmaps = tmaps[:, :max_word_len].to(device)\n        wmap_lengths = wmap_lengths.to(device)\n        cmap_lengths = cmap_lengths.to(device)\n\n        # Forward prop.\n        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n                                                                                   cmaps_b,\n                                                                                   cmarkers_f,\n                                                                                   cmarkers_b,\n                                                                                   wmaps,\n                                                                                   tmaps,\n                                                                                   wmap_lengths,\n                                                                                   cmap_lengths)\n\n        # Viterbi / CRF layer loss\n        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted.to(\"cpu\"))\n\n        # Viterbi decode to find accuracy / f1\n        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n\n        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n        decoded = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n        tmaps_sorted = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n\n        # f1\n        f1 = f1_score(tmaps_sorted.data.to(\"cpu\"), decoded.data.numpy(), average='macro')\n        \n        labels.append(tmaps_sorted.data)\n        predictions.append(decoded.data)\n        # Keep track of metrics\n        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        if i % print_freq == 0:\n            print('Validation: [{0}/{1}]\\t'\n                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n                                                                  vb_loss=vb_losses, f1=f1s))\n\n    print(\n        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n                                                                          f1=f1s))\n\n    return f1s.avg, labels, predictions","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:19:10.329967Z","iopub.execute_input":"2022-09-09T09:19:10.330326Z","iopub.status.idle":"2022-09-09T09:19:10.365146Z","shell.execute_reply.started":"2022-09-09T09:19:10.330295Z","shell.execute_reply":"2022-09-09T09:19:10.363157Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"time: 8.59 ms (started: 2022-09-09 09:19:10 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(start_epoch, epochs):\n\n    # One epoch's training\n    train(train_loader=train_loader,\n          model=model,\n          lm_criterion=lm_criterion,\n          crf_criterion=crf_criterion,\n          optimizer=optimizer,\n          epoch=epoch,\n          vb_decoder=vb_decoder)\n\n    # One epoch's validation\n    val_f1, labels, predictions = validate(val_loader=val_loader,\n                      model=model,\n                      crf_criterion=crf_criterion,\n                      vb_decoder=vb_decoder)\n\n    # Did validation F1 score improve?\n    is_best = val_f1 > best_f1\n    best_f1 = max(val_f1, best_f1)\n    if not is_best:\n        epochs_since_improvement += 1\n        print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n    else:\n        epochs_since_improvement = 0\n        \n    # Decay learning rate every epoch\n    adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))","metadata":{"execution":{"iopub.status.busy":"2022-09-09T09:19:10.367690Z","iopub.execute_input":"2022-09-09T09:19:10.368803Z","iopub.status.idle":"2022-09-09T10:04:20.472554Z","shell.execute_reply.started":"2022-09-09T09:19:10.368746Z","shell.execute_reply":"2022-09-09T10:04:20.470467Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch: [0]\t Epoch Time 1023.2107527256012 \tCE Loss 14.7385 (15.9708)\tVB Loss 161.5673 (203.0695)\tF1 0.165 (0.161)\nValidation: [0/206]\tBatch Time 0.849 (0.849)\tVB Loss 181.2461 (181.2461)\tF1 Score 0.165 (0.165)\t\nValidation: [100/206]\tBatch Time 0.468 (0.491)\tVB Loss 190.3750 (171.5852)\tF1 Score 0.165 (0.176)\t\nValidation: [200/206]\tBatch Time 0.467 (0.494)\tVB Loss 218.4609 (173.1048)\tF1 Score 0.165 (0.176)\t\n\n * LOSS - 173.513, F1 SCORE - 0.176\n\n\nDECAYING learning rate.\nThe new learning rate is 0.014286\n\nEpoch: [1]\t Epoch Time 1023.7744996547699 \tCE Loss 14.5567 (14.6788)\tVB Loss 150.9423 (147.5731)\tF1 0.165 (0.154)\nValidation: [0/206]\tBatch Time 0.885 (0.885)\tVB Loss 100.4375 (100.4375)\tF1 Score 0.165 (0.165)\t\nValidation: [100/206]\tBatch Time 0.467 (0.513)\tVB Loss 183.1562 (122.5514)\tF1 Score 0.197 (0.177)\t\nValidation: [200/206]\tBatch Time 0.463 (0.505)\tVB Loss 110.2188 (121.2852)\tF1 Score 0.198 (0.177)\t\n\n * LOSS - 121.174, F1 SCORE - 0.176\n\n\nDECAYING learning rate.\nThe new learning rate is 0.013636\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/216096906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           vb_decoder=vb_decoder)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# One epoch's validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/2311861377.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder)\u001b[0m\n\u001b[1;32m     46\u001b[0m                                                                                                              \u001b[0mtmaps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                                                                              \u001b[0mwmap_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                                                                                                              cmap_lengths)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# LM loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/354289137.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mcf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforw_char_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# packed sequence of char_rnn_dim, with real sequence lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_char_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 765\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stdout","text":"time: 45min 10s (started: 2022-09-09 09:19:10 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"len(labels), len(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:01:57.195143Z","iopub.execute_input":"2022-08-06T20:01:57.196257Z","iopub.status.idle":"2022-08-06T20:01:57.204910Z","shell.execute_reply.started":"2022-08-06T20:01:57.196208Z","shell.execute_reply":"2022-08-06T20:01:57.203935Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(206, 206)"},"metadata":{}},{"name":"stdout","text":"time: 3.46 ms (started: 2022-08-06 20:01:57 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"labels1 = [l.to(\"cpu\").item() for ls in labels for l in ls]\npredictions1 = [l.to(\"cpu\").item() for ls in predictions for l in ls]","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:02:06.403376Z","iopub.execute_input":"2022-08-06T20:02:06.403743Z","iopub.status.idle":"2022-08-06T20:02:26.688885Z","shell.execute_reply.started":"2022-08-06T20:02:06.403711Z","shell.execute_reply":"2022-08-06T20:02:26.687806Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"time: 20.3 s (started: 2022-08-06 20:02:06 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"tag_to_cat = {v:k for k, v in tag_map.items()}\nlabels2 = [tag_to_cat[l] for l in labels1]\npredictions2 = [tag_to_cat[l] for l in predictions1]","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:02:26.691074Z","iopub.execute_input":"2022-08-06T20:02:26.691439Z","iopub.status.idle":"2022-08-06T20:02:26.849427Z","shell.execute_reply.started":"2022-08-06T20:02:26.691402Z","shell.execute_reply":"2022-08-06T20:02:26.848277Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"time: 153 ms (started: 2022-08-06 20:02:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"words = []\nfor i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n        val_loader):\n\n    max_word_len = max(wmap_lengths.tolist())\n    max_char_len = max(cmap_lengths.tolist())\n\n    # Reduce batch's padded length to maximum in-batch sequence\n    # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n    wmaps = wmaps[:, :max_word_len]\n    words.append(wmaps)","metadata":{"execution":{"iopub.status.busy":"2022-08-01T16:15:03.009507Z","iopub.execute_input":"2022-08-01T16:15:03.010346Z","iopub.status.idle":"2022-08-01T16:15:03.242804Z","shell.execute_reply.started":"2022-08-01T16:15:03.010295Z","shell.execute_reply":"2022-08-01T16:15:03.241595Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"time: 225 ms (started: 2022-08-01 16:15:03 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"dev_sent_words = test_dataset2['tokens']\nwords2 = [d for dev in dev_sent_words for d in dev]","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:02:26.850880Z","iopub.execute_input":"2022-08-06T20:02:26.851403Z","iopub.status.idle":"2022-08-06T20:02:26.939785Z","shell.execute_reply.started":"2022-08-06T20:02:26.851365Z","shell.execute_reply":"2022-08-06T20:02:26.938582Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"time: 83.8 ms (started: 2022-08-06 20:02:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"len(words2), len(labels2)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:02:26.941857Z","iopub.execute_input":"2022-08-06T20:02:26.942173Z","iopub.status.idle":"2022-08-06T20:02:26.953000Z","shell.execute_reply.started":"2022-08-06T20:02:26.942142Z","shell.execute_reply":"2022-08-06T20:02:26.950905Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(1101738, 1101738)"},"metadata":{}},{"name":"stdout","text":"time: 3.78 ms (started: 2022-08-06 20:02:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"tag_to_word = {v:k for k, v in temp_word_map.items()}\nwords2 = [tag_to_word[l] for l in words1]","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:02:30.768666Z","iopub.execute_input":"2022-08-06T20:02:30.769030Z","iopub.status.idle":"2022-08-06T20:02:31.238030Z","shell.execute_reply.started":"2022-08-06T20:02:30.768998Z","shell.execute_reply":"2022-08-06T20:02:31.235620Z"},"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3897969141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtag_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_word_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'words1' is not defined"],"ename":"NameError","evalue":"name 'words1' is not defined","output_type":"error"},{"name":"stdout","text":"time: 462 ms (started: 2022-08-06 20:02:30 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def countKeywords(words, predictions, labels):\n    kws_pairs = []\n    for i in zip(words, predictions, labels):\n        kws_pairs.append(i)\n        \n    predict_kws = []\n    tmp_wd = []\n    tmp_pred, tmp_gold = '', ''\n    for word, pred, gold in kws_pairs:\n        if pred != 'O':\n            if len(tmp_wd) == 0: \n                tmp_pred = pred\n            else:\n                if tmp_pred != pred:\n                    predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                    tmp_wd = []\n                    tmp_pred = pred\n            tmp_wd.append(word)\n        else:\n            if len(tmp_wd) > 0:\n                predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                tmp_wd = []\n                tmp_pred = pred\n\n    # check whether there are new keywords\n    gold_dict = build_gold_dict()\n    new_kws, kw_tags = [], []\n    gold_lists = list(gold_dict.keys())\n    gold_lists = [g.lower() for g in gold_lists]\n    for word, label in predict_kws:\n        if not word.lower() in gold_lists:\n            new_kws.append(word.lower())\n            kw_tags.append(tag2cat[label])\n    new_keywords = list(zip(new_kws, kw_tags))\n\n    new_kws_dict, new_kws_pos = {}, []\n    for kw, cat in new_keywords:\n        pos = pos_tag(kw.split())\n        if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n            key = ' '.join([p for p, t in pos])\n            tag = [t for p, t in pos]\n            new_kws_pos.append(key)\n            if not key in new_kws_dict.keys():\n                new_kws_dict[key] = (tag, cat)\n\n    new_kws = list(new_kws_dict.keys())\n    cats = [value[1] for value in new_kws_dict.values()]\n\n    df = pd.DataFrame({\"keyword\": new_kws, \"category\": cats})\n\n    s = df['keyword'].apply(lengthOfTokens)\n    s.sort_values()\n\n    predict_kws = Counter(predict_kws)\n    new_kws_pos = Counter(new_kws_pos)\n\n    predict_kws = sorted(predict_kws.items(), key=lambda pair: pair[1], reverse=True)\n    new_kws_pos = sorted(new_kws_pos.items(), key=lambda pair: pair[1], reverse=True)\n\n    return predict_kws, new_kws_pos, df\n\ndef lengthOfTokens(pair):\n    return len(pair.split())","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:02:39.514727Z","iopub.execute_input":"2022-08-06T20:02:39.515775Z","iopub.status.idle":"2022-08-06T20:02:39.558630Z","shell.execute_reply.started":"2022-08-06T20:02:39.515728Z","shell.execute_reply":"2022-08-06T20:02:39.557580Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"time: 2.95 ms (started: 2022-08-06 20:02:39 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"predict_kws, new_kws_pos, df = countKeywords(words2, predictions2, labels2)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:02:42.636283Z","iopub.execute_input":"2022-08-06T20:02:42.636904Z","iopub.status.idle":"2022-08-06T20:02:46.453722Z","shell.execute_reply.started":"2022-08-06T20:02:42.636869Z","shell.execute_reply":"2022-08-06T20:02:46.452650Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n  import sys\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n  \n","output_type":"stream"},{"name":"stdout","text":"time: 3.81 s (started: 2022-08-06 20:02:42 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"predict_kws","metadata":{"execution":{"iopub.status.busy":"2022-08-02T11:56:50.917895Z","iopub.execute_input":"2022-08-02T11:56:50.918920Z","iopub.status.idle":"2022-08-02T11:56:50.991359Z","shell.execute_reply.started":"2022-08-02T11:56:50.918813Z","shell.execute_reply":"2022-08-02T11:56:50.989761Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/1682295673.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_kws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'predict_kws' is not defined"],"ename":"NameError","evalue":"name 'predict_kws' is not defined","output_type":"error"}]},{"cell_type":"code","source":"new_kws_pos","metadata":{"execution":{"iopub.status.busy":"2022-08-02T11:10:16.401031Z","iopub.execute_input":"2022-08-02T11:10:16.401813Z","iopub.status.idle":"2022-08-02T11:10:16.458511Z","shell.execute_reply.started":"2022-08-02T11:10:16.401780Z","shell.execute_reply":"2022-08-02T11:10:16.457584Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[('development', 126),\n ('business', 112),\n ('products', 81),\n ('product', 71),\n ('research', 51),\n ('sales', 50),\n ('management', 47),\n ('market', 46),\n ('company', 45),\n ('services', 44),\n ('data', 44),\n ('information', 43),\n ('future', 42),\n ('technology', 42),\n ('customers', 39),\n ('costs', 37),\n ('customer', 37),\n ('risks', 37),\n ('results', 35),\n ('year', 35),\n ('risk', 35),\n ('operations', 35),\n ('activities', 34),\n ('assets', 33),\n ('performance', 33),\n ('group', 33),\n ('value', 32),\n ('revenue', 31),\n ('solutions', 29),\n ('use', 26),\n ('expenses', 26),\n ('regulations', 26),\n ('property', 25),\n ('time', 25),\n ('software', 25),\n ('tax', 24),\n ('support', 24),\n ('continue', 24),\n ('energy', 24),\n ('companies', 24),\n ('impact', 24),\n ('operating', 24),\n ('report', 24),\n ('board', 24),\n ('marketing', 23),\n ('net', 23),\n ('ability', 23),\n ('statements', 22),\n ('change', 22),\n ('systems', 22),\n ('rights', 21),\n ('provide', 21),\n ('december', 21),\n ('requirements', 21),\n ('service', 21),\n ('growth', 20),\n ('emissions', 20),\n ('result', 20),\n ('process', 20),\n ('changes', 20),\n ('agreement', 19),\n ('key', 19),\n ('increase', 19),\n ('markets', 19),\n ('manufacturing', 18),\n ('obligations', 18),\n ('climate', 18),\n ('include', 17),\n ('loss', 17),\n ('access', 17),\n ('security', 17),\n ('platform', 17),\n ('income', 16),\n ('years', 16),\n ('period', 16),\n ('investment', 16),\n ('digital', 16),\n ('programs', 16),\n ('model', 16),\n ('protection', 15),\n ('candidates', 15),\n ('parties', 15),\n ('quality', 15),\n ('number', 15),\n ('cash', 15),\n ('cost', 14),\n ('health', 14),\n ('strategy', 14),\n ('price', 14),\n ('equipment', 14),\n ('facilities', 14),\n ('demand', 14),\n ('control', 14),\n ('design', 14),\n ('companys', 13),\n ('patents', 13),\n ('basis', 13),\n ('addition', 13),\n ('experience', 13),\n ('opportunities', 13),\n ('protect', 13),\n ('subject', 13),\n ('executive', 13),\n ('plan', 13),\n ('standards', 13),\n ('conditions', 13),\n ('production', 13),\n ('sustainability', 13),\n ('adverse', 12),\n ('technologies', 12),\n ('sale', 12),\n ('industry', 12),\n ('states', 12),\n ('resources', 12),\n ('compliance', 12),\n ('delays', 12),\n ('collaboration', 12),\n ('developed', 12),\n ('system', 12),\n ('terms', 12),\n ('materials', 12),\n ('reporting', 12),\n ('maintain', 12),\n ('matters', 12),\n ('laws', 11),\n ('drug', 11),\n ('target', 11),\n ('governance', 11),\n ('part', 11),\n ('table', 11),\n ('innovation', 11),\n ('enhance', 11),\n ('patent', 11),\n ('government', 11),\n ('date', 11),\n ('supply', 11),\n ('targets', 11),\n ('groups', 11),\n ('segment', 11),\n ('liabilities', 11),\n ('policies', 11),\n ('people', 11),\n ('environment', 10),\n ('expense', 10),\n ('amount', 10),\n ('acquisition', 10),\n ('transition', 10),\n ('trials', 10),\n ('relationships', 10),\n ('capabilities', 10),\n ('platforms', 10),\n ('objectives', 10),\n ('pandemic', 10),\n ('interest', 10),\n ('network', 10),\n ('chief', 10),\n ('reduction', 10),\n ('position', 10),\n ('providers', 10),\n ('suppliers', 10),\n ('create', 10),\n ('conduct', 10),\n ('scope', 10),\n ('needs', 10),\n ('share', 10),\n ('help', 10),\n ('operate', 10),\n ('rate', 10),\n ('agreements', 10),\n ('employees', 10),\n ('license', 10),\n ('countries', 9),\n ('processes', 9),\n ('program', 9),\n ('meet', 9),\n ('term', 9),\n ('commercialization', 9),\n ('failure', 9),\n ('capital', 9),\n ('others', 9),\n ('factors', 9),\n ('offering', 9),\n ('areas', 9),\n ('approval', 9),\n ('life', 9),\n ('work', 9),\n ('end', 9),\n ('initiatives', 9),\n ('projects', 9),\n ('employee', 9),\n ('deliver', 9),\n ('level', 9),\n ('committee', 9),\n ('purchase', 9),\n ('accounting', 9),\n ('adequate', 9),\n ('revenues', 8),\n ('condition', 8),\n ('comply', 8),\n ('place', 8),\n ('actions', 8),\n ('losses', 8),\n ('consumer', 8),\n ('impairment', 8),\n ('face', 8),\n ('mobile', 8),\n ('expertise', 8),\n ('measures', 8),\n ('fail', 8),\n ('accounts', 8),\n ('review', 8),\n ('arrangements', 8),\n ('waste', 8),\n ('sites', 8),\n ('require', 8),\n ('test', 8),\n ('contents', 8),\n ('human', 8),\n ('payments', 8),\n ('members', 8),\n ('efforts', 8),\n ('engineering', 8),\n ('range', 8),\n ('distribution', 8),\n ('core', 8),\n ('intelligence', 8),\n ('implement', 8),\n ('set', 8),\n ('principal', 8),\n ('levels', 8),\n ('asset', 8),\n ('personnel', 8),\n ('fda', 7),\n ('acquisitions', 7),\n ('order', 7),\n ('accordance', 7),\n ('expansion', 7),\n ('covid-19', 7),\n ('brand', 7),\n ('processing', 7),\n ('compensation', 7),\n ('benefit', 7),\n ('uk', 7),\n ('cannot', 7),\n ('equity', 7),\n ('events', 7),\n ('effect', 7),\n ('team', 7),\n ('depend', 7),\n ('fair', 7),\n ('uncertainties', 7),\n ('things', 7),\n ('drive', 7),\n ('march', 7),\n ('need', 7),\n ('affect', 7),\n ('threats', 7),\n ('content', 7),\n ('oil', 7),\n ('statement', 7),\n ('party', 7),\n ('build', 7),\n ('europe', 7),\n ('leadership', 7),\n ('networks', 7),\n ('diversity', 7),\n ('example', 7),\n ('client', 7),\n ('improvement', 7),\n ('segments', 7),\n ('principles', 7),\n ('investments', 7),\n ('response', 7),\n ('credit', 7),\n ('efficiency', 7),\n ('base', 7),\n ('prevent', 7),\n ('measure', 7),\n ('pursuant', 7),\n ('flow', 7),\n ('page', 7),\n ('assurance', 7),\n ('manage', 7),\n ('sell', 6),\n ('litigation', 6),\n ('applications', 6),\n ('success', 6),\n ('receive', 6),\n ('thousands', 6),\n ('third-party', 6),\n ('computer', 6),\n ('generation', 6),\n ('audit', 6),\n ('criteria', 6),\n ('stock', 6),\n ('trial', 6),\n ('harm', 6),\n ('subsidiaries', 6),\n ('royalty', 6),\n ('generate', 6),\n ('compete', 6),\n ('integrate', 6),\n ('entities', 6),\n ('procedures', 6),\n ('exchange', 6),\n ('commitment', 6),\n ('achieve', 6),\n ('strategies', 6),\n ('portfolio', 6),\n ('action', 6),\n ('act', 6),\n ('site', 6),\n ('profit', 6),\n ('contract', 6),\n ('issues', 6),\n ('water', 6),\n ('brands', 6),\n ('responsibility', 6),\n ('claims', 6),\n ('offerings', 6),\n ('material', 6),\n ('appropriate', 6),\n ('center', 6),\n ('goals', 6),\n ('day', 6),\n ('communications', 6),\n ('workforce', 6),\n ('gas', 6),\n ('role', 6),\n ('effects', 6),\n ('businesses', 6),\n ('fee', 6),\n ('users', 6),\n ('address', 6),\n ('contracts', 6),\n ('promote', 6),\n ('directors', 6),\n ('collection', 6),\n ('state', 6),\n ('allows', 6),\n ('ltd', 6),\n ('periods', 6),\n ('challenges', 6),\n ('approach', 6),\n ('offer', 6),\n ('inc', 6),\n ('past', 6),\n ('safety', 6),\n ('note', 6),\n ('form', 6),\n ('ii', 6),\n ('assessment', 6),\n ('partners', 6),\n ('focus', 6),\n ('evaluate', 6),\n ('decline', 6),\n ('awards', 6),\n ('patient', 5),\n ('monitoring', 5),\n ('project', 5),\n ('pay', 5),\n ('facility', 5),\n ('percentage', 5),\n ('jurisdictions', 5),\n ('lives', 5),\n ('trade', 5),\n ('practices', 5),\n ('officer', 5),\n ('asia', 5),\n ('amounts', 5),\n ('trends', 5),\n ('attract', 5),\n ('excellence', 5),\n ('expand', 5),\n ('transactions', 5),\n ('china', 5),\n ('forms', 5),\n ('maintenance', 5),\n ('suite', 5),\n ('debt', 5),\n ('liability', 5),\n ('competition', 5),\n ('training', 5),\n ('food', 5),\n ('pricing', 5),\n ('competitors', 5),\n ('survey', 5),\n ('amortization', 5),\n ('prices', 5),\n ('reach', 5),\n ('transportation', 5),\n ('planning', 5),\n ('privacy', 5),\n ('proceedings', 5),\n ('portion', 5),\n ('source', 5),\n ('skills', 5),\n ('inventory', 5),\n ('notes', 5),\n ('become', 5),\n ('teams', 5),\n ('developments', 5),\n ('greenhouse', 5),\n ('progress', 5),\n ('devices', 5),\n ('lease', 5),\n ('enables', 5),\n ('expectations', 5),\n ('policy', 5),\n ('section', 5),\n ('contribution', 5),\n ('predict', 5),\n ('insurance', 5),\n ('recognition', 5),\n ('communities', 5),\n ('sector', 5),\n ('robust', 5),\n ('space', 5),\n ('invest', 5),\n ('factor', 5),\n ('eu', 5),\n ('benefits', 5),\n ('prospects', 5),\n ('organizations', 5),\n ('operators', 5),\n ('chain', 5),\n ('manufacture', 5),\n ('driven', 5),\n ('currency', 5),\n ('world', 5),\n ('solar', 5),\n ('restrictions', 5),\n ('built', 5),\n ('orders', 5),\n ('activity', 4),\n ('application', 4),\n ('traffic', 4),\n ('gdpr', 4),\n ('grow', 4),\n ('shares', 4),\n ('funding', 4),\n ('liquidity', 4),\n ('history', 4),\n ('framework', 4),\n ('anticipate', 4),\n ('university', 4),\n ('force', 4),\n ('economy', 4),\n ('values', 4),\n ('multiple', 4),\n ('analysts', 4),\n ('study', 4),\n ('timing', 4),\n ('completion', 4),\n ('merger', 4),\n ('respect', 4),\n ('california', 4),\n ('inventories', 4),\n ('valuation', 4),\n ('opportunity', 4),\n ('healthcare', 4),\n ('power', 4),\n ('intend', 4),\n ('care', 4),\n ('mitigate', 4),\n ('supplies', 4),\n ('authorities', 4),\n ('item', 4),\n ('manner', 4),\n ('return', 4),\n ('rates', 4),\n ('efficient', 4),\n ('minimum', 4),\n ('items', 4),\n ('eur', 4),\n ('towards', 4),\n ('infrastructure', 4),\n ('expenditure', 4),\n ('unit', 4),\n ('months', 4),\n ('transaction', 4),\n ('carbon', 4),\n ('announcements', 4),\n ('cause', 4),\n ('treatment', 4),\n ('footprint', 4),\n ('capacity', 4),\n ('concerns', 4),\n ('vision', 4),\n ('code', 4),\n ('way', 4),\n ('secure', 4),\n ('executives', 4),\n ('studies', 4),\n ('consumption', 4),\n ('reductions', 4),\n ('transformation', 4),\n ('remuneration', 4),\n ('found', 4),\n ('account', 4),\n ('assess', 4),\n ('availability', 4),\n ('area', 4),\n ('component', 4),\n ('credits', 4),\n ('samples', 4),\n ('crisis', 4),\n ('line', 4),\n ('corporation', 4),\n ('concern', 4),\n ('disputes', 4),\n ('elements', 4),\n ('january', 4),\n ('sufficient', 4),\n ('case', 4),\n ('departments', 4),\n ('alternative', 4),\n ('means', 4),\n ('president', 4),\n ('relevant', 4),\n ('wireless', 4),\n ('delivery', 4),\n ('commercialize', 4),\n ('member', 4),\n ('ideas', 4),\n ('issuance', 4),\n ('financing', 4),\n ('online', 4),\n ('iot', 4),\n ('finance', 4),\n ('savings', 4),\n ('fund', 4),\n ('packaging', 4),\n ('provisions', 4),\n ('shareholders', 4),\n ('volume', 4),\n ('breach', 4),\n ('engagement', 4),\n ('categories', 4),\n ('phase', 4),\n ('worldwide', 4),\n ('notice', 4),\n ('sources', 4),\n ('damages', 4),\n ('zero', 3),\n ('serve', 3),\n ('innovate', 3),\n ('combination', 3),\n ('incur', 3),\n ('coverage', 3),\n ('law', 3),\n ('mg', 3),\n ('excess', 3),\n ('patients', 3),\n ('implementation', 3),\n ('put', 3),\n ('half', 3),\n ('head', 3),\n ('percent', 3),\n ('supervisory', 3),\n ('owners', 3),\n ('amortisation', 3),\n ('east', 3),\n ('enterprise', 3),\n ('auo', 3),\n ('low-carbon', 3),\n ('seek', 3),\n ('practice', 3),\n ('event', 3),\n ('cancer', 3),\n ('quarter', 3),\n ('please', 3),\n ('leverage', 3),\n ('guarantees', 3),\n ('commission', 3),\n ('machine', 3),\n ('forth', 3),\n ('world-class', 3),\n ('demands', 3),\n ('purposes', 3),\n ('licenses', 3),\n ('dependent', 3),\n ('relates', 3),\n ('collaborators', 3),\n ('oems', 3),\n ('complement', 3),\n ('record', 3),\n ('april', 3),\n ('importance', 3),\n ('portfolios', 3),\n (\"company's\", 3),\n ('effort', 3),\n ('fluctuations', 3),\n ('cybersecurity', 3),\n ('schedule', 3),\n ('home', 3),\n ('relate', 3),\n ('switches', 3),\n ('borrowings', 3),\n ('adss', 3),\n ('disease', 3),\n ('respective', 3),\n ('comprise', 3),\n ('approvals', 3),\n ('recall', 3),\n ('combinations', 3),\n ('disruptions', 3),\n ('air', 3),\n ('digitalization', 3),\n ('reports', 3),\n ('optimization', 3),\n ('lesser', 3),\n ('procurement', 3),\n ('spent', 3),\n ('field', 3),\n ('exist', 3),\n ('preparation', 3),\n ('context', 3),\n ('user', 3),\n ('enter', 3),\n ('summary', 3),\n ('adapt', 3),\n ('culture', 3),\n ('lending', 3),\n ('flows', 3),\n ('types', 3),\n ('iii', 3),\n ('indefinite', 3),\n ('auditors', 3),\n ('plant', 3),\n ('stakeholder', 3),\n ('reduces', 3),\n ('purpose', 3),\n ('trademark', 3),\n ('launch', 3),\n ('frequent', 3),\n ('degree', 3),\n ('restructuring', 3),\n ('loan', 3),\n ('ceo', 3),\n ('commitments', 3),\n ('incentive', 3),\n ('proliferation', 3),\n ('terrorism', 3),\n ('diseases', 3),\n ('connectivity', 3),\n ('provider', 3),\n ('africa', 3),\n ('boat', 3),\n ('public', 3),\n ('understand', 3),\n ('estimates', 3),\n ('course', 3),\n ('inherent', 3),\n ('analytics', 3),\n ('landscape', 3),\n ('centers', 3),\n ('internet', 3),\n ('operates', 3),\n ('goods', 3),\n ('union', 3),\n ('cell', 3),\n ('ours', 3),\n ('perform', 3),\n ('grant', 3),\n ('relation', 3),\n ('ten', 3),\n ('secrets', 3),\n ('prove', 3),\n ('partner', 3),\n ('plans', 3),\n ('standard', 3),\n ('assumptions', 3),\n ('contractor', 3),\n ('banking', 3),\n ('connection', 3),\n ('advantage', 3),\n ('networking', 3),\n ('estimation', 3),\n ('earnings', 3),\n ('acceptance', 3),\n ('payment', 3),\n ('today', 3),\n ('channels', 3),\n ('delay', 3),\n ('collect', 3),\n ('lead', 3),\n ('universities', 3),\n ('tests', 3),\n ('royalties', 3),\n ('tools', 3),\n ('database', 3),\n ('ventures', 3),\n ('taxes', 3),\n ('month', 3),\n ('designs', 3),\n ('none', 3),\n ('securities', 3),\n ('arrangement', 3),\n ('updates', 3),\n ('positions', 3),\n ('organisations', 3),\n ('controls', 3),\n ('programme', 3),\n ('communication', 3),\n ('accelerator', 3),\n ('aim', 3),\n ('capability', 3),\n ('co2', 3),\n ('series', 3),\n ('viability', 3),\n ('margin', 3),\n ('integrity', 3),\n ('ways', 3),\n ('schedules', 3),\n ('adoption', 3),\n ('assessments', 3),\n ('functions', 3),\n ('worlds', 3),\n ('transfer', 3),\n ('structure', 3),\n ('penalties', 3),\n ('components', 3),\n ('strength', 3),\n ('balance', 3),\n ('tsmc', 3),\n ('imerys', 3),\n ('server', 2),\n ('controller', 2),\n ('americas', 2),\n ('seeks', 2),\n ('notbe', 2),\n ('steps', 2),\n ('gmbh', 2),\n ('generic', 2),\n ('blood', 2),\n ('gencaro', 2),\n ('limit', 2),\n ('delayed', 2),\n ('right', 2),\n ('education', 2),\n ('intensity', 2),\n ('year-end', 2),\n ('feasibility', 2),\n ('reflect', 2),\n ('ghg', 2),\n ('anderson', 2),\n ('america', 2),\n ('execute', 2),\n ('descriptions', 2),\n ('status', 2),\n ('taiwan', 2),\n ('upstream', 2),\n ('classification', 2),\n ('interim', 2),\n ('regard', 2),\n ('milestone', 2),\n ('inception', 2),\n ('tetraphase', 2),\n ('discussion', 2),\n ('collaborative', 2),\n ('zalviso', 2),\n ('imposition', 2),\n ('claim', 2),\n ('offers', 2),\n ('subscribers', 2),\n ('adobe', 2),\n ('promise', 2),\n ('travel', 2),\n ('mix', 2),\n ('definition', 2),\n ('attempt', 2),\n ('divestitures', 2),\n ('step', 2),\n ('instruments', 2),\n ('infringe', 2),\n ('drives', 2),\n ('permit', 2),\n ('dental', 2),\n ('consta', 2),\n ('achievement', 2),\n ('grants', 2),\n ('participation', 2),\n ('difficulties', 2),\n ('properties', 2),\n ('location', 2),\n ('cycles', 2),\n ('exists', 2),\n ('retirement', 2),\n ('consolidation', 2),\n ('exposures', 2),\n ('lung', 2),\n ('gene', 2),\n ('raise', 2),\n ('expenditures', 2),\n ('option', 2),\n ('semiconductor', 2),\n ('device', 2),\n ('lack', 2),\n ('pandemics', 2),\n ('warranty', 2),\n ('broken', 2),\n ('science', 2),\n ('in-house', 2),\n ('engineers', 2),\n ('store', 2),\n ('movements', 2),\n ('custom', 2),\n ('explore', 2),\n ('volatility', 2),\n ('limits', 2),\n ('creates', 2),\n ('regulation', 2),\n ('weaknesses', 2),\n ('separation', 2),\n ('family', 2),\n ('milk', 2),\n ('oncology', 2),\n ('outsource', 2),\n ('raw', 2),\n ('inform', 2),\n ('stroll', 2),\n ('modalities', 2),\n ('com', 2),\n ('receivables', 2),\n ('nil', 2),\n ('knowledge', 2),\n ('represent', 2),\n ('noise', 2),\n ('vital', 2),\n ('atrias', 2),\n ('autumn', 2),\n ('streams', 2),\n ('population', 2),\n ('identity', 2),\n ('manufacturer', 2),\n ('reason', 2),\n ('filing', 2),\n ('contractors', 2),\n ('discover', 2),\n ('non-compliance', 2),\n ('the company', 2),\n ('evaluation', 2),\n ('agm', 2),\n ('office', 2),\n ('cmos', 2),\n ('marketplace', 2),\n ('disclosure', 2),\n ('representatives', 2),\n ('representative', 2),\n ('cuts', 2),\n ('brazil', 2),\n ('contact', 2),\n ('workplace', 2),\n ('employers', 2),\n ('mental', 2),\n ('additions', 2),\n ('profitability', 2),\n ('gains', 2),\n ('agriculture', 2),\n ('copper', 2),\n ('investor', 2),\n ('examples', 2),\n ('view', 2),\n ('restoration', 2),\n ('commodities', 2),\n ('organisation', 2),\n ('flexibility', 2),\n ('schools', 2),\n ('perspective', 2),\n ('removal', 2),\n ('misappropriation', 2),\n ('investors', 2),\n ('point', 2),\n ('buildings', 2),\n ('alignment', 2),\n ('peru', 2),\n ('recruitment', 2),\n ('respond', 2),\n ('video', 2),\n ('contain', 2),\n ('rise', 2),\n ('drugs', 2),\n ('globe', 2),\n ('registration', 2),\n ('associations', 2),\n ('assumption', 2),\n ('retain', 2),\n ('urbanization', 2),\n ('distinct', 2),\n ('depreciation', 2),\n ('competence', 2),\n ('produce', 2),\n ('characteristics', 2),\n ('foundation', 2),\n ('electricity', 2),\n ('apis', 2),\n ('leases', 2),\n ('harmful', 2),\n ('thereto', 2),\n ('sample', 2),\n ('locations', 2),\n ('lien', 2),\n ('france', 2),\n ('ingelheim', 2),\n ('thanks', 2),\n ('dollars', 2),\n ('incomplete', 2),\n ('initiative', 2),\n ('gri', 2),\n ('simple', 2),\n ('confidential', 2),\n ('indication', 2),\n ('november', 2),\n ('intent', 2),\n ('proximity', 2),\n ('times', 2),\n ('foster', 2),\n ('partnership', 2),\n ('occurrence', 2),\n ('ias', 2),\n ('bozhong', 2),\n ('subsequent', 2),\n ('commercialisation', 2),\n ('refusal', 2),\n ('in-licensing', 2),\n ('inventions', 2),\n ('becomes', 2),\n ('suffer', 2),\n ('carryforwards', 2),\n ('biodiversity', 2),\n ('renewal', 2),\n ('adjustments', 2),\n ('variables', 2),\n ('fields', 2),\n ('ratio', 2),\n ('kpis', 2),\n ('extent', 2),\n ('insights', 2),\n ('contains', 2),\n ('ownership', 2),\n ('termination', 2),\n ('cover', 2),\n ('llc', 2),\n ('medicare', 2),\n ('chemicals', 2),\n ('vulnerabilities', 2),\n ('chemical', 2),\n ('diagnostics', 2),\n ('agencies', 2),\n ('designation', 2),\n ('execution', 2),\n ('sponsor', 2),\n ('facilitate', 2),\n ('description', 2),\n ('models', 2),\n ('file', 2),\n ('decrease', 2),\n ('dividends', 2),\n ('guidance', 2),\n ('scheme', 2),\n ('canada', 2),\n ('manufacturers', 2),\n ('held', 2),\n ('strengthens', 2),\n ('employment', 2),\n ('promotion', 2),\n ('telecom', 2),\n ('telecommunications', 2),\n ('call', 2),\n ('dynamics', 2),\n ('overhead', 2),\n ('peer', 2),\n ('choose', 2),\n ('remain', 2),\n ('perspectives', 2),\n ('assesses', 2),\n ('breaches', 2),\n ('opinion', 2),\n ('vacancy', 2),\n ('expend', 2),\n ('primary', 2),\n ('stages', 2),\n ('novel', 2),\n ('speed', 2),\n ('scale', 2),\n ('pursuit', 2),\n ('industries', 2),\n ('analysis', 2),\n ('joint', 2),\n ('august', 2),\n ('productivity', 2),\n ('innovations', 2),\n ('export', 2),\n ('north', 2),\n ('equivalents', 2),\n ('occur', 2),\n ('society', 2),\n ('guarantee', 2),\n ('consequences', 2),\n ...]"},"metadata":{}},{"name":"stdout","text":"time: 52.5 ms (started: 2022-08-02 11:10:16 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_index_positions(list_of_elems, element):\n    ''' Returns the indexes of all occurrences of give element in\n    the list- listOfElements '''\n    index_pos_list = []\n    index_pos = 0\n    while True:\n        try:\n            # Search for item in list from indexPos to the end of list\n            index_pos = list_of_elems.index(element, index_pos)\n            # Add the index position in list\n            index_pos_list.append(index_pos)\n            index_pos += 1\n        except ValueError as e:\n            break\n    return index_pos_list\n\ndef get_distribution(inputs, name):\n    true_res = {}\n    for k, v in tag2cat.items():\n        indexlist = get_index_positions(inputs, k)\n        left, right = 0, 0\n        new_labels = []\n        for i in indexlist:\n            if left == 0:\n                left = i\n                right = i\n                continue\n            if right+1 == i:\n                right = i\n            else:\n                new_labels.append((left, right))\n                left = i\n                right = i\n        true_res[v] = new_labels\n    print(f\"The distribution of {name} is:\")\n    for k,v in true_res.items():\n        print(f\"There are {len(v)} occurances in {k}\")\n    return true_res\n\ndef get_confusion_matrix(true_labels, true_predictions):\n    # strict mode\n    tp_dict = {}\n    for true_k, true_v in true_labels.items():\n        true_total = len(true_v)\n        tmp_dict = {}\n        tmp_total = 0\n        for pred_k, pred_v in true_predictions.items():\n            num = 0\n            for v in pred_v:\n                if v in true_v:\n                    num += 1\n            tmp_dict[pred_k] = num\n            tmp_total += num\n        tmp_dict[\"O\"] = true_total - tmp_total\n        tp_dict[true_k] = tmp_dict\n    tp_dict\n    tmp_dict = {}\n    for pred_k, pred_v in true_predictions.items():\n        true_total = len(pred_v)\n        num = 0\n        for tp_k, tp_v in tp_dict.items():\n            num += tp_v[pred_k]\n        tmp_dict[pred_k] = true_total - num\n    tmp_dict[\"O\"] = 0\n    tp_dict[\"O\"] = tmp_dict\n    return pd.DataFrame(tp_dict).T","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:04:08.633550Z","iopub.execute_input":"2022-08-06T20:04:08.633997Z","iopub.status.idle":"2022-08-06T20:04:08.661531Z","shell.execute_reply.started":"2022-08-06T20:04:08.633958Z","shell.execute_reply":"2022-08-06T20:04:08.660095Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"time: 4.36 ms (started: 2022-08-06 20:04:08 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = get_distribution(labels2, \"labels\")\ntrue_predictions = get_distribution(predictions2, \"predictions\")\ndf = get_confusion_matrix(true_labels, true_predictions)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:04:09.801811Z","iopub.execute_input":"2022-08-06T20:04:09.802536Z","iopub.status.idle":"2022-08-06T20:04:18.626610Z","shell.execute_reply.started":"2022-08-06T20:04:09.802499Z","shell.execute_reply":"2022-08-06T20:04:18.625569Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"The distribution of labels is:\nThere are 3292 occurances in Sustainability preoccupations\nThere are 2312 occurances in Digital transformation\nThere are 409 occurances in Change in management\nThere are 11764 occurances in Innovation activities\nThere are 2659 occurances in Business Model\nThere are 41 occurances in Corporate social responsibility ou CSR\nThere are 3 occurances in marco-label\nThe distribution of predictions is:\nThere are 3818 occurances in Sustainability preoccupations\nThere are 2432 occurances in Digital transformation\nThere are 332 occurances in Change in management\nThere are 13661 occurances in Innovation activities\nThere are 2867 occurances in Business Model\nThere are 0 occurances in Corporate social responsibility ou CSR\nThere are 0 occurances in marco-label\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                        Sustainability preoccupations  \\\nSustainability preoccupations                                    2681   \nDigital transformation                                              0   \nChange in management                                                0   \nInnovation activities                                               2   \nBusiness Model                                                      0   \nCorporate social responsibility ou CSR                              0   \nmarco-label                                                         0   \nO                                                                1135   \n\n                                        Digital transformation  \\\nSustainability preoccupations                                0   \nDigital transformation                                    1740   \nChange in management                                         0   \nInnovation activities                                        0   \nBusiness Model                                               0   \nCorporate social responsibility ou CSR                       0   \nmarco-label                                                  0   \nO                                                          692   \n\n                                        Change in management  \\\nSustainability preoccupations                              0   \nDigital transformation                                     0   \nChange in management                                     241   \nInnovation activities                                      6   \nBusiness Model                                             0   \nCorporate social responsibility ou CSR                     0   \nmarco-label                                                0   \nO                                                         85   \n\n                                        Innovation activities  Business Model  \\\nSustainability preoccupations                               1               0   \nDigital transformation                                      0               0   \nChange in management                                       15               0   \nInnovation activities                                   10055              16   \nBusiness Model                                              6            2565   \nCorporate social responsibility ou CSR                      0               0   \nmarco-label                                                 0               3   \nO                                                        3584             283   \n\n                                        Corporate social responsibility ou CSR  \\\nSustainability preoccupations                                                0   \nDigital transformation                                                       0   \nChange in management                                                         0   \nInnovation activities                                                        0   \nBusiness Model                                                               0   \nCorporate social responsibility ou CSR                                       0   \nmarco-label                                                                  0   \nO                                                                            0   \n\n                                        marco-label     O  \nSustainability preoccupations                     0   610  \nDigital transformation                            0   572  \nChange in management                              0   153  \nInnovation activities                             0  1685  \nBusiness Model                                    0    88  \nCorporate social responsibility ou CSR            0    41  \nmarco-label                                       0     0  \nO                                                 0     0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sustainability preoccupations</th>\n      <th>Digital transformation</th>\n      <th>Change in management</th>\n      <th>Innovation activities</th>\n      <th>Business Model</th>\n      <th>Corporate social responsibility ou CSR</th>\n      <th>marco-label</th>\n      <th>O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sustainability preoccupations</th>\n      <td>2681</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>610</td>\n    </tr>\n    <tr>\n      <th>Digital transformation</th>\n      <td>0</td>\n      <td>1740</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>572</td>\n    </tr>\n    <tr>\n      <th>Change in management</th>\n      <td>0</td>\n      <td>0</td>\n      <td>241</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>153</td>\n    </tr>\n    <tr>\n      <th>Innovation activities</th>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>10055</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1685</td>\n    </tr>\n    <tr>\n      <th>Business Model</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2565</td>\n      <td>0</td>\n      <td>0</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>Corporate social responsibility ou CSR</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>marco-label</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>O</th>\n      <td>1135</td>\n      <td>692</td>\n      <td>85</td>\n      <td>3584</td>\n      <td>283</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"time: 8.82 s (started: 2022-08-06 20:04:09 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"new_kws_dict = {}\nnew_kw_df = pd.DataFrame({})\nnew_kws_dict['predicted_keywords_lstm'] = predict_kws\nnew_kws_dict['new_keywords_epoch_lstm'] = new_kws_pos\n\nnew_kw_df = pd.concat([new_kw_df, df])\nnew_kw_df = new_kw_df.reset_index(drop=True)\nprint(\"The newly generated keywords and the predicted category corresponding to:\")\nprint(new_kw_df)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:05:01.129630Z","iopub.execute_input":"2022-08-06T20:05:01.132211Z","iopub.status.idle":"2022-08-06T20:05:01.142825Z","shell.execute_reply.started":"2022-08-06T20:05:01.132162Z","shell.execute_reply":"2022-08-06T20:05:01.141445Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"The newly generated keywords and the predicted category corresponding to:\n   Sustainability preoccupations  Digital transformation  \\\n0                           2681                       0   \n1                              0                    1740   \n2                              0                       0   \n3                              2                       0   \n4                              0                       0   \n5                              0                       0   \n6                              0                       0   \n7                           1135                     692   \n\n   Change in management  Innovation activities  Business Model  \\\n0                     0                      1               0   \n1                     0                      0               0   \n2                   241                     15               0   \n3                     6                  10055              16   \n4                     0                      6            2565   \n5                     0                      0               0   \n6                     0                      0               3   \n7                    85                   3584             283   \n\n   Corporate social responsibility ou CSR  marco-label     O  \n0                                       0            0   610  \n1                                       0            0   572  \n2                                       0            0   153  \n3                                       0            0  1685  \n4                                       0            0    88  \n5                                       0            0    41  \n6                                       0            0     0  \n7                                       0            0     0  \ntime: 4.3 ms (started: 2022-08-06 20:05:01 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"label_pred_dict = {}\nlabel_pred_dict[\"prediction\"] = predictions\nlabel_pred_dict[\"labels\"] = labels\n\nwith open(\"./label_pred_dict_1.pkl\", \"wb\") as f:\n    pickle.dump(label_pred_dict, f)\n\nwith open(\"./new_kws_dict_1.pkl\", \"wb\") as f:\n    pickle.dump(new_kws_dict, f)\n\nnew_kw_df.to_csv(\"./new-keywords-cate_1.csv\")  \n\ndf.to_csv(\"./confusion-matrix_1.csv\")\n\nprint(\"Saving models ... \")\ntorch.save(model.state_dict, \"./Model_state_dict_1.pickle\")\n\ntorch.save(model, \"./model_1.pickle\")","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:33:35.383852Z","iopub.execute_input":"2022-08-06T20:33:35.384722Z","iopub.status.idle":"2022-08-06T20:33:36.920967Z","shell.execute_reply.started":"2022-08-06T20:33:35.384681Z","shell.execute_reply":"2022-08-06T20:33:36.919570Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Saving models ... \ntime: 1.53 s (started: 2022-08-06 20:33:35 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/Fan5Shi/KeywordExtraction.git","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:31:56.237369Z","iopub.execute_input":"2022-08-06T20:31:56.237788Z","iopub.status.idle":"2022-08-06T20:31:58.167012Z","shell.execute_reply.started":"2022-08-06T20:31:56.237755Z","shell.execute_reply":"2022-08-06T20:31:58.165413Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Cloning into 'KeywordExtraction'...\nremote: Enumerating objects: 3, done.\u001b[K\nremote: Counting objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 596 bytes | 596.00 KiB/s, done.\ntime: 1.92 s (started: 2022-08-06 20:31:56 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git init","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:33:55.732146Z","iopub.execute_input":"2022-08-06T20:33:55.732571Z","iopub.status.idle":"2022-08-06T20:33:56.896127Z","shell.execute_reply.started":"2022-08-06T20:33:55.732534Z","shell.execute_reply":"2022-08-06T20:33:56.894728Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Initialized empty Git repository in /kaggle/working/.git/\ntime: 1.16 s (started: 2022-08-06 20:33:55 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git add .","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:34:48.190475Z","iopub.execute_input":"2022-08-06T20:34:48.191292Z","iopub.status.idle":"2022-08-06T20:35:20.142414Z","shell.execute_reply.started":"2022-08-06T20:34:48.191253Z","shell.execute_reply":"2022-08-06T20:35:20.141186Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"warning: adding embedded git repository: KeywordExtraction\n\u001b[33mhint: You've added another git repository inside your current repository.\u001b[m\n\u001b[33mhint: Clones of the outer repository will not contain the contents of\u001b[m\n\u001b[33mhint: the embedded repository and will not know how to obtain it.\u001b[m\n\u001b[33mhint: If you meant to add a submodule, use:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit submodule add <url> KeywordExtraction\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: If you added this path by mistake, you can remove it from the\u001b[m\n\u001b[33mhint: index with:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit rm --cached KeywordExtraction\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: See \"git help submodule\" for more information.\u001b[m\ntime: 31.9 s (started: 2022-08-06 20:34:48 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git config --global user.email \"963326056he@gmail.com\"\n!git config --global user.name \"Fan5Shi\"","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:36:39.869123Z","iopub.execute_input":"2022-08-06T20:36:39.869534Z","iopub.status.idle":"2022-08-06T20:36:42.245343Z","shell.execute_reply.started":"2022-08-06T20:36:39.869500Z","shell.execute_reply":"2022-08-06T20:36:42.244107Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"time: 2.37 s (started: 2022-08-06 20:36:39 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git commit -a","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:36:43.611161Z","iopub.execute_input":"2022-08-06T20:36:43.612342Z","iopub.status.idle":"2022-08-06T22:12:02.744169Z","shell.execute_reply.started":"2022-08-06T20:36:43.612259Z","shell.execute_reply":"2022-08-06T22:12:02.742811Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"hint: Waiting for your editor to close the file... \u001b7\u001b[?47h\u001b[>4;2m\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[22;2t\u001b[22;1t\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"/kaggle/working/.git/COMMIT_EDITMSG\" 24L, 693C\u001b[2;1H▽\u001b[6n\u001b[2;1H  \u001b[1;1H\u001b[>c\u001b]10;?\u0007\u001b]11;?\u0007\u001b[2;1H\u001b[34m# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n#\n# On branch \u001b[m\u001b[35mmaster\u001b[m\n\u001b[34m#\n# Initial commit\n#\n# \u001b[m\u001b[35mChanges to be committed:\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  KeywordExtraction\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  Model_state_dict.pickle\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  Model_state_dict_1.pickle\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  __notebook_source__.ipynb\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  confusion-matrix.csv\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  confusion-matrix_1.csv\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  label_pred_dict.pkl\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  label_pred_dict_1.pkl\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  model.pickle\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  model_1.pickle\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  new-keywords-cate.csv\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  new-keywords-cate_1.csv\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  new_kws_dict.pkl\u001b[m\n\u001b[34m#       \u001b[m\u001b[32mnew file\u001b[m\u001b[34m: \u001b[m\u001b[31m  new_kws_dict_1.pkl\u001b[m\u001b[24;63H1,0-1\u001b[9CTop\u001b[1;1H\u001b[?25h\u001b[?25l\u001b[24;1HType  :qa  and press <Enter> to exit Vim\u001b[24;41H\u001b[K\u0007\u001b[24;63H1,0-1\u001b[9CTop\u001b[1;1H\u001b[?25htime: 1h 35min 19s (started: 2022-08-06 20:36:43 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git push --set-upstream https://github.com/Fan5Shi/KeywordExtraction.git","metadata":{"execution":{"iopub.status.busy":"2022-08-06T22:13:31.132560Z","iopub.execute_input":"2022-08-06T22:13:31.132943Z","iopub.status.idle":"2022-08-06T22:13:32.318842Z","shell.execute_reply.started":"2022-08-06T22:13:31.132911Z","shell.execute_reply":"2022-08-06T22:13:32.317545Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"fatal: The current branch master has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream https://github.com/Fan5Shi/KeywordExtraction.git master\n\ntime: 1.18 s (started: 2022-08-06 22:13:31 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_precision(matrix):\n    keys = matrix.sum(axis=0).keys()\n    values = matrix.sum(axis=0).to_numpy()\n    return [(keys[i], np.nan_to_num(matrix.iloc[i][i] / values[i])) for i in range(len(matrix)-1)]\n\ndef get_recall(matrix):\n    keys = matrix.sum(axis=1).keys()\n    values = matrix.sum(axis=1).to_numpy()\n    return [(keys[i], np.nan_to_num(matrix.iloc[i][i] / values[i])) for i in range(len(matrix)-1)]\n\ndef get_f1score(matrix):\n    precision = get_precision(matrix)\n    recall = get_recall(matrix)\n    return [(precision[i][0], \n             np.nan_to_num(2 * precision[i][1] * recall[i][1] / (precision[i][1] + recall[i][1])))\n            for i in range(len(matrix)-1)]\n\ndef get_average_precision(matrix):\n    precision = get_precision(matrix)\n    return np.mean([precision[i][1] for i in range(len(precision))])\n\ndef get_average_recall(matrix):\n    recall = get_recall(matrix)\n    return np.mean([recall[i][1] for i in range(len(recall))])\n\ndef get_average_f1score(matrix):\n    precision = get_average_precision(matrix)\n    recall = get_average_recall(matrix)\n    return 2 * precision * recall / (precision + recall)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:11:35.519947Z","iopub.execute_input":"2022-08-06T20:11:35.520321Z","iopub.status.idle":"2022-08-06T20:11:35.533466Z","shell.execute_reply.started":"2022-08-06T20:11:35.520288Z","shell.execute_reply":"2022-08-06T20:11:35.531131Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"time: 1.87 ms (started: 2022-08-06 20:11:35 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"get_average_precision(df), get_average_recall(df), get_average_f1score(df)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:11:52.270913Z","iopub.execute_input":"2022-08-06T20:11:52.271282Z","iopub.status.idle":"2022-08-06T20:11:52.291010Z","shell.execute_reply.started":"2022-08-06T20:11:52.271250Z","shell.execute_reply":"2022-08-06T20:11:52.290040Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in long_scalars\n  after removing the cwd from sys.path.\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(0.5391806500168269, 0.5679443427203611, 0.5531888483959754)"},"metadata":{}},{"name":"stdout","text":"time: 15.3 ms (started: 2022-08-06 20:11:52 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass Highway(nn.Module):\n    \"\"\"\n    Highway Network.\n    \"\"\"\n\n    def __init__(self, size, num_layers=1, dropout=0.5):\n        \"\"\"\n        :param size: size of linear layer (matches input size)\n        :param num_layers: number of transform and gate layers\n        :param dropout: dropout\n        \"\"\"\n        super(Highway, self).__init__()\n        self.size = size\n        self.num_layers = num_layers\n        self.transform = nn.ModuleList()  # list of transform layers\n        self.gate = nn.ModuleList()  # list of gate layers\n        self.dropout = nn.Dropout(p=dropout)\n\n        for i in range(num_layers):\n            transform = nn.Linear(size, size)\n            gate = nn.Linear(size, size)\n            self.transform.append(transform)\n            self.gate.append(gate)\n\n    def forward(self, x):\n        \"\"\"\n        Forward propagation.\n\n        :param x: input tensor\n        :return: output tensor, with same dimensions as input tensor\n        \"\"\"\n        transformed = nn.functional.relu(self.transform[0](x))  # transform input\n        g = torch.sigmoid(self.gate[0](x))  # calculate how much of the transformed input to keep\n\n        out = g * transformed + (1 - g) * x  # combine input and transformed input in this ratio\n\n        # If there are additional layers\n        for i in range(1, self.num_layers):\n            out = self.dropout(out)\n            transformed = nn.functional.relu(self.transform[i](out))\n            g = torch.sigmoid(self.gate[i](out))\n\n            out = g * transformed + (1 - g) * out\n\n        return out\n\n\nclass CRF(nn.Module):\n    \"\"\"\n    Conditional Random Field.\n    \"\"\"\n\n    def __init__(self, hidden_dim, tagset_size):\n        \"\"\"\n        :param hidden_dim: size of word RNN/BLSTM's output\n        :param tagset_size: number of tags\n        \"\"\"\n        super(CRF, self).__init__()\n        self.tagset_size = tagset_size\n        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n        self.transition.data.zero_()\n\n    def forward(self, feats):\n        \"\"\"\n        Forward propagation.\n\n        :param feats: output of word RNN/BLSTM, a tensor of dimensions (batch_size, timesteps, hidden_dim)\n        :return: CRF scores, a tensor of dimensions (batch_size, timesteps, tagset_size, tagset_size)\n        \"\"\"\n        self.batch_size = feats.size(0)\n        self.timesteps = feats.size(1)\n\n        emission_scores = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n        emission_scores = emission_scores.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size,\n                                                              self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n\n        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(\n            0)  # (batch_size, timesteps, tagset_size, tagset_size)\n        return crf_scores\n\n\nclass LM_LSTM_CRF(nn.Module):\n    \"\"\"\n    The encompassing LM-LSTM-CRF model.\n    \"\"\"\n\n    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n        \"\"\"\n        :param tagset_size: number of tags\n        :param charset_size: size of character vocabulary\n        :param char_emb_dim: size of character embeddings\n        :param char_rnn_dim: size of character RNNs/LSTMs\n        :param char_rnn_layers: number of layers in character RNNs/LSTMs\n        :param vocab_size: input vocabulary size\n        :param lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n        :param word_emb_dim: size of word embeddings\n        :param word_rnn_dim: size of word RNN/BLSTM\n        :param word_rnn_layers:  number of layers in word RNNs/LSTMs\n        :param dropout: dropout\n        :param highway_layers: number of transform and gate layers\n        \"\"\"\n\n        super(LM_LSTM_CRF, self).__init__()\n\n        self.tagset_size = tagset_size  # this is the size of the output vocab of the tagging model\n\n        self.charset_size = charset_size\n        self.char_emb_dim = char_emb_dim\n        self.char_rnn_dim = char_rnn_dim\n        self.char_rnn_layers = char_rnn_layers\n\n        self.wordset_size = vocab_size  # this is the size of the input vocab (embedding layer) of the tagging model\n        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n        self.word_emb_dim = word_emb_dim\n        self.word_rnn_dim = word_rnn_dim\n        self.word_rnn_layers = word_rnn_layers\n\n        self.highway_layers = highway_layers\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim)  # character embedding layer\n        self.forw_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n                                      bidirectional=False, dropout=dropout)  # forward character LSTM\n        self.back_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n                                      bidirectional=False, dropout=dropout)  # backward character LSTM\n\n        self.word_embeds = nn.Embedding(self.wordset_size, self.word_emb_dim)  # word embedding layer\n        self.word_blstm = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, self.word_rnn_dim // 2,\n                                  num_layers=self.word_rnn_layers, bidirectional=True, dropout=dropout)  # word BLSTM\n\n        self.crf = CRF((self.word_rnn_dim // 2) * 2, self.tagset_size)  # conditional random field\n\n        self.forw_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n                                  dropout=dropout)  # highway to transform forward char LSTM output for the forward language model\n        self.back_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n                                  dropout=dropout)  # highway to transform backward char LSTM output for the backward language model\n        self.subword_hw = Highway(2 * self.char_rnn_dim, num_layers=self.highway_layers,\n                                  dropout=dropout)  # highway to transform combined forward and backward char LSTM outputs for use in the word BLSTM\n\n        self.forw_lm_out = nn.Linear(self.char_rnn_dim,\n                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the forward language model\n        self.back_lm_out = nn.Linear(self.char_rnn_dim,\n                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the backward language model\n\n    def init_word_embeddings(self, embeddings):\n        \"\"\"\n        Initialize embeddings with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        \"\"\"\n        self.word_embeds.weight = nn.Parameter(embeddings)\n\n    def fine_tune_word_embeddings(self, fine_tune=False):\n        \"\"\"\n        Fine-tune embedding layer? (Not fine-tuning only makes sense if using pre-trained embeddings).\n\n        :param fine_tune: Fine-tune?\n        \"\"\"\n        for p in self.word_embeds.parameters():\n            p.requires_grad = fine_tune\n\n    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n        \"\"\"\n        Forward propagation.\n\n        :param cmaps_f: padded encoded forward character sequences, a tensor of dimensions (batch_size, char_pad_len)\n        :param cmaps_b: padded encoded backward character sequences, a tensor of dimensions (batch_size, char_pad_len)\n        :param cmarkers_f: padded forward character markers, a tensor of dimensions (batch_size, word_pad_len)\n        :param cmarkers_b: padded backward character markers, a tensor of dimensions (batch_size, word_pad_len)\n        :param wmaps: padded encoded word sequences, a tensor of dimensions (batch_size, word_pad_len)\n        :param tmaps: padded tag sequences, a tensor of dimensions (batch_size, word_pad_len)\n        :param wmap_lengths: word sequence lengths, a tensor of dimensions (batch_size)\n        :param cmap_lengths: character sequence lengths, a tensor of dimensions (batch_size, word_pad_len)\n        \"\"\"\n        self.batch_size = cmaps_f.size(0)\n        self.word_pad_len = wmaps.size(1)\n\n        # Sort by decreasing true char. sequence length\n        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n        cmaps_f = cmaps_f[char_sort_ind]\n        cmaps_b = cmaps_b[char_sort_ind]\n        cmarkers_f = cmarkers_f[char_sort_ind]\n        cmarkers_b = cmarkers_b[char_sort_ind]\n        wmaps = wmaps[char_sort_ind]\n        tmaps = tmaps[char_sort_ind]\n        wmap_lengths = wmap_lengths[char_sort_ind]\n\n        # Embedding look-up for characters\n        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n        cb = self.char_embeds(cmaps_b)\n\n        # Dropout\n        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n        cb = self.dropout(cb)\n\n        # Pack padded sequence\n        cf = pack_padded_sequence(cf, cmap_lengths.tolist(),\n                                  batch_first=True)  # packed sequence of char_emb_dim, with real sequence lengths\n        cb = pack_padded_sequence(cb, cmap_lengths.tolist(), batch_first=True)\n\n        # LSTM\n        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n        cb, _ = self.back_char_lstm(cb)\n\n        # Unpack packed sequence\n        cf, _ = pad_packed_sequence(cf, batch_first=True)  # (batch_size, max_char_len_in_batch, char_rnn_dim)\n        cb, _ = pad_packed_sequence(cb, batch_first=True)\n\n        # Sanity check\n        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n\n        # Select RNN outputs only at marker points (spaces in the character sequence)\n        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n        cf_selected = torch.gather(cf, 1, cmarkers_f)  # (batch_size, word_pad_len, char_rnn_dim)\n        cb_selected = torch.gather(cb, 1, cmarkers_b)\n\n        # Only for co-training, not useful for tagging after model is trained\n        if self.training:\n            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n\n        # Sort by decreasing true word sequence length\n        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n        wmaps = wmaps[word_sort_ind]\n        tmaps = tmaps[word_sort_ind]\n        cf_selected = cf_selected[word_sort_ind]  # for language model\n        cb_selected = cb_selected[word_sort_ind]\n        if self.training:\n            lm_f_scores = lm_f_scores[word_sort_ind]\n            lm_b_scores = lm_b_scores[word_sort_ind]\n\n        # Embedding look-up for words\n        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n        w = self.dropout(w)\n\n        # Sub-word information at each word\n        subword = self.subword_hw(self.dropout(\n            torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n        subword = self.dropout(subword)\n\n        # Concatenate word embeddings and sub-word features\n        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, word_emb_dim + 2 * char_rnn_dim)\n\n        # Pack padded sequence\n        w = pack_padded_sequence(w, list(wmap_lengths),\n                                 batch_first=True)  # packed sequence of word_emb_dim + 2 * char_rnn_dim, with real sequence lengths\n\n        # LSTM\n        w, _ = self.word_blstm(w)  # packed sequence of word_rnn_dim, with real sequence lengths\n\n        # Unpack packed sequence\n        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n        w = self.dropout(w)\n\n        crf_scores = self.crf(w)  # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n\n        if self.training:\n            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n        else:\n            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req.\n\n\nclass ViterbiLoss(nn.Module):\n    \"\"\"\n    Viterbi Loss.\n    \"\"\"\n\n    def __init__(self, tag_map):\n        \"\"\"\n        :param tag_map: tag map\n        \"\"\"\n        super(ViterbiLoss, self).__init__()\n        self.tagset_size = len(tag_map)\n        self.start_tag = tag_map['<start>']\n        self.end_tag = tag_map['<end>']\n\n    def forward(self, scores, targets, lengths):\n        \"\"\"\n        Forward propagation.\n\n        :param scores: CRF scores\n        :param targets: true tags indices in unrolled CRF scores\n        :param lengths: word sequence lengths\n        :return: viterbi loss\n        \"\"\"\n\n        batch_size = scores.size(0)\n        word_pad_len = scores.size(1)\n\n        # Gold score\n\n        targets = targets.unsqueeze(2)\n        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n            2)  # (batch_size, word_pad_len)\n\n        # Everything is already sorted by lengths\n        scores_at_targets = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n        gold_score = scores_at_targets.data.sum()\n\n        # All paths' scores\n\n        # Create a tensor to hold accumulated sequence scores at each current tag\n        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n\n        for t in range(max(lengths)):\n            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n            if t == 0:\n                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n            else:\n                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n                # So, broadcast prev. timestep's cur_tag scores along cur. timestep's cur_tag dimension\n                scores_upto_t[:batch_size_t] = log_sum_exp(\n                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n                    dim=1)  # (batch_size, tagset_size)\n\n        # We only need the final accumulated scores at the <end> tag\n        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n\n        viterbi_loss = all_paths_scores - gold_score\n        viterbi_loss = viterbi_loss / batch_size\n\n        return viterbi_loss","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:42:53.256665Z","iopub.execute_input":"2022-09-09T08:42:53.257214Z","iopub.status.idle":"2022-09-09T08:42:53.312872Z","shell.execute_reply.started":"2022-09-09T08:42:53.257121Z","shell.execute_reply":"2022-09-09T08:42:53.311268Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"time: 14.7 ms (started: 2022-09-09 08:42:53 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import Counter\nimport codecs\nimport itertools\nfrom functools import reduce\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n\ndef read_words_tags(file, tag_ind, caseless=True):\n    \"\"\"\n    Reads raw data in the CoNLL 2003 format and returns word and tag sequences.\n\n    :param file: file with raw data in the CoNLL 2003 format\n    :param tag_ind: column index of tag\n    :param caseless: lowercase words?\n    :return: word, tag sequences\n    \"\"\"\n    with codecs.open(file, 'r', 'utf-8') as f:\n        lines = f.readlines()\n    words = []\n    tags = []\n    temp_w = []\n    temp_t = []\n    for line in lines:\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == '-DOCSTART-')):\n            feats = line.rstrip('\\n').split()\n            temp_w.append(feats[0].lower() if caseless else feats[0])\n            temp_t.append(feats[tag_ind])\n        elif len(temp_w) > 0:\n            assert len(temp_w) == len(temp_t)\n            words.append(temp_w)\n            tags.append(temp_t)\n            temp_w = []\n            temp_t = []\n    # last sentence\n    if len(temp_w) > 0:\n        assert len(temp_w) == len(temp_t)\n        words.append(temp_w)\n        tags.append(temp_t)\n\n    # Sanity check\n    assert len(words) == len(tags)\n\n    return words, tags\n\n\ndef create_maps(words, tags, min_word_freq=5, min_char_freq=1):\n    \"\"\"\n    Creates word, char, tag maps.\n\n    :param words: word sequences\n    :param tags: tag sequences\n    :param min_word_freq: words that occur fewer times than this threshold are binned as <unk>s\n    :param min_char_freq: characters that occur fewer times than this threshold are binned as <unk>s\n    :return: word, char, tag maps\n    \"\"\"\n    word_freq = Counter()\n    char_freq = Counter()\n    tag_map = set()\n    for w, t in zip(words, tags):\n        word_freq.update(w)\n        char_freq.update(list(reduce(lambda x, y: list(x) + [' '] + list(y), w)))\n        tag_map.update(t)\n\n    word_map = {k: v + 1 for v, k in enumerate([w for w in word_freq.keys() if word_freq[w] > min_word_freq])}\n    char_map = {k: v + 1 for v, k in enumerate([c for c in char_freq.keys() if char_freq[c] > min_char_freq])}\n    tag_map = {k: v + 1 for v, k in enumerate(tag_map)}\n\n    word_map['<pad>'] = 0\n    word_map['<end>'] = len(word_map)\n    word_map['<unk>'] = len(word_map)\n    char_map['<pad>'] = 0\n    char_map['<end>'] = len(char_map)\n    char_map['<unk>'] = len(char_map)\n    tag_map['<pad>'] = 0\n    tag_map['<start>'] = len(tag_map)\n    tag_map['<end>'] = len(tag_map)\n\n    return word_map, char_map, tag_map\n\n\ndef create_input_tensors(words, tags, word_map, char_map, tag_map):\n    \"\"\"\n    Creates input tensors that will be used to create a PyTorch Dataset.\n\n    :param words: word sequences\n    :param tags: tag sequences\n    :param word_map: word map\n    :param char_map: character map\n    :param tag_map: tag map\n    :return: padded encoded words, padded encoded forward chars, padded encoded backward chars,\n            padded forward character markers, padded backward character markers, padded encoded tags,\n            word sequence lengths, char sequence lengths\n    \"\"\"\n    # Encode sentences into word maps with <end> at the end\n    # [['dunston', 'checks', 'in', '<end>']] -> [[4670, 4670, 185, 4669]]\n    wmaps = list(map(lambda s: list(map(lambda w: word_map.get(w, word_map['<unk>']), s)) + [word_map['<end>']], words))\n\n    # Forward and backward character streams\n    # [['d', 'u', 'n', 's', 't', 'o', 'n', ' ', 'c', 'h', 'e', 'c', 'k', 's', ' ', 'i', 'n', ' ']]\n    chars_f = list(map(lambda s: list(reduce(lambda x, y: list(x) + [' '] + list(y), s)) + [' '], words))\n    # [['n', 'i', ' ', 's', 'k', 'c', 'e', 'h', 'c', ' ', 'n', 'o', 't', 's', 'n', 'u', 'd', ' ']]\n    chars_b = list(\n        map(lambda s: list(reversed([' '] + list(reduce(lambda x, y: list(x) + [' '] + list(y), s)))), words))\n\n    # Encode streams into forward and backward character maps with <end> at the end\n    # [[29, 2, 12, 8, 7, 14, 12, 3, 6, 18, 1, 6, 21, 8, 3, 17, 12, 3, 60]]\n    cmaps_f = list(\n        map(lambda s: list(map(lambda c: char_map.get(c, char_map['<unk>']), s)) + [char_map['<end>']], chars_f))\n    # [[12, 17, 3, 8, 21, 6, 1, 18, 6, 3, 12, 14, 7, 8, 12, 2, 29, 3, 60]]\n    cmaps_b = list(\n        map(lambda s: list(map(lambda c: char_map.get(c, char_map['<unk>']), s)) + [char_map['<end>']], chars_b))\n\n    # Positions of spaces and <end> character\n    # Words are predicted or encoded at these places in the language and tagging models respectively\n    # [[7, 14, 17, 18]] are points after '...dunston', '...checks', '...in', '...<end>' respectively\n    cmarkers_f = list(map(lambda s: [ind for ind in range(len(s)) if s[ind] == char_map[' ']] + [len(s) - 1], cmaps_f))\n    # Reverse the markers for the backward stream before adding <end>, so the words of the f and b markers coincide\n    # i.e., [[17, 9, 2, 18]] are points after '...notsnud', '...skcehc', '...ni', '...<end>' respectively\n    cmarkers_b = list(\n        map(lambda s: list(reversed([ind for ind in range(len(s)) if s[ind] == char_map[' ']])) + [len(s) - 1],\n            cmaps_b))\n\n    # Encode tags into tag maps with <end> at the end\n    tmaps = list(map(lambda s: list(map(lambda t: tag_map[t], s)) + [tag_map['<end>']], tags))\n    # Since we're using CRF scores of size (prev_tags, cur_tags), find indices of target sequence in the unrolled scores\n    # This will be row_index (i.e. prev_tag) * n_columns (i.e. tagset_size) + column_index (i.e. cur_tag)\n    tmaps = list(map(lambda s: [tag_map['<start>'] * len(tag_map) + s[0]] + [s[i - 1] * len(tag_map) + s[i] for i in\n                                                                             range(1, len(s))], tmaps))\n    # Note - the actual tag indices can be recovered with tmaps % len(tag_map)\n\n    # Pad, because need fixed length to be passed around by DataLoaders and other layers\n    word_pad_len = max(list(map(lambda s: len(s), wmaps)))\n    char_pad_len = max(list(map(lambda s: len(s), cmaps_f)))\n\n    # Sanity check\n    assert word_pad_len == max(list(map(lambda s: len(s), tmaps)))\n\n    padded_wmaps = []\n    padded_cmaps_f = []\n    padded_cmaps_b = []\n    padded_cmarkers_f = []\n    padded_cmarkers_b = []\n    padded_tmaps = []\n    wmap_lengths = []\n    cmap_lengths = []\n\n    for w, cf, cb, cmf, cmb, t in zip(wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps):\n        # Sanity  checks\n        assert len(w) == len(cmf) == len(cmb) == len(t)\n        assert len(cmaps_f) == len(cmaps_b)\n\n        # Pad\n        # A note -  it doesn't really matter what we pad with, as long as it's a valid index\n        # i.e., we'll extract output at those pad points (to extract equal lengths), but never use them\n\n        padded_wmaps.append(w + [word_map['<pad>']] * (word_pad_len - len(w)))\n        padded_cmaps_f.append(cf + [char_map['<pad>']] * (char_pad_len - len(cf)))\n        padded_cmaps_b.append(cb + [char_map['<pad>']] * (char_pad_len - len(cb)))\n\n        # 0 is always a valid index to pad markers with (-1 is too but torch.gather has some issues with it)\n        padded_cmarkers_f.append(cmf + [0] * (word_pad_len - len(w)))\n        padded_cmarkers_b.append(cmb + [0] * (word_pad_len - len(w)))\n\n        padded_tmaps.append(t + [tag_map['<pad>']] * (word_pad_len - len(t)))\n\n        wmap_lengths.append(len(w))\n        cmap_lengths.append(len(cf))\n\n        # Sanity check\n        assert len(padded_wmaps[-1]) == len(padded_tmaps[-1]) == len(padded_cmarkers_f[-1]) == len(\n            padded_cmarkers_b[-1]) == word_pad_len\n        assert len(padded_cmaps_f[-1]) == len(padded_cmaps_b[-1]) == char_pad_len\n\n    padded_wmaps = torch.LongTensor(padded_wmaps)\n    padded_cmaps_f = torch.LongTensor(padded_cmaps_f)\n    padded_cmaps_b = torch.LongTensor(padded_cmaps_b)\n    padded_cmarkers_f = torch.LongTensor(padded_cmarkers_f)\n    padded_cmarkers_b = torch.LongTensor(padded_cmarkers_b)\n    padded_tmaps = torch.LongTensor(padded_tmaps)\n    wmap_lengths = torch.LongTensor(wmap_lengths)\n    cmap_lengths = torch.LongTensor(cmap_lengths)\n\n    return padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, \\\n           wmap_lengths, cmap_lengths\n\n\ndef init_embedding(input_embedding):\n    \"\"\"\n    Initialize embedding tensor with values from the uniform distribution.\n\n    :param input_embedding: embedding tensor\n    :return:\n    \"\"\"\n    bias = np.sqrt(3.0 / input_embedding.size(1))\n    nn.init.uniform_(input_embedding, -bias, bias)\n\n\ndef load_embeddings(emb_file, word_map, expand_vocab=True):\n    \"\"\"\n    Load pre-trained embeddings for words in the word map.\n\n    :param emb_file: file with pre-trained embeddings (in the GloVe format)\n    :param word_map: word map\n    :param expand_vocab: expand vocabulary of word map to vocabulary of pre-trained embeddings?\n    :return: embeddings for words in word map, (possibly expanded) word map,\n            number of words in word map that are in-corpus (subject to word frequency threshold)\n    \"\"\"\n    with open(emb_file, 'r') as f:\n        emb_len = len(f.readline().split(' ')) - 1\n\n    print(\"Embedding length is %d.\" % emb_len)\n\n    # Create tensor to hold embeddings for words that are in-corpus\n    ic_embs = torch.FloatTensor(len(word_map), emb_len)\n    init_embedding(ic_embs)\n\n    if expand_vocab:\n        print(\"You have elected to include embeddings that are out-of-corpus.\")\n        ooc_words = []\n        ooc_embs = []\n    else:\n        print(\"You have elected NOT to include embeddings that are out-of-corpus.\")\n\n    # Read embedding file\n    print(\"\\nLoading embeddings...\")\n    for line in open(emb_file, 'r'):\n        line = line.split(' ')\n\n        emb_word = line[0]\n        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n\n        if not expand_vocab and emb_word not in word_map:\n            continue\n\n        # If word is in train_vocab, store at the correct index (as in the word_map)\n        if emb_word in word_map:\n            ic_embs[word_map[emb_word]] = torch.FloatTensor(embedding)\n\n        # If word is in dev or test vocab, store it and its embedding into lists\n        elif expand_vocab:\n            ooc_words.append(emb_word)\n            ooc_embs.append(embedding)\n\n    lm_vocab_size = len(word_map)  # keep track of lang. model's output vocab size (no out-of-corpus words)\n\n    if expand_vocab:\n        print(\"'word_map' is being updated accordingly.\")\n        for word in ooc_words:\n            word_map[word] = len(word_map)\n        ooc_embs = torch.FloatTensor(np.asarray(ooc_embs))\n        embeddings = torch.cat([ic_embs, ooc_embs], 0)\n\n    else:\n        embeddings = ic_embs\n\n    # Sanity check\n    assert embeddings.size(0) == len(word_map)\n\n    print(\"\\nDone.\\n Embedding vocabulary: %d\\n Language Model vocabulary: %d.\\n\" % (len(word_map), lm_vocab_size))\n\n    return embeddings, word_map, lm_vocab_size\n\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clip gradients computed during backpropagation to prevent gradient explosion.\n\n    :param optimizer: optimized with the gradients to be clipped\n    :param grad_clip: gradient clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best):\n    \"\"\"\n    Save model checkpoint.\n\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimized\n    :param val_f1: validation F1 score\n    :param word_map: word map\n    :param char_map: char map\n    :param tag_map: tag map\n    :param lm_vocab_size: number of words in-corpus, i.e. size of output vocabulary of linear model\n    :param is_best: is this checkpoint the best so far?\n    :return:\n    \"\"\"\n    state = {'epoch': epoch,\n             'f1': val_f1,\n             'model': model,\n             'optimizer': optimizer,\n             'word_map': word_map,\n             'tag_map': tag_map,\n             'char_map': char_map,\n             'lm_vocab_size': lm_vocab_size}\n    filename = 'checkpoint_lm_lstm_crf.pth.tar'\n    torch.save(state, filename)\n    # If checkpoint is the best so far, create a copy to avoid being overwritten by a subsequent worse checkpoint\n    if is_best:\n        torch.save(state, 'BEST_' + filename)\n\n\nclass AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, new_lr):\n    \"\"\"\n    Shrinks learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rates must be decayed\n    :param new_lr: new learning rate\n    \"\"\"\n\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n\n\ndef log_sum_exp(tensor, dim):\n    \"\"\"\n    Calculates the log-sum-exponent of a tensor's dimension in a numerically stable way.\n\n    :param tensor: tensor\n    :param dim: dimension to calculate log-sum-exp of\n    :return: log-sum-exp\n    \"\"\"\n    m, _ = torch.max(tensor, dim)\n    m_expanded = m.unsqueeze(dim).expand_as(tensor)\n    return m + torch.log(torch.sum(torch.exp(tensor - m_expanded), dim))","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:42:53.196504Z","iopub.execute_input":"2022-09-09T08:42:53.197510Z","iopub.status.idle":"2022-09-09T08:42:53.254005Z","shell.execute_reply.started":"2022-09-09T08:42:53.197467Z","shell.execute_reply":"2022-09-09T08:42:53.252547Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"time: 13.4 ms (started: 2022-09-09 08:42:53 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n\nclass ViterbiDecoder():\n    \"\"\"\n    Viterbi Decoder.\n    \"\"\"\n\n    def __init__(self, tag_map):\n        \"\"\"\n        :param tag_map: tag map\n        \"\"\"\n        self.tagset_size = len(tag_map)\n        self.start_tag = tag_map['<start>']\n        self.end_tag = tag_map['<end>']\n\n    def decode(self, scores, lengths):\n        \"\"\"\n        :param scores: CRF scores\n        :param lengths: word sequence lengths\n        :return: decoded sequences\n        \"\"\"\n        batch_size = scores.size(0)\n        word_pad_len = scores.size(1)\n\n        # Create a tensor to hold accumulated sequence scores at each current tag\n        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n\n        # Create a tensor to hold back-pointers\n        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n\n        for t in range(max(lengths)):\n            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n            if t == 0:\n                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n                                                               dtype=torch.long) * self.start_tag\n            else:\n                # We add scores at current timestep to scores accumulated up to previous timestep, and\n                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n                    dim=1)  # (batch_size, tagset_size)\n\n        # Decode/trace best path backwards\n        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n        pointer = torch.ones((batch_size, 1),\n                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n\n        for t in list(reversed(range(backpointers.size(1)))):\n            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n\n        # Sanity check\n        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n\n        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n                            dim=1)\n\n        return decoded","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:42:53.168839Z","iopub.execute_input":"2022-09-09T08:42:53.169403Z","iopub.status.idle":"2022-09-09T08:42:53.192606Z","shell.execute_reply.started":"2022-09-09T08:42:53.169343Z","shell.execute_reply":"2022-09-09T08:42:53.190840Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"time: 4.67 ms (started: 2022-09-09 08:42:53 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\n\nclass WCDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n    \"\"\"\n\n    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n        \"\"\"\n        :param wmaps: padded encoded word sequences\n        :param cmaps_f: padded encoded forward character sequences\n        :param cmaps_b: padded encoded backward character sequences\n        :param cmarkers_f: padded forward character markers\n        :param cmarkers_b: padded backward character markers\n        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n        :param wmap_lengths: word sequence lengths\n        :param cmap_lengths: character sequence lengths\n        \"\"\"\n        self.wmaps = wmaps\n        self.cmaps_f = cmaps_f\n        self.cmaps_b = cmaps_b\n        self.cmarkers_f = cmarkers_f\n        self.cmarkers_b = cmarkers_b\n        self.tmaps = tmaps\n        self.wmap_lengths = wmap_lengths\n        self.cmap_lengths = cmap_lengths\n\n        self.data_size = self.wmaps.size(0)\n\n    def __getitem__(self, i):\n        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n               self.wmap_lengths[i], self.cmap_lengths[i]\n\n    def __len__(self):\n        return self.data_size","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:42:53.148709Z","iopub.execute_input":"2022-09-09T08:42:53.150076Z","iopub.status.idle":"2022-09-09T08:42:53.165735Z","shell.execute_reply.started":"2022-09-09T08:42:53.150031Z","shell.execute_reply":"2022-09-09T08:42:53.164079Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"time: 4.8 ms (started: 2022-09-09 08:42:53 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Other models","metadata":{}},{"cell_type":"code","source":"# LSTM\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import utils as nn_utils\n\n\nclass LSTMEncoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(LSTMEncoder, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, batch_first=True,\n                            num_layers=1, bidirectional=True)\n        self.dropout = nn.Dropout(0.5)\n    \n\n    def init_hidden(self, batch_size):\n        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(device),\n                torch.randn(2, batch_size, self.hidden_dim // 2).to(device))\n    \n    def forward(self, sentences, seq_lens):\n        '''\n        Args:\n            sentences: batch_size*word_num, long tensor\n            seq_lens: batch_size, long tensor\n        Return:\n            hidden states for each sequence of tokens\n        '''\n        batch_size, word_num = sentences.size()\n        \n        self.hidden = self.init_hidden(batch_size)\n#         print(sentences.device)\n        embeds = self.word_embeds(sentences)#batch_size*word_num*emb_dim\n        embeds = self.dropout(embeds)\n        \n        #Sorting according to the lengths\n        perm_seq_lens, perm_idx = seq_lens.sort(0, descending=True)\n        _, desorted_perm_idx = torch.sort(perm_idx, descending=False)\n        embeds = embeds[perm_idx]\n        pack_embeds = nn_utils.rnn.pack_padded_sequence(embeds, \n                                                        perm_seq_lens.to('cpu'), batch_first=True)\n        \n        lstm_out, self.hidden = self.lstm(pack_embeds, self.hidden)\n        lstm_out, _ = nn_utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n        #Restore the order of sentences\n        return lstm_out[desorted_perm_idx]","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:13:38.343644Z","iopub.execute_input":"2022-07-28T15:13:38.344868Z","iopub.status.idle":"2022-07-28T15:13:40.713301Z","shell.execute_reply.started":"2022-07-28T15:13:38.344736Z","shell.execute_reply":"2022-07-28T15:13:40.711839Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"1. Word embedding\n2. Training","metadata":{}},{"cell_type":"code","source":"import torchtext\ndef tokenize_example(example, max_length=300):\n    return example.split(' ')[:max_length]\ndef split_tags(example, max_length=300):\n    return example.split(',')[:max_length]\n\ndata2['tokens'] = data2['sentence'].map(tokenize_example)\ndata2['labels'] = data2['word_labels2'].map(split_tags)\n\ntrain_size = 0.8\ntrain_dataset2 = data2.sample(frac=train_size,random_state=200)\ntest_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\ntrain_dataset2 = train_dataset2.reset_index(drop=True)\n\nmin_freq = 3\nspecial_tokens = ['<unk>', '<pad>']\n\nvocab = torchtext.vocab.build_vocab_from_iterator(train_dataset2['tokens'],\n                                                  min_freq=min_freq,\n                                                  specials=special_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:32:22.827302Z","iopub.execute_input":"2022-07-28T15:32:22.827822Z","iopub.status.idle":"2022-07-28T15:32:25.583377Z","shell.execute_reply.started":"2022-07-28T15:32:22.827772Z","shell.execute_reply":"2022-07-28T15:32:25.582080Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"unk_index = vocab['<unk>']\npad_index = vocab['<pad>']\nvocab.set_default_index(unk_index)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:33:43.648920Z","iopub.execute_input":"2022-07-28T15:33:43.649466Z","iopub.status.idle":"2022-07-28T15:33:43.655691Z","shell.execute_reply.started":"2022-07-28T15:33:43.649423Z","shell.execute_reply":"2022-07-28T15:33:43.654494Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectors = torchtext.vocab.FastText()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:34:07.640238Z","iopub.execute_input":"2022-07-27T17:34:07.640717Z","iopub.status.idle":"2022-07-27T17:43:49.361547Z","shell.execute_reply.started":"2022-07-27T17:34:07.640680Z","shell.execute_reply":"2022-07-27T17:43:49.359822Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":".vector_cache/wiki.en.vec: 6.60GB [03:23, 32.4MB/s]                                \n100%|██████████| 2519370/2519370 [05:20<00:00, 7849.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"time: 9min 41s (started: 2022-07-27 17:34:07 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\ndef sent2ids(text):\n    return [vocab[w] for w in text]\n\ndef labels2ids(labels):\n    return [labels_to_ids2[w] for w in labels]\n\n\ndef data_generator(sents, labels, batch_size=32, is_training=True, index=0):\n    if is_training:\n        select_indices = np.random.choice(len(sents), batch_size, replace=False)\n    else:\n        start = index\n        end = min(start + batch_size, len(sents)) \n        select_indices = list(range(start, end))\n    #select_indices = list(range(batch_size))\n    batch_sents = np.array(sents)[select_indices]\n    batch_labels = np.array(labels)[select_indices]\n    \n    batch_sents = list(map(sent2ids, batch_sents))\n    batch_labels = list(map(labels2ids, batch_labels))\n    \n    seq_lens = [len(s) for s in batch_sents]\n    seq_lens = torch.LongTensor(seq_lens)\n    max_len = max(seq_lens)\n    \n    batch_sents = [torch.LongTensor(s) for s in batch_sents]\n    \n    \n    batch_sents = pad_sequence(batch_sents, batch_first=True)\n    \n    if not is_training:\n        return batch_sents, batch_labels, seq_lens, end\n    batch_labels = [torch.LongTensor(s) for s in batch_labels]\n    batch_labels = pad_sequence(batch_labels, batch_first=True)\n\n    return batch_sents, batch_labels, seq_lens","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:45:30.378296Z","iopub.execute_input":"2022-07-27T17:45:30.379340Z","iopub.status.idle":"2022-07-27T17:45:30.393275Z","shell.execute_reply.started":"2022-07-27T17:45:30.379283Z","shell.execute_reply":"2022-07-27T17:45:30.392297Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"time: 2.07 ms (started: 2022-07-27 17:45:30 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# deal with the input format\ntrain_sent_words = train_dataset2['tokens']\ntrain_sent_tags = train_dataset2['labels']\nbatch_sents, batch_label, seq_lens = data_generator(train_sent_words, train_sent_tags, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:45:30.664632Z","iopub.execute_input":"2022-07-27T17:45:30.664929Z","iopub.status.idle":"2022-07-27T17:45:30.692375Z","shell.execute_reply.started":"2022-07-27T17:45:30.664903Z","shell.execute_reply":"2022-07-27T17:45:30.691259Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"time: 23.1 ms (started: 2022-07-27 17:45:30 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"class LinearChainCRF(nn.Module):\n    def __init__(self, label_size):\n        super(LinearChainCRF, self).__init__()\n        \n        self.label_size = label_size\n        self.start_label = label_size - 1\n        self.stop_label = label_size - 2\n        self.transitions = nn.Parameter(\n            torch.randn(self.label_size, self.label_size))\n        self.transitions.data[:, self.start_label] = -10000\n        self.transitions.data[self.stop_label] = -10000\n     \n    def forward_alg(self, features):\n        '''\n        Forward algorithm\n        Arg:\n            features: features of a sentence, sent_len*label_size, tensor\n        Return:\n            alpha: the log sum of \n        '''\n        #Complete the code\n        init_alphas = torch.full((1, self.label_size), -10000.)\n        # START_TAG has all of the score.\n        init_alphas[0, self.start_label] = 0.\n\n        # Wrap in a variable so that we will get automatic backprop\n        forward_var = init_alphas.to(device)\n        # Iterate through the sentence\n        for feat in features:\n            alphas_t = []  # The forward tensors at this timestep\n            for next_tag in range(self.label_size):\n                # broadcast the emission score: it is the same regardless of\n                # the previous tag\n                emit_score = feat[next_tag].view(\n                    1, -1).expand(1, self.label_size)\n                # the ith entry of trans_score is the score of transitioning to\n                # next_tag from i\n                trans_score = self.transitions[:, next_tag].view(1, -1)\n                # The ith entry of next_tag_var is the value for the\n                # edge (i -> next_tag) before we do log-sum-exp\n                next_tag_var = forward_var + trans_score + emit_score\n                # The forward variable for this tag is log-sum-exp of all the\n                # scores.\n                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n            forward_var = torch.cat(alphas_t).view(1, -1)\n        terminal_var = forward_var + self.transitions[:, self.stop_label]\n        alpha = log_sum_exp(terminal_var)\n        return alpha\n    \n    def sentence_scorer(self, features, labels):\n        '''\n        Score a sentence given its label sequence\n        Args:\n            features: features of a sentence, sent_len*label_size, tensor\n            labels: a sequence of labels, sent_len, tensor\n        Return:\n            score: the score of the labeled sentence, a scalar\n        '''\n        #Complete the code\n        score = torch.zeros(1).to(device)\n        labels = torch.cat([torch.tensor([self.start_label], dtype=torch.long).to(device), labels])\n        for i, feat in enumerate(features):\n#             print(labels[i+1])\n            score = score + \\\n                self.transitions[labels[i], labels[i + 1]] + feat[labels[i + 1]]\n        score = score + self.transitions[labels[-1], torch.tensor([self.stop_label], dtype=torch.long).to(device)]\n        return score\n    \n    def viterbi_decoder(self, features):\n        '''\n        Viterbi decoder\n        Arg:\n            features: features of a sentence, sent_len*label_size, tensor\n        Return:\n            best_path_score: best path score, a scalar\n            best_path: best path, a list\n        '''\n        #Complete the code\n        backpointers = []\n\n        # Initialize the viterbi variables in log space\n        init_vvars = torch.full((1, self.label_size), -10000.)\n        init_vvars[0][self.start_label] = 0\n        # forward_var at step i holds the viterbi variables for step i-1\n        forward_var = init_vvars.to(device)\n        for feat in features:\n            bptrs_t = []  # holds the backpointers for this step\n            viterbivars_t = []  # holds the viterbi variables for this step\n\n            for next_tag in range(self.label_size):\n                # next_tag_var[i] holds the viterbi variable for tag i at the\n                # previous step, plus the score of transitioning\n                # from tag i to next_tag.\n                # We don't include the emission scores here because the max\n                # does not depend on them (we add them in below)\n                next_tag_var = forward_var + self.transitions[:, next_tag].view(1, -1)\n                best_tag_id = argmax(next_tag_var)\n                bptrs_t.append(best_tag_id)\n                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n            # Now add in the emission scores, and assign forward_var to the set\n            # of viterbi variables we just computed\n            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append(bptrs_t)\n\n        # Transition to STOP_TAG\n        terminal_var = forward_var + self.transitions[:, self.stop_label].view(1, -1)\n        best_tag_id = argmax(terminal_var)\n        best_path_score = terminal_var[0][best_tag_id]\n\n        # Follow the back pointers to decode the best path.\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n        # Pop off the start tag (we dont want to return that to the caller)\n        start = best_path.pop()\n        assert start == self.start_label  # Sanity check\n        best_path.reverse()\n        return best_path_score, best_path\n    \n\ndef argmax(x):\n    # return the argmax as a python int\n    _, idx = torch.max(x, 1)\n    return idx.item()\n\n    \ndef log_sum_exp(x):\n    #return log sum exp of a tensor\n    m = torch.max(x, -1)[0]\n    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(-1)), -1))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:45:31.048232Z","iopub.execute_input":"2022-07-27T17:45:31.048612Z","iopub.status.idle":"2022-07-27T17:45:31.112781Z","shell.execute_reply.started":"2022-07-27T17:45:31.048580Z","shell.execute_reply":"2022-07-27T17:45:31.111650Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"time: 4.47 ms (started: 2022-07-27 17:45:31 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"class SequenceLabeling(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, path):\n        super(SequenceLabeling, self).__init__()\n        self.crf = LinearChainCRF(label_size)\n        self.bilstm = LSTMEncoder(vocab_size, embedding_dim, hidden_dim)\n        self.hidden2label_space = nn.Linear(hidden_dim, label_size)\n        self.load_pretrained_emb(path)\n        \n        \n    def forward(self, sentences, seq_lens, sent_tags):\n        '''\n        Compute the loss\n        Args:\n            sentences: batch_size*word_num, long tensor\n            seq_lens: batch_size, long tensor\n            sent_tags: batch_size*word_num, long tensor\n        Return:\n            loss: the average loss of a batch\n        '''\n        sentences = sentences.to(device)\n        seq_lens = seq_lens.to(device)\n        sent_tags = sent_tags.to(device)\n        hiddens = self.bilstm(sentences, seq_lens)\n        #batch_size*word_num*tag_size\n        batch_feats = self.hidden2label_space(hiddens)\n        neg_likelihoods = []\n        for i, feats in enumerate(batch_feats):\n            length = seq_lens[i]\n            tags = sent_tags[i]\n#             print(feats.device, tags.device)\n            gold_score = self.crf.sentence_scorer(feats[:length], tags[:length])\n            forward_score = self.crf.forward_alg(feats[:length])\n            neg_likelihoods.append(forward_score-gold_score)\n        loss = torch.stack(neg_likelihoods).mean()\n        return loss\n\n    def decoder(self, sentences, seq_lens):\n        '''\n        Viterbi decoder\n        Args:\n            sentences: batch_size*word_num, long tensor\n            seq_lens: batch_size, long tensor\n        Return:\n            pred_path: batch_size, list\n        '''\n        sentences = sentences.to(device)\n        seq_lens = seq_lens.to(device)\n        hiddens = self.bilstm(sentences, seq_lens)\n        #batch_size*word_num*tag_size\n        batch_feats = self.hidden2label_space(hiddens)\n        preds = []\n        for i, feats in enumerate(batch_feats):\n            length = seq_lens[i]\n            _, pred = self.crf.viterbi_decoder(feats[:length])\n            preds.append(pred)\n        return preds\n    \n    def load_pretrained_emb(self, vectors):\n        '''\n        Load pre-trained word embeddings from the path\n        Arg:\n            path: the binary file of local Glove embeddings\n        '''\n#         with open(path, 'rb') as f:\n# pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n# model.embedding.weight.data = pretrained_embedding\n        \n        print(\"Vectors with shape {}\".format(vectors.shape))\n        print(f\"Size of bi-LSTM {self.bilstm.word_embeds.weight.size()}\")\n        assert vectors.shape == self.bilstm.word_embeds.weight.size()\n        self.bilstm.word_embeds.weight.data = vectors\n        self.bilstm.word_embeds.weight.requires_grad = False\n        print('embeddings loaded')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:45:31.419367Z","iopub.execute_input":"2022-07-27T17:45:31.421197Z","iopub.status.idle":"2022-07-27T17:45:31.437667Z","shell.execute_reply.started":"2022-07-27T17:45:31.421168Z","shell.execute_reply":"2022-07-27T17:45:31.436705Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"time: 2.69 ms (started: 2022-07-27 17:45:31 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import optim\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\npretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\nmodel = SequenceLabeling(len(vocab), 300, 64, len(labels_to_ids2), pretrained_embedding)\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:45:31.833352Z","iopub.execute_input":"2022-07-27T17:45:31.833988Z","iopub.status.idle":"2022-07-27T17:45:33.089734Z","shell.execute_reply.started":"2022-07-27T17:45:31.833955Z","shell.execute_reply":"2022-07-27T17:45:33.088772Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"cuda\nVectors with shape torch.Size([32731, 300])\nSize of bi-LSTM torch.Size([32731, 300])\nembeddings loaded\ntime: 1.25 s (started: 2022-07-27 17:45:31 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"from time import time\nepoch = 3\nloop_num = int(len(train_sent_words)/32)\nprint(f\"The loop numbers: {loop_num}\")\nfor i in range(epoch):\n    start = time()\n    model.train()\n    for j in range(loop_num):\n        optimizer.zero_grad()\n        batch_sents, batch_tags, seq_lens = data_generator(train_sent_words,train_sent_tags)\n        loss = model(batch_sents, seq_lens, batch_tags)\n        loss.backward()\n        optimizer.step()\n        if j % 20 == 0:\n            print(f'Loss: {loss.item()} === Cost time per step: {time() - start}')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T05:33:42.759430Z","iopub.execute_input":"2022-07-28T05:33:42.760312Z","iopub.status.idle":"2022-07-28T05:33:42.876578Z","shell.execute_reply.started":"2022-07-28T05:33:42.760194Z","shell.execute_reply":"2022-07-28T05:33:42.874773Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/1045646490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloop_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sent_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The loop numbers: {loop_num}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_sent_words' is not defined"],"ename":"NameError","evalue":"name 'train_sent_words' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# model_scripted = torch.jit.script(model) # Export to TorchScript\n# model_scripted.save('model_scripted.pt') # Save","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:00:07.945314Z","iopub.execute_input":"2022-07-27T10:00:07.945769Z","iopub.status.idle":"2022-07-27T10:00:07.978087Z","shell.execute_reply.started":"2022-07-27T10:00:07.945736Z","shell.execute_reply":"2022-07-27T10:00:07.977010Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"time: 11.9 ms (started: 2022-07-27 10:00:07 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(model, \"Model_entire.pickle\")\n# # model.state_dict","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:00:08.131815Z","iopub.execute_input":"2022-07-27T10:00:08.132177Z","iopub.status.idle":"2022-07-27T10:00:08.161393Z","shell.execute_reply.started":"2022-07-27T10:00:08.132148Z","shell.execute_reply":"2022-07-27T10:00:08.160318Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"time: 10.5 ms (started: 2022-07-27 10:00:08 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"code","source":"dev_sent_words = test_dataset2['tokens']\ndev_sent_tags = test_dataset2['labels']\nmodel.eval()\nindex = 0\npred_label_list = []\nwhile index < len(dev_sent_words):\n    batch_sents, batch_tags, seq_lens, index = data_generator(dev_sent_words, \n                                                              dev_sent_tags, batch_size=32, \n                                                              is_training=False, index=index)\n    pred_labels = model.decoder(batch_sents, seq_lens)\n    for label_seq in pred_labels:\n        pred_labels = [ids_to_labels2[t] for t in label_seq]\n        pred_label_list.append(pred_labels)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T12:21:49.850673Z","iopub.execute_input":"2022-07-27T12:21:49.851049Z","iopub.status.idle":"2022-07-27T12:21:57.955999Z","shell.execute_reply.started":"2022-07-27T12:21:49.851018Z","shell.execute_reply":"2022-07-27T12:21:57.954820Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"time: 8.05 s (started: 2022-07-27 12:21:49 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = [p for pl in pred_label_list for p in pl]\nlabels = [d for dev in dev_sent_tags for d in dev]\nwords = [d for dev in dev_sent_words for d in dev]\nlen(predictions), len(labels), len(words)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T12:22:02.705119Z","iopub.execute_input":"2022-07-27T12:22:02.705569Z","iopub.status.idle":"2022-07-27T12:22:02.727473Z","shell.execute_reply.started":"2022-07-27T12:22:02.705528Z","shell.execute_reply":"2022-07-27T12:22:02.726578Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(9685, 9685, 9685)"},"metadata":{}},{"name":"stdout","text":"time: 14.6 ms (started: 2022-07-27 12:22:02 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# newly generated words\ndef lengthOfTokens(pair):\n    return len(pair.split())\n\ndef countKeywords(test_dataset, model):\n    kws_pairs = []\n    for tmp_num in range(len(test_dataset)):\n        sentence = test_dataset[\"sentence\"].iloc[tmp_num]\n\n        inputs = tokenizer(sentence.split(),\n                            is_split_into_words=True, \n                            return_offsets_mapping=True, \n                            padding='max_length', \n                            truncation=True, \n                            max_length=MAX_LEN,\n                            return_tensors=\"pt\")\n\n        # move to gpu\n        ids = inputs[\"input_ids\"].to(device)\n        mask = inputs[\"attention_mask\"].to(device)\n        # forward pass\n        outputs = model(ids, attention_mask=mask)\n        logits = outputs.logits\n\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n\n        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n        token_predictions = [ids_to_labels2[i] for i in flattened_predictions.cpu().numpy()]\n        wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n        prediction = []\n        for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n          #only predictions on first word pieces are important\n          if mapping[0] == 0 and mapping[1] != 0:\n            prediction.append(token_pred[1])\n          else:\n            continue\n\n        for i in zip(sentence.split(), prediction, test_dataset[\"word_labels2\"].iloc[tmp_num].split(',')):\n            kws_pairs.append(i)\n\n    predict_kws = []\n    tmp_wd = []\n    tmp_pred, tmp_gold = '', ''\n    for word, pred, gold in kws_pairs:\n        if pred != 'O':\n            if len(tmp_wd) == 0: \n                tmp_pred = pred\n            else:\n                if tmp_pred != pred:\n                    predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                    tmp_wd = []\n                    tmp_pred = pred\n            tmp_wd.append(word)\n        else:\n            if len(tmp_wd) > 0:\n                predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                tmp_wd = []\n                tmp_pred = pred\n\n    # check whether there are new keywords\n    gold_dict = build_gold_dict()\n    new_kws, kw_tags = [], []\n    gold_lists = list(gold_dict.keys())\n    gold_lists = [g.lower() for g in gold_lists]\n    for word, label in predict_kws:\n        if not word.lower() in gold_lists:\n            new_kws.append(word.lower())\n            kw_tags.append(tag2cat[label])\n    new_keywords = list(zip(new_kws, kw_tags))\n\n    new_kws_dict, new_kws_pos = {}, []\n    for kw, cat in new_keywords:\n        pos = pos_tag(kw.split())\n        if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n            key = ' '.join([p for p, t in pos])\n            tag = [t for p, t in pos]\n            new_kws_pos.append(key)\n            if not key in new_kws_dict.keys():\n                new_kws_dict[key] = (tag, cat)\n\n    new_kws = list(new_kws_dict.keys())\n    cats = [value[1] for value in new_kws_dict.values()]\n\n    df = pd.DataFrame({\"keyword\": new_kws, \"category\": cats})\n\n    s = df['keyword'].apply(lengthOfTokens)\n    s.sort_values()\n\n    predict_kws = Counter(predict_kws)\n    new_kws_pos = Counter(new_kws_pos)\n\n    predict_kws = sorted(predict_kws.items(), key=lambda pair: pair[1], reverse=True)\n    new_kws_pos = sorted(new_kws_pos.items(), key=lambda pair: pair[1], reverse=True)\n\n    return predict_kws, new_kws_pos, df","metadata":{"execution":{"iopub.status.busy":"2022-07-27T12:22:14.430118Z","iopub.execute_input":"2022-07-27T12:22:14.430541Z","iopub.status.idle":"2022-07-27T12:22:14.455511Z","shell.execute_reply.started":"2022-07-27T12:22:14.430507Z","shell.execute_reply":"2022-07-27T12:22:14.454328Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"time: 4.24 ms (started: 2022-07-27 12:22:14 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokens = [t for tags in dev_sent_tags for t in tags]\n# token_predictions = [p for pred in pred_label_list for p in pred]\n# len(tokens), len(token_predictions)\nkws_pairs = []\nfor i in zip(words, predictions, labels):\n    kws_pairs.append(i)\n    \npredict_kws = []\ntmp_wd = []\ntmp_pred, tmp_gold = '', ''\nfor word, pred, gold in kws_pairs:\n    if pred != 'O':\n        if len(tmp_wd) == 0: \n            tmp_pred = pred\n        else:\n            if tmp_pred != pred:\n                predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                tmp_wd = []\n                tmp_pred = pred\n        tmp_wd.append(word)\n    else:\n        if len(tmp_wd) > 0:\n            predict_kws.append((' '.join(tmp_wd), tmp_pred))\n            tmp_wd = []\n            tmp_pred = pred\n\n# check whether there are new keywords\ngold_dict = build_gold_dict()\nnew_kws, kw_tags = [], []\ngold_lists = list(gold_dict.keys())\ngold_lists = [g.lower() for g in gold_lists]\nfor word, label in predict_kws:\n    if not word.lower() in gold_lists:\n        new_kws.append(word.lower())\n        kw_tags.append(tag2cat[label])\nnew_keywords = list(zip(new_kws, kw_tags))\n\nnew_kws_dict, new_kws_pos = {}, []\nfor kw, cat in new_keywords:\n    pos = pos_tag(kw.split())\n    if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n        key = ' '.join([p for p, t in pos])\n        tag = [t for p, t in pos]\n        new_kws_pos.append(key)\n        if not key in new_kws_dict.keys():\n            new_kws_dict[key] = (tag, cat)\n\nnew_kws = list(new_kws_dict.keys())\ncats = [value[1] for value in new_kws_dict.values()]\n\ndf = pd.DataFrame({\"keyword\": new_kws, \"category\": cats})\n\ns = df['keyword'].apply(lengthOfTokens)\ns.sort_values()\n\npredict_kws = Counter(predict_kws)\nnew_kws_pos = Counter(new_kws_pos)\n\npredict_kws = sorted(predict_kws.items(), key=lambda pair: pair[1], reverse=True)\nnew_kws_pos = sorted(new_kws_pos.items(), key=lambda pair: pair[1], reverse=True)\n\n# predict_kws, new_kws_pos, df\n\n# predict_kws, new_kws_pos, tmp_kws_df = countKeywords(test_dataset2, model)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T12:25:07.219114Z","iopub.execute_input":"2022-07-27T12:25:07.219465Z","iopub.status.idle":"2022-07-27T12:25:07.826986Z","shell.execute_reply.started":"2022-07-27T12:25:07.219435Z","shell.execute_reply":"2022-07-27T12:25:07.825281Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n  import sys\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n  \n","output_type":"stream"},{"name":"stdout","text":"time: 592 ms (started: 2022-07-27 12:25:07 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"new_kws_dict = {}\nnew_kw_df = pd.DataFrame({})\nnew_kws_dict['predicted_keywords_lstm'] = predict_kws\nnew_kws_dict['new_keywords_epoch_lstm'] = new_kws_pos\n\nnew_kw_df = pd.concat([new_kw_df, df])\nnew_kw_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T12:27:08.044543Z","iopub.execute_input":"2022-07-27T12:27:08.044891Z","iopub.status.idle":"2022-07-27T12:27:08.063148Z","shell.execute_reply.started":"2022-07-27T12:27:08.044859Z","shell.execute_reply":"2022-07-27T12:27:08.061836Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"          keyword              category\n0       aerospace  Change in management\n1            cost  Change in management\n2           intel        Business Model\n3            core  Change in management\n4        property  Change in management\n..            ...                   ...\n109       america        Business Model\n110        russia  Change in management\n111  subsidiaries        Business Model\n112        future  Change in management\n113     taxationc        Business Model\n\n[114 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aerospace</td>\n      <td>Change in management</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cost</td>\n      <td>Change in management</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>intel</td>\n      <td>Business Model</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>core</td>\n      <td>Change in management</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>property</td>\n      <td>Change in management</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>america</td>\n      <td>Business Model</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>russia</td>\n      <td>Change in management</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>subsidiaries</td>\n      <td>Business Model</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>future</td>\n      <td>Change in management</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>taxationc</td>\n      <td>Business Model</td>\n    </tr>\n  </tbody>\n</table>\n<p>114 rows × 2 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"time: 13.2 ms (started: 2022-07-27 12:27:08 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_index_positions(list_of_elems, element):\n    ''' Returns the indexes of all occurrences of give element in\n    the list- listOfElements '''\n    index_pos_list = []\n    index_pos = 0\n    while True:\n        try:\n            # Search for item in list from indexPos to the end of list\n            index_pos = list_of_elems.index(element, index_pos)\n            # Add the index position in list\n            index_pos_list.append(index_pos)\n            index_pos += 1\n        except ValueError as e:\n            break\n    return index_pos_list\n\ndef get_distribution(inputs, name):\n    true_res = {}\n    for k, v in tag2cat.items():\n        indexlist = get_index_positions(inputs, k)\n        left, right = 0, 0\n        new_labels = []\n        for i in indexlist:\n            if left == 0:\n                left = i\n                right = i\n                continue\n            if right+1 == i:\n                right = i\n            else:\n                new_labels.append((left, right))\n                left = i\n                right = i\n        true_res[v] = new_labels\n    print(f\"The distribution of {name} is:\")\n    for k,v in true_res.items():\n        print(f\"There are {len(v)} occurances in {k}\")\n    return true_res\n\ndef get_confusion_matrix(true_labels, true_predictions):\n    # strict mode\n    tp_dict = {}\n    for true_k, true_v in true_labels.items():\n        true_total = len(true_v)\n        tmp_dict = {}\n        tmp_total = 0\n        for pred_k, pred_v in true_predictions.items():\n            num = 0\n            for v in pred_v:\n                if v in true_v:\n                    num += 1\n            tmp_dict[pred_k] = num\n            tmp_total += num\n        tmp_dict[\"O\"] = true_total - tmp_total\n        tp_dict[true_k] = tmp_dict\n    tp_dict\n    tmp_dict = {}\n    for pred_k, pred_v in true_predictions.items():\n        true_total = len(pred_v)\n        num = 0\n        for tp_k, tp_v in tp_dict.items():\n            num += tp_v[pred_k]\n        tmp_dict[pred_k] = true_total - num\n    tmp_dict[\"O\"] = 0\n    tp_dict[\"O\"] = tmp_dict\n    return pd.DataFrame(tp_dict).T","metadata":{"execution":{"iopub.status.busy":"2022-08-01T15:58:26.446118Z","iopub.execute_input":"2022-08-01T15:58:26.446822Z","iopub.status.idle":"2022-08-01T15:58:26.467592Z","shell.execute_reply.started":"2022-08-01T15:58:26.446785Z","shell.execute_reply":"2022-08-01T15:58:26.466675Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"time: 3.05 ms (started: 2022-08-01 15:58:26 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = get_distribution(labels, \"labels\")\ntrue_predictions = get_distribution(predictions, \"predictions\")\ndf = get_confusion_matrix(true_labels, true_predictions)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-07-27T12:28:08.411799Z","iopub.execute_input":"2022-07-27T12:28:08.412524Z","iopub.status.idle":"2022-07-27T12:28:08.432451Z","shell.execute_reply.started":"2022-07-27T12:28:08.412488Z","shell.execute_reply":"2022-07-27T12:28:08.431123Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"The distribution of labels is:\nThere are 16 occurances in Sustainability preoccupations\nThere are 14 occurances in Digital transformation\nThere are 1 occurances in Change in management\nThere are 27 occurances in Innovation activities\nThere are 10 occurances in Business Model\nThere are 0 occurances in Corporate social responsibility ou CSR\nThere are 0 occurances in marco-label\nThe distribution of predictions is:\nThere are 0 occurances in Sustainability preoccupations\nThere are 0 occurances in Digital transformation\nThere are 198 occurances in Change in management\nThere are 23 occurances in Innovation activities\nThere are 139 occurances in Business Model\nThere are 0 occurances in Corporate social responsibility ou CSR\nThere are 0 occurances in marco-label\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                        Sustainability preoccupations  \\\nSustainability preoccupations                                       0   \nDigital transformation                                              0   \nChange in management                                                0   \nInnovation activities                                               0   \nBusiness Model                                                      0   \nCorporate social responsibility ou CSR                              0   \nmarco-label                                                         0   \nO                                                                   0   \n\n                                        Digital transformation  \\\nSustainability preoccupations                                0   \nDigital transformation                                       0   \nChange in management                                         0   \nInnovation activities                                        0   \nBusiness Model                                               0   \nCorporate social responsibility ou CSR                       0   \nmarco-label                                                  0   \nO                                                            0   \n\n                                        Change in management  \\\nSustainability preoccupations                              0   \nDigital transformation                                     0   \nChange in management                                       0   \nInnovation activities                                      0   \nBusiness Model                                             0   \nCorporate social responsibility ou CSR                     0   \nmarco-label                                                0   \nO                                                        198   \n\n                                        Innovation activities  Business Model  \\\nSustainability preoccupations                               0               0   \nDigital transformation                                      0               0   \nChange in management                                        0               0   \nInnovation activities                                       0               0   \nBusiness Model                                              0               0   \nCorporate social responsibility ou CSR                      0               0   \nmarco-label                                                 0               0   \nO                                                          23             139   \n\n                                        Corporate social responsibility ou CSR  \\\nSustainability preoccupations                                                0   \nDigital transformation                                                       0   \nChange in management                                                         0   \nInnovation activities                                                        0   \nBusiness Model                                                               0   \nCorporate social responsibility ou CSR                                       0   \nmarco-label                                                                  0   \nO                                                                            0   \n\n                                        marco-label   O  \nSustainability preoccupations                     0  16  \nDigital transformation                            0  14  \nChange in management                              0   1  \nInnovation activities                             0  27  \nBusiness Model                                    0  10  \nCorporate social responsibility ou CSR            0   0  \nmarco-label                                       0   0  \nO                                                 0   0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sustainability preoccupations</th>\n      <th>Digital transformation</th>\n      <th>Change in management</th>\n      <th>Innovation activities</th>\n      <th>Business Model</th>\n      <th>Corporate social responsibility ou CSR</th>\n      <th>marco-label</th>\n      <th>O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sustainability preoccupations</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>Digital transformation</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>Change in management</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Innovation activities</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>Business Model</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>Corporate social responsibility ou CSR</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>marco-label</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>O</th>\n      <td>0</td>\n      <td>0</td>\n      <td>198</td>\n      <td>23</td>\n      <td>139</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"time: 15.3 ms (started: 2022-07-27 12:28:08 +00:00)\n","output_type":"stream"}]},{"cell_type":"code","source":"def write_embeddings(path, embeddings, vocab):\n    \n    with open(path, 'w') as f:\n        for i, embedding in enumerate(tqdm(embeddings)):\n            word = vocab.get_itos()[i]\n            #skip words with unicode symbols\n            if len(word) != len(word.encode()):\n                continue\n            vector = ' '.join([str(i) for i in embedding.tolist()])\n            f.write(f'{word} {vector}\\n')\n\nwrite_embeddings('bilstm_trained_embeddings.txt', \n                 model.bilstm.word_embeds.weight.data, \n                 vocab)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T15:43:49.058631Z","iopub.execute_input":"2022-07-25T15:43:49.058982Z","iopub.status.idle":"2022-07-25T15:44:55.658363Z","shell.execute_reply.started":"2022-07-25T15:43:49.058945Z","shell.execute_reply":"2022-07-25T15:44:55.657330Z"},"trusted":true},"execution_count":82,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26746 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332e0244a8ec4dcb8e594fc32c594f33"}},"metadata":{}},{"name":"stdout","text":"time: 1min 6s (started: 2022-07-25 15:43:49 +00:00)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Previous Experiments","metadata":{}},{"cell_type":"code","source":"datasetdf = pd.read_csv(\"../input/annual-report/dataset_ap_07_18.csv\")\ndatasetdf = datasetdf.drop_duplicates(subset=['Company', 'Year', 'Text_para', 'Text_block', 'Catogory', 'Keyword'], keep='first', ignore_index=True)\ndatasetdf.drop(datasetdf.columns[datasetdf.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n\ndef remove_back_slash_n(text):\n    return text.replace('\\n', ' ')\ndatasetdf['Text_para'] = datasetdf['Text_para'].apply(remove_back_slash_n)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:39:15.623274Z","iopub.execute_input":"2022-07-19T11:39:15.623663Z","iopub.status.idle":"2022-07-19T11:39:17.413137Z","shell.execute_reply.started":"2022-07-19T11:39:15.623632Z","shell.execute_reply":"2022-07-19T11:39:17.412159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fixed window\nfwdf = pd.read_csv(\"../input/ftoken/dataset_07_07_ftoken.csv\")\nfwdf = fwdf.drop_duplicates(subset=['Company', 'Sector', 'Text_para', 'Nb_company', 'Text_block', 'Catogory', 'Keyword'], keep='first', ignore_index=True)\nfwdf.drop(fwdf.columns[fwdf.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\nprint(len(fwdf))\nfwdf = fwdf.dropna()\nprint(len(fwdf))\ndef remove_back_slash_n(text):\n    return text.replace('\\n', ' ')\nfwdf['Text_para'] = fwdf['Text_para'].apply(remove_back_slash_n)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:11:32.902865Z","iopub.execute_input":"2022-07-07T11:11:32.903432Z","iopub.status.idle":"2022-07-07T11:11:33.848695Z","shell.execute_reply.started":"2022-07-07T11:11:32.903396Z","shell.execute_reply":"2022-07-07T11:11:33.846832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_dict = {}\ntag_dict['Sustainability preoccupations'] = 'I-sus'\ntag_dict['Digital transformation'] = 'I-dig'\ntag_dict['Change in management'] = 'I-mag'\ntag_dict['Innovation activities'] = 'I-inn'\ntag_dict['Business Model'] = 'I-bus'\ntag_dict['Corporate social responsibility ou CSR'] = 'I-cor'\ntag_dict['marco-label'] = 'I-mar'\ntag2cat = {v: k for k, v in tag_dict.items()}\n\nlabels_to_ids2 = {'O':0, 'I-sus':1, 'I-dig':2, 'I-mag':3, 'I-inn':4, 'I-bus':5, 'I-cor':6, 'I-mar':7}\nids_to_labels2 = {v: k for k, v in labels_to_ids2.items()}\n\ngold_dict = build_gold_dict()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:19:21.487044Z","iopub.execute_input":"2022-07-28T15:19:21.488281Z","iopub.status.idle":"2022-07-28T15:19:21.496133Z","shell.execute_reply.started":"2022-07-28T15:19:21.488227Z","shell.execute_reply":"2022-07-28T15:19:21.494965Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def DatasetBuilder_tr(datasetdf):\n    TAGS2, TEXTS2 = [], []\n    for m in range(len(datasetdf)):\n        ts = datasetdf['Text_para'].iloc[m].split()\n#         print(datasetdf['Keyword'].iloc[m])\n        ks = datasetdf['Keyword'].iloc[m].split(',')\n        # deal with the special case\n        texts = []\n        for t in ts:\n            if t == '4.0' or t == 'R' or t == 'D' or t == '&':\n                texts.append(t)\n            else:\n                # text = re.findall(r\"([\\w']+(?:\\S-\\S)?[\\w'])+\", t)\n                text = re.findall(r\"([\\w']+[-]?[\\w']+)+\", t)\n                texts.extend(text)\n        # deal with the special case\n        keywords = []\n        for k in ks:\n            if k == 'Maintenance 4.0' or k == 'R & D' or k == 'R & D teams' or k == 'revolution 4.0':\n                keywords.append(k.split())\n            else:\n                keyword = re.findall(r\"([\\w']+[-]?[\\w']+)+\", k)\n                keywords.append(keyword)\n        tags = ['O'] * len(texts)\n        # cases: for each of the keyword in each block\n        # 1. only one keyword in the block, and no marco lable overlapping\n        # 2. more keyword in the block, and no marco label overlapping\n        # 3. only one keyword in the block, but there's marco label overlapping\n        # 4. more keyword in the block, and maroc label overlapping exists\n        for kw in keywords:\n            tt = tag_dict[gold_dict[' '.join(kw)][0]]\n            length_kw = len(kw)\n            item = kw[0]\n\n            if length_kw == 1:\n                start = 0\n                end = len(texts)\n                while True:\n                    try: \n                        index = texts.index(item, start, end)\n                        tags[index] = tt\n                        start = index+1\n                    except:\n                        break\n            \n            if length_kw == 2:\n                start = 0\n                end = len(texts)\n                while True:\n                    try: \n                        index = texts.index(item, start, end)\n                        start = index+1\n                        if texts[index+1] == kw[1]:\n                            if (tags[index] != 'O' and tags[index] != tt) or (tags[index+1] != 'O' and tags[index+1] != tt):\n                                tt = 'I-mar'\n                            for i in range(index, index+length_kw):\n                                tags[i] = tt\n                    except:\n                        break\n\n            if length_kw >= 3:\n                start = 0\n                end = len(texts)\n                while True:\n                    try: \n                        index = texts.index(item, start, end)\n                        start = index+1\n                        if texts[index+1] == kw[1] and texts[index+2] == kw[2]:\n                            if (tags[index] != 'O' and tags[index] != tt) or (tags[index+1] != 'O' and tags[index+1] != tt) or (tags[index+2] != 'O' and tags[index+2] != tt):\n                                tt = 'I-mar'\n                            for i in range(index, index+length_kw):\n                                tags[i] = tt\n                    except:\n                        break\n\n        TAGS2.append(tags)\n        TEXTS2.append(texts)\n\n    datasetdf['word_labels2'] = [','.join(bt) for bt in TAGS2]\n    datasetdf['sentence'] = [' '.join(tx) for tx in TEXTS2]\n\n    return datasetdf[[\"sentence\", \"word_labels2\"]].drop_duplicates().reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:38:02.754496Z","iopub.execute_input":"2022-07-19T11:38:02.754937Z","iopub.status.idle":"2022-07-19T11:38:02.785378Z","shell.execute_reply.started":"2022-07-19T11:38:02.754897Z","shell.execute_reply":"2022-07-19T11:38:02.784194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2 = DatasetBuilder_tr(datasetdf)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:39:22.679103Z","iopub.execute_input":"2022-07-19T11:39:22.679474Z","iopub.status.idle":"2022-07-19T11:39:38.239415Z","shell.execute_reply.started":"2022-07-19T11:39:22.679445Z","shell.execute_reply":"2022-07-19T11:39:38.238282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = DatasetBuilder_tr(fwdf)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:11:49.323014Z","iopub.execute_input":"2022-07-07T11:11:49.325854Z","iopub.status.idle":"2022-07-07T11:11:57.990639Z","shell.execute_reply.started":"2022-07-07T11:11:49.325814Z","shell.execute_reply":"2022-07-07T11:11:57.989485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset2(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n  def __getitem__(self, index):\n        # step 1: get the sentence and word labels \n        sentence = self.data.sentence[index].strip().split()  \n        word_labels = self.data.word_labels2[index].split(\",\") \n\n        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n        encoding = self.tokenizer(sentence,\n                             is_split_into_words=True, \n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        # step 3: create token labels only for first word pieces of each tokenized word\n        labels = [labels_to_ids2[label] for label in word_labels] \n        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n        # create an empty array of -100 of length max_length\n        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n        \n        # set only labels whose first offset position is 0 and the second is not 0\n        i = 0\n        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n          if mapping[0] == 0 and mapping[1] != 0:\n            # overwrite label\n            encoded_labels[idx] = labels[i]\n            i += 1\n\n        # step 4: turn everything into PyTorch tensors\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        item['labels'] = torch.as_tensor(encoded_labels)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:38:11.363278Z","iopub.execute_input":"2022-07-19T11:38:11.363996Z","iopub.status.idle":"2022-07-19T11:38:11.377631Z","shell.execute_reply.started":"2022-07-19T11:38:11.363953Z","shell.execute_reply":"2022-07-19T11:38:11.376662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the training function on the 80% of the dataset for tuning the bert model\ndef train2(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # put model in training mode\n    model2.train()\n    \n    for idx, batch in enumerate(training_loader2):\n        \n        ids = batch['input_ids'].to(device, dtype = torch.long)\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\n        labels = batch['labels'].to(device, dtype = torch.long)\n\n        results = model2(input_ids=ids, attention_mask=mask, labels=labels)\n        loss = results.loss\n        tr_logits = results.logits\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n#         if idx % 100==0:\n#             loss_step = tr_loss/nb_tr_steps\n#             print(f\"Training loss per 100 training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model2.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tr_labels.extend(labels)\n        tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model2.parameters(), max_norm=MAX_GRAD_NORM\n        )\n        \n        # backward pass\n        optimizer2.zero_grad()\n        loss.backward()\n        optimizer2.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")\n    return epoch_loss, tr_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:38:11.378792Z","iopub.execute_input":"2022-07-19T11:38:11.380141Z","iopub.status.idle":"2022-07-19T11:38:11.395414Z","shell.execute_reply.started":"2022-07-19T11:38:11.380111Z","shell.execute_reply":"2022-07-19T11:38:11.394022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels, eval_probability = [], [], []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(testing_loader):\n            \n            ids = batch['input_ids'].to(device, dtype = torch.long)\n            mask = batch['attention_mask'].to(device, dtype = torch.long)\n            labels = batch['labels'].to(device, dtype = torch.long)\n            \n            results = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = results.loss\n            eval_logits = results.logits\n\n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += labels.size(0)\n        \n#             if idx % 100==0:\n#                 loss_step = eval_loss/nb_eval_steps\n#                 print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n              \n            # compute evaluation accuracy\n            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n            \n            flattened_probability = F.softmax(active_logits, dim=1)\n            \n            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n            \n            # only compute accuracy at active labels\n            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        \n            labels = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            probability = []\n            for i, act in enumerate(active_accuracy):\n                if act:\n                    probability.append(flattened_probability[i])\n            \n            eval_labels.extend(labels)\n            eval_preds.extend(predictions)\n            eval_probability.extend(probability)\n            \n            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n            eval_accuracy += tmp_eval_accuracy\n\n#     labels = [ids_to_labels2[id.item()] for id in eval_labels]\n#     predictions = [ids_to_labels2[id.item()] for id in eval_preds]\n    labels = eval_labels\n    predictions = eval_preds\n    \n    eval_loss = eval_loss / nb_eval_steps\n    eval_accuracy = eval_accuracy / nb_eval_steps\n    print(f\"Validation Loss: {eval_loss}\")\n    print(f\"Validation Accuracy: {eval_accuracy}\")\n\n    return labels, predictions, eval_loss, eval_accuracy, eval_probability","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:38:11.397732Z","iopub.execute_input":"2022-07-19T11:38:11.398281Z","iopub.status.idle":"2022-07-19T11:38:11.414969Z","shell.execute_reply.started":"2022-07-19T11:38:11.398246Z","shell.execute_reply":"2022-07-19T11:38:11.413524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 4\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n\nprint(\"ANNUAL REPORT Dataset: {}\".format(data2.shape))\n\ndata_set2 = dataset2(data2, tokenizer, MAX_LEN)\n\ntrain_params2 = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ndata_loader2 = DataLoader(data_set2, **train_params2)\n\nmodel2 = BertForTokenClassification.from_pretrained(\"../input/bert-base-model\")\n# model2 = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(labels_to_ids2))\n# model2 = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(labels_to_ids2))\nmodel2.to(device)\n\noptimizer2 = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n\nlabels, predictions, eval_loss, eval_accuracy, eval_probabilty = valid(model2, data_loader2) ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:39:38.241342Z","iopub.execute_input":"2022-07-19T11:39:38.242052Z","iopub.status.idle":"2022-07-19T11:44:43.971538Z","shell.execute_reply.started":"2022-07-19T11:39:38.241986Z","shell.execute_reply":"2022-07-19T11:44:43.970579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lengthOfTokens(pair):\n        return len(pair.split())\n\ndef countKeywords(test_dataset, model):\n    kws_pairs = []\n    for tmp_num in range(len(test_dataset)):\n        sentence = test_dataset[\"sentence\"].iloc[tmp_num]\n\n        inputs = tokenizer(sentence.split(),\n                            is_split_into_words=True, \n                            return_offsets_mapping=True, \n                            padding='max_length', \n                            truncation=True, \n                            max_length=MAX_LEN,\n                            return_tensors=\"pt\")\n\n        # move to gpu\n        ids = inputs[\"input_ids\"].to(device)\n        mask = inputs[\"attention_mask\"].to(device)\n        # forward pass\n        outputs = model(ids, attention_mask=mask)\n        logits = outputs.logits\n\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n\n        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n        token_predictions = [ids_to_labels2[i] for i in flattened_predictions.cpu().numpy()]\n        wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n        prediction = []\n        for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n          #only predictions on first word pieces are important\n          if mapping[0] == 0 and mapping[1] != 0:\n            prediction.append(token_pred[1])\n          else:\n            continue\n\n        for i in zip(sentence.split(), prediction, test_dataset[\"word_labels2\"].iloc[tmp_num].split(',')):\n            kws_pairs.append(i)\n\n    predict_kws = []\n    tmp_wd = []\n    tmp_pred, tmp_gold = '', ''\n    for word, pred, gold in kws_pairs:\n        if pred != 'O':\n            if len(tmp_wd) == 0: \n                tmp_pred = pred\n            else:\n                if tmp_pred != pred:\n                    predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                    tmp_wd = []\n                    tmp_pred = pred\n            tmp_wd.append(word)\n        else:\n            if len(tmp_wd) > 0:\n                predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                tmp_wd = []\n                tmp_pred = pred\n\n    # check whether there are new keywords\n    gold_dict = build_gold_dict()\n    new_kws, kw_tags = [], []\n    gold_lists = list(gold_dict.keys())\n    gold_lists = [g.lower() for g in gold_lists]\n    for word, label in predict_kws:\n        if not word.lower() in gold_lists:\n            new_kws.append(word.lower())\n            kw_tags.append(tag2cat[label])\n    new_keywords = list(zip(new_kws, kw_tags))\n\n    new_kws_dict, new_kws_pos = {}, []\n    for kw, cat in new_keywords:\n        pos = pos_tag(kw.split())\n        if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n            key = ' '.join([p for p, t in pos])\n            tag = [t for p, t in pos]\n            new_kws_pos.append(key)\n            if not key in new_kws_dict.keys():\n                new_kws_dict[key] = (tag, cat)\n\n    new_kws = list(new_kws_dict.keys())\n    cats = [value[1] for value in new_kws_dict.values()]\n\n    df = pd.DataFrame({\"keyword\": new_kws, \"category\": cats})\n\n    s = df['keyword'].apply(lengthOfTokens)\n    s.sort_values()\n\n    predict_kws = Counter(predict_kws)\n    new_kws_pos = Counter(new_kws_pos)\n\n    predict_kws = sorted(predict_kws.items(), key=lambda pair: pair[1], reverse=True)\n    new_kws_pos = sorted(new_kws_pos.items(), key=lambda pair: pair[1], reverse=True)\n\n    return predict_kws, new_kws_pos, df","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:44:48.519790Z","iopub.execute_input":"2022-07-19T11:44:48.520150Z","iopub.status.idle":"2022-07-19T11:44:48.546009Z","shell.execute_reply.started":"2022-07-19T11:44:48.520121Z","shell.execute_reply":"2022-07-19T11:44:48.544747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_kws, new_kws_pos, new_kws_dict = countKeywords(data2, model2)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:48:02.924878Z","iopub.execute_input":"2022-07-19T11:48:02.925266Z","iopub.status.idle":"2022-07-19T11:55:07.533454Z","shell.execute_reply.started":"2022-07-19T11:48:02.925233Z","shell.execute_reply":"2022-07-19T11:55:07.532429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_index_positions(list_of_elems, element):\n    ''' Returns the indexes of all occurrences of give element in\n    the list- listOfElements '''\n    index_pos_list = []\n    index_pos = 0\n    while True:\n        try:\n            # Search for item in list from indexPos to the end of list\n            index_pos = list_of_elems.index(element, index_pos)\n            # Add the index position in list\n            index_pos_list.append(index_pos)\n            index_pos += 1\n        except ValueError as e:\n            break\n    return index_pos_list\n\ndef get_distribution(inputs, name):\n    true_res = {}\n    for k, v in tag2cat.items():\n        indexlist = get_index_positions(inputs, k)\n        left, right = 0, 0\n        new_labels = []\n        for i in indexlist:\n            if left == 0:\n                left = i\n                right = i\n                continue\n            if right+1 == i:\n                right = i\n            else:\n                new_labels.append((left, right))\n                left = i\n                right = i\n        true_res[v] = new_labels\n    print(f\"The distribution of {name} is:\")\n    for k,v in true_res.items():\n        print(f\"There are {len(v)} occurances in {k}\")\n    return true_res\n\ndef get_confusion_matrix(true_labels, true_predictions):\n    # strict mode\n    tp_dict = {}\n    for true_k, true_v in true_labels.items():\n        true_total = len(true_v)\n        tmp_dict = {}\n        tmp_total = 0\n        for pred_k, pred_v in true_predictions.items():\n            num = 0\n            for v in pred_v:\n                if v in true_v:\n                    num += 1\n            tmp_dict[pred_k] = num\n            tmp_total += num\n        tmp_dict[\"O\"] = true_total - tmp_total\n        tp_dict[true_k] = tmp_dict\n    tp_dict\n    tmp_dict = {}\n    for pred_k, pred_v in true_predictions.items():\n        true_total = len(pred_v)\n        num = 0\n        for tp_k, tp_v in tp_dict.items():\n            num += tp_v[pred_k]\n        tmp_dict[pred_k] = true_total - num\n    tmp_dict[\"O\"] = 0\n    tp_dict[\"O\"] = tmp_dict\n    return pd.DataFrame(tp_dict).T","metadata":{"execution":{"iopub.status.busy":"2022-07-19T11:57:03.896963Z","iopub.execute_input":"2022-07-19T11:57:03.897308Z","iopub.status.idle":"2022-07-19T11:57:03.911494Z","shell.execute_reply.started":"2022-07-19T11:57:03.897280Z","shell.execute_reply":"2022-07-19T11:57:03.910425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_distribution(labels, \"labels\")","metadata":{"execution":{"iopub.status.busy":"2022-07-19T12:14:04.690968Z","iopub.execute_input":"2022-07-19T12:14:04.691635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels = get_distribution(labels, \"labels\")\ntrue_predictions = get_distribution(predictions, \"predictions\")\ndff = get_confusion_matrix(true_labels, true_predictions)\ndff","metadata":{"execution":{"iopub.status.busy":"2022-07-18T16:25:13.712546Z","iopub.execute_input":"2022-07-18T16:25:13.712882Z","iopub.status.idle":"2022-07-18T17:05:24.649335Z","shell.execute_reply.started":"2022-07-18T16:25:13.712852Z","shell.execute_reply":"2022-07-18T17:05:24.648350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open(\"predicted_keywords_ap.pkl\", 'wb') as fh:\n   pickle.dump(predict_kws, fh)\n\nwith open(\"newly_keywords_ap.pkl\", 'wb') as fh:\n   pickle.dump(new_kws_pos, fh)\n\nnew_kws_dict.to_csv(\"new_keywords_ap_07_18.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 1\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\ntrain_size = 0.8\ntrain_dataset2 = data2.sample(frac=train_size,random_state=200)\ntest_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\ntrain_dataset2 = train_dataset2.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data2.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset2.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset2.shape))\n\ntraining_set2 = dataset2(train_dataset2, tokenizer, MAX_LEN)\ntesting_set2 = dataset2(test_dataset2, tokenizer, MAX_LEN)\n\ntrain_params2 = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params2 = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader2 = DataLoader(training_set2, **train_params2)\ntesting_loader2 = DataLoader(testing_set2, **test_params2)\n\nmodel2 = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids2))\nmodel2.to(device)\n\noptimizer2 = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n\ntrLosslist, trAcclist, evalLosslist, evalAcclist = [], [], [], []\n# label_pred_dict = {}\n# for epoch in range(EPOCHS):\n#     print(f\"Training epoch: {epoch + 1}\")\n#     tmp_dict = {}\n#     epoch_loss, tr_accuracy = train2(epoch)\n#     labels, predictions, eval_loss, eval_accuracy, eval_probability = valid(model2, testing_loader2)\n#     trLosslist.append(epoch_loss)\n#     trAcclist.append(tr_accuracy)\n#     evalLosslist.append(eval_loss)\n#     evalAcclist.append(eval_accuracy)\n#     tmp_dict[\"labels\"] = labels\n#     tmp_dict[\"predictions\"] = predictions\n#     tmp_dict[\"probability\"] = eval_probability\n#     label_pred_dict[epoch] = tmp_dict\n\n# resultdf = pd.DataFrame({\"Epoch\": list(range(1, EPOCHS+1)),\n#                          \"Train_loss\": trLosslist,\n#                          \"Eval_loss\": evalLosslist,\n#                          \"Train_Acc\": trAcclist,\n#                          \"Eval_Acc\": evalAcclist})\n# resultdf    ","metadata":{"execution":{"iopub.status.busy":"2022-07-13T14:56:59.164743Z","iopub.execute_input":"2022-07-13T14:56:59.165657Z","iopub.status.idle":"2022-07-13T14:57:24.135742Z","shell.execute_reply.started":"2022-07-13T14:56:59.165612Z","shell.execute_reply":"2022-07-13T14:57:24.134190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install pthflops\n# from pthflops import count_ops\n\nmodel2.train()\nids = batch['input_ids'].to(device, dtype = torch.long)\nmask = batch['attention_mask'].to(device, dtype = torch.long)\nlabels = batch['labels'].to(device, dtype = torch.long)\ncount_ops(model2, ids)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:34:32.527866Z","iopub.execute_input":"2022-07-13T15:34:32.528898Z","iopub.status.idle":"2022-07-13T15:34:32.612482Z","shell.execute_reply.started":"2022-07-13T15:34:32.528831Z","shell.execute_reply":"2022-07-13T15:34:32.610351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install thop-0.0.31.post2005241907-py3-none-any.whl\nfrom thop import profile\nids = batch['input_ids'].to(device, dtype = torch.long)\nmask = batch['attention_mask'].to(device, dtype = torch.long)\nlabels = batch['labels'].to(device, dtype = torch.long)\nmacs, params = profile(model2, inputs=(ids, mask), verbose=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:25:07.603817Z","iopub.execute_input":"2022-07-13T15:25:07.604188Z","iopub.status.idle":"2022-07-13T15:25:07.664890Z","shell.execute_reply.started":"2022-07-13T15:25:07.604158Z","shell.execute_reply":"2022-07-13T15:25:07.662825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, predictions, eval_loss, eval_accuracy, eval_probability = valid(model2, testing_loader2)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:34:57.200625Z","iopub.execute_input":"2022-07-13T12:34:57.200979Z","iopub.status.idle":"2022-07-13T12:35:38.805839Z","shell.execute_reply.started":"2022-07-13T12:34:57.200950Z","shell.execute_reply":"2022-07-13T12:35:38.804845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(labels), len(predictions), len(eval_probability)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:35:38.807629Z","iopub.execute_input":"2022-07-13T12:35:38.808818Z","iopub.status.idle":"2022-07-13T12:35:38.818791Z","shell.execute_reply.started":"2022-07-13T12:35:38.808778Z","shell.execute_reply":"2022-07-13T12:35:38.817607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels1 = [i.cpu().detach().numpy() for i in labels]\npredictions1 = [i.cpu().detach().numpy() for i in predictions]\neval_probability1 = [i.cpu().detach().numpy() for i in eval_probability]","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:37:25.976089Z","iopub.execute_input":"2022-07-13T12:37:25.977139Z","iopub.status.idle":"2022-07-13T12:37:50.565691Z","shell.execute_reply.started":"2022-07-13T12:37:25.977085Z","shell.execute_reply":"2022-07-13T12:37:50.563798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score\n\ndef autodl_auc(outputs, targets, predictions):\n    \n    targets = np.asarray(targets)\n    predictions = np.asarray(predictions)\n    \n    numclass = targets.max()+1\n    \n    boolean_array = np.zeros((len(outputs),numclass), dtype=bool)\n    \n    for labelindex in range(numclass):\n        boolean_array[:,labelindex]= (targets == labelindex)\n    \n    \n    auc = roc_auc_score(boolean_array, outputs)\n    auc_1 = 2*auc-1\n    \n    \n    return round(auc, 2), round(auc_1, 2)\n\nautodl_auc(eval_probability1, labels1, predictions1)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:38:14.268539Z","iopub.execute_input":"2022-07-13T12:38:14.269030Z","iopub.status.idle":"2022-07-13T12:38:16.436834Z","shell.execute_reply.started":"2022-07-13T12:38:14.268984Z","shell.execute_reply":"2022-07-13T12:38:16.435752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curves(valid_outputs, valid_targets, valid_predictions):\n    \n    print(\"--------------------------------------------\")\n    print(\"ROC Curves\")\n    print(\"--------------------------------------------\")\n \n    valid_targets = np.asarray(valid_targets)\n    valid_predictions = np.asarray(valid_predictions)\n    valid_outputs = np.asarray(valid_outputs)\n\n    numclass = valid_predictions.max()+1 \n    \n    valid_auc_curves = []\n\n    for labelindex in range(numclass):\n        valid_binary_targets = (valid_targets == labelindex)\n        valid_binary_predictions = (valid_predictions == labelindex)\n       \n        selected_valid_outputs = valid_outputs[:,labelindex]  \n        valid_fpr, valid_tpr, _ = roc_curve(valid_binary_targets, selected_valid_outputs)\n        valid_auc_curves.append([valid_fpr,valid_tpr])  \n    \n     # Plot ROC Curves\n\n    fig_height = 8 if numclass <= 5 else 16\n    fig_width =  16 if numclass <= 5 else 30\n    fig = plt.figure(figsize=(fig_width,fig_height))\n    \n    for idx, auc_cur in enumerate(valid_auc_curves): \n        plt.plot(auc_cur[0], auc_cur[1], marker='.',  label='Class:'+str(idx))\n    plt.plot([0,1], [0,1], linestyle='--', color='black')\n    plt.title('Valid ROC Curves - ', fontsize=20)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.legend()\n    \n    plt.pause(0.1)\n    \nplot_roc_curves(eval_probability1, labels1, predictions1)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:54:32.512281Z","iopub.execute_input":"2022-07-13T12:54:32.512655Z","iopub.status.idle":"2022-07-13T12:54:34.400807Z","shell.execute_reply.started":"2022-07-13T12:54:32.512624Z","shell.execute_reply":"2022-07-13T12:54:34.399802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = 0.8\ntrain_dataset3 = data3.sample(frac=train_size,random_state=200)\ntest_dataset3 = data3.drop(train_dataset3.index).reset_index(drop=True)\ntrain_dataset3 = train_dataset3.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data3.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset3.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset3.shape))\n\ntraining_set3 = dataset2(train_dataset3, tokenizer, MAX_LEN)\ntesting_set3 = dataset2(test_dataset3, tokenizer, MAX_LEN)\n\ntrain_params3 = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params3 = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader3 = DataLoader(training_set3, **train_params3)\ntesting_loader3 = DataLoader(testing_set3, **test_params2)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:12:42.570547Z","iopub.execute_input":"2022-07-07T11:12:42.570915Z","iopub.status.idle":"2022-07-07T11:12:42.633584Z","shell.execute_reply.started":"2022-07-07T11:12:42.570882Z","shell.execute_reply":"2022-07-07T11:12:42.631311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 4\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\ntrain_size = 0.8\ntrain_dataset2 = data2.sample(frac=train_size,random_state=200)\ntest_dataset2 = data2.drop(train_dataset2.index).reset_index(drop=True)\ntrain_dataset2 = train_dataset2.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data2.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset2.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset2.shape))\n\ntraining_set2 = dataset2(train_dataset2, tokenizer, MAX_LEN)\ntesting_set2 = dataset2(test_dataset2, tokenizer, MAX_LEN)\n\ntrain_params2 = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params2 = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader2 = DataLoader(training_set2, **train_params2)\ntesting_loader2 = DataLoader(testing_set2, **test_params2)\n\nmodel2 = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids2))\nmodel2.to(device)\n\noptimizer2 = torch.optim.Adam(params=model2.parameters(), lr=LEARNING_RATE)\n\nepoch_loss_list, tr_accuracy_list, eval_loss_list, eval_accuracy_list = [], [], [], []\nlabels_list, predictions_list = [],[]\nfor epoch in range(EPOCHS):\n    print(f\"Training epoch: {epoch + 1}\")\n    epoch_loss, tr_accuracy = train2(epoch)\n    labels, predictions, eval_loss, eval_accuracy = valid(model2, testing_loader2)\n    epoch_loss_list.append(epoch_loss) \n    tr_accuracy_list.append(tr_accuracy) \n    eval_loss_list.append(eval_loss) \n    eval_accuracy_list.append(eval_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:12:49.02939Z","iopub.execute_input":"2022-07-07T11:12:49.029786Z","iopub.status.idle":"2022-07-07T11:38:29.386137Z","shell.execute_reply.started":"2022-07-07T11:12:49.029751Z","shell.execute_reply":"2022-07-07T11:38:29.384741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_results = pd.DataFrame({'Epoch': list(range(1, EPOCHS+1)) * 2,\n                            'Loss': epoch_loss_list + eval_loss_list,\n                            'Label': ['Training'] * EPOCHS + ['Validation'] * EPOCHS})\n\nacc_results = pd.DataFrame({'Epoch': list(range(1, EPOCHS+1)) * 2,\n                            'Accuracy': tr_accuracy_list + eval_accuracy_list,\n                            'Label': ['Training'] * EPOCHS + ['Validation'] * EPOCHS})\n\n\nsns.set_theme(style='darkgrid', palette='deep', font='sans-serif', font_scale=1.3, rc={'figure.figsize':(20.7,8.27)})\nfig, axs = plt.subplots(1, 2)\nsns.lineplot(data=loss_results, x=\"Epoch\", y='Loss', hue=\"Label\", ax=axs[0])\nplt.legend(labels=[\"Training loss\", \"Validation loss\"])\nsns.lineplot(data=acc_results, x=\"Epoch\", y='Accuracy', hue=\"Label\", ax=axs[1])\nplt.legend(labels=[\"Training accuracy\", \"Validation accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:52:14.676967Z","iopub.execute_input":"2022-07-07T09:52:14.67744Z","iopub.status.idle":"2022-07-07T09:52:15.507227Z","shell.execute_reply.started":"2022-07-07T09:52:14.677405Z","shell.execute_reply":"2022-07-07T09:52:15.505854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(\n    fpr[\"micro\"],\n    tpr[\"micro\"],\n    label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n    color=\"deeppink\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\nplt.plot(\n    fpr[\"macro\"],\n    tpr[\"macro\"],\n    label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n    color=\"navy\",\n    linestyle=\":\",\n    linewidth=4,\n)\n\ncolors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(\n        fpr[i],\n        tpr[i],\n        color=color,\n        lw=lw,\n        label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]),\n    )\n\nplt.plot([0, 1], [0, 1], \"k--\", lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Some extension of Receiver operating characteristic to multiclass\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\nSome extension of Receiver operating characteristic to multiclass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval\nfrom seqeval.metrics import classification_report\n\nlabels = [labels]\npredictions = [predictions]\nprint(classification_report(labels, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:12:11.010843Z","iopub.execute_input":"2022-07-07T11:12:11.011235Z","iopub.status.idle":"2022-07-07T11:12:25.515017Z","shell.execute_reply.started":"2022-07-07T11:12:11.011205Z","shell.execute_reply":"2022-07-07T11:12:25.513255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, predictions, eval_loss, eval_accuracy = valid(model2, testing_loader3)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:46.654085Z","iopub.execute_input":"2022-07-07T11:48:46.654545Z","iopub.status.idle":"2022-07-07T11:48:46.716153Z","shell.execute_reply.started":"2022-07-07T11:48:46.654495Z","shell.execute_reply":"2022-07-07T11:48:46.713818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval\nfrom seqeval.metrics import classification_report\n\nlabels = [labels]\npredictions = [predictions]\nprint(classification_report(labels, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:47.080601Z","iopub.execute_input":"2022-07-07T11:48:47.081419Z","iopub.status.idle":"2022-07-07T11:49:04.415085Z","shell.execute_reply.started":"2022-07-07T11:48:47.081377Z","shell.execute_reply":"2022-07-07T11:49:04.413872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.save_pretrained(\"./ftoken_model_epoch4\")","metadata":{"execution":{"iopub.status.busy":"2022-07-07T10:22:36.090361Z","iopub.execute_input":"2022-07-07T10:22:36.090838Z","iopub.status.idle":"2022-07-07T10:22:37.177165Z","shell.execute_reply.started":"2022-07-07T10:22:36.090806Z","shell.execute_reply":"2022-07-07T10:22:37.175466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lengthOfTokens(pair):\n        return len(pair.split())\n\ndef countKeywords(test_dataset, model):\n    kws_pairs = []\n    for tmp_num in range(len(test_dataset)):\n        sentence = test_dataset[\"sentence\"].iloc[tmp_num]\n\n        inputs = tokenizer(sentence.split(),\n                            is_split_into_words=True, \n                            return_offsets_mapping=True, \n                            padding='max_length', \n                            truncation=True, \n                            max_length=MAX_LEN,\n                            return_tensors=\"pt\")\n\n        # move to gpu\n        ids = inputs[\"input_ids\"].to(device)\n        mask = inputs[\"attention_mask\"].to(device)\n        # forward pass\n        outputs = model(ids, attention_mask=mask)\n        logits = outputs.logits\n\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n\n        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n        token_predictions = [ids_to_labels2[i] for i in flattened_predictions.cpu().numpy()]\n        wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n        prediction = []\n        for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n          #only predictions on first word pieces are important\n          if mapping[0] == 0 and mapping[1] != 0:\n            prediction.append(token_pred[1])\n          else:\n            continue\n\n        for i in zip(sentence.split(), prediction, test_dataset[\"word_labels2\"].iloc[tmp_num].split(',')):\n            kws_pairs.append(i)\n\n    predict_kws = []\n    tmp_wd = []\n    tmp_pred, tmp_gold = '', ''\n    for word, pred, gold in kws_pairs:\n        if pred != 'O':\n            if len(tmp_wd) == 0: \n                tmp_pred = pred\n            else:\n                if tmp_pred != pred:\n                    predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                    tmp_wd = []\n                    tmp_pred = pred\n            tmp_wd.append(word)\n        else:\n            if len(tmp_wd) > 0:\n                predict_kws.append((' '.join(tmp_wd), tmp_pred))\n                tmp_wd = []\n                tmp_pred = pred\n\n    # check whether there are new keywords\n    gold_dict = build_gold_dict()\n    new_kws, kw_tags = [], []\n    gold_lists = list(gold_dict.keys())\n    gold_lists = [g.lower() for g in gold_lists]\n    for word, label in predict_kws:\n        if not word.lower() in gold_lists:\n            new_kws.append(word.lower())\n            kw_tags.append(tag2cat[label])\n    new_keywords = list(zip(new_kws, kw_tags))\n\n    new_kws_dict, new_kws_pos = {}, []\n    for kw, cat in new_keywords:\n        pos = pos_tag(kw.split())\n        if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n            key = ' '.join([p for p, t in pos])\n            tag = [t for p, t in pos]\n            new_kws_pos.append(key)\n            if not key in new_kws_dict.keys():\n                new_kws_dict[key] = (tag, cat)\n\n    new_kws = list(new_kws_dict.keys())\n    cats = [value[1] for value in new_kws_dict.values()]\n\n    df = pd.DataFrame({\"keyword\": new_kws, \"category\": cats})\n\n#     s = df['keyword'].apply(lengthOfTokens)\n#     df.sort_values(by=['keyword'], key=lambda x: lengthOfTokens(x))\n\n    predict_kws = Counter(predict_kws)\n    new_kws_pos = Counter(new_kws_pos)\n\n    predict_kws = sorted(predict_kws.items(), key=lambda pair: pair[1], reverse=True)\n    new_kws_pos = sorted(new_kws_pos.items(), key=lambda pair: pair[1], reverse=True)\n\n    return predict_kws, new_kws_pos, df","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:50:20.430747Z","iopub.execute_input":"2022-07-07T11:50:20.43134Z","iopub.status.idle":"2022-07-07T11:50:20.470378Z","shell.execute_reply.started":"2022-07-07T11:50:20.431302Z","shell.execute_reply":"2022-07-07T11:50:20.469089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_kws, new_kws_pos, new_kws_dict = countKeywords(test_dataset2, model2)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T10:12:55.787561Z","iopub.execute_input":"2022-07-07T10:12:55.788002Z","iopub.status.idle":"2022-07-07T10:14:06.244152Z","shell.execute_reply.started":"2022-07-07T10:12:55.787969Z","shell.execute_reply":"2022-07-07T10:14:06.242799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_kws_dict","metadata":{"execution":{"iopub.status.busy":"2022-07-07T10:14:06.247643Z","iopub.execute_input":"2022-07-07T10:14:06.248519Z","iopub.status.idle":"2022-07-07T10:14:06.269857Z","shell.execute_reply.started":"2022-07-07T10:14:06.248449Z","shell.execute_reply":"2022-07-07T10:14:06.268532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"predicted_keywords_ftoken_upgrade1.pkl\", 'wb') as fh:\n   pickle.dump(predict_kws, fh)\n\nwith open(\"newly_keywords_ftoken_upgrade1.pkl\", 'wb') as fh:\n   pickle.dump(new_kws_pos, fh)\n\nnew_kws_dict.to_csv(\"new_keywords_ftoken_07_07.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-07-07T10:14:06.271505Z","iopub.execute_input":"2022-07-07T10:14:06.272269Z","iopub.status.idle":"2022-07-07T10:14:06.286929Z","shell.execute_reply.started":"2022-07-07T10:14:06.272209Z","shell.execute_reply":"2022-07-07T10:14:06.285209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM code checking","metadata":{}},{"cell_type":"code","source":"import ast\nimport pandas as pd\nimport torch\nimport nltk\nfrom nltk import pos_tag\nimport numpy as np\nnltk.download('averaged_perceptron_tagger')\nfrom collections import Counter\n\ntag_dict = {}\ntag_dict['Sustainability preoccupations'] = 'I-sus'\ntag_dict['Digital transformation'] = 'I-dig'\ntag_dict['Change in management'] = 'I-mag'\ntag_dict['Innovation activities'] = 'I-inn'\ntag_dict['Business Model'] = 'I-bus'\ntag_dict['Corporate social responsibility ou CSR'] = 'I-cor'\ntag_dict['marco-label'] = 'I-mar'\ntag2cat = {v: k for k, v in tag_dict.items()}\n\nlabels_to_ids2 = {'O':0, 'I-sus':1, 'I-dig':2, 'I-mag':3, 'I-inn':4, 'I-bus':5, 'I-cor':6, 'I-mar':7}\nids_to_labels2 = {v: k for k, v in labels_to_ids2.items()}\n\ndef build_gold_dict():\n    # read from excel\n    read_file = pd.read_excel('../input/gold-dict/Terms malantin 1er juin 2022.xlsx',\n                              sheet_name='categories 1 juin 2022')\n    read_file.dropna(0, how='all', inplace=True)\n    read_file.dropna(1, how='all', inplace=True)\n\n    gold_dict = {}\n    for i in range(1, len(read_file)):\n        if read_file.iloc[i]['Main form'] is None:\n            continue\n        gold_dict[read_file.iloc[i]['Main form'].strip()] = []\n\n    for i in range(1, len(read_file)):\n        index = read_file.iloc[i]['Main form'].strip()\n        if index is None:\n            continue\n        if not read_file.iloc[i].isna()['Sustainability preoccupations']:\n            gold_dict[index].append('Sustainability preoccupations')\n        if not read_file.iloc[i].isna()['Digital transformation']:\n            gold_dict[index].append('Digital transformation')\n        if not read_file.iloc[i].isna()['Change in management']:\n            gold_dict[index].append('Change in management')\n        if not read_file.iloc[i].isna()['Innovation activities']:\n            gold_dict[index].append('Innovation activities')\n        if not read_file.iloc[i].isna()['Business Model']:\n            gold_dict[index].append('Business Model')\n        if not read_file.iloc[i].isna()['Corporate social responsibility ou CSR']:\n            gold_dict[index].append('Corporate social responsibility ou CSR') \n\n    # Change the category for four keywords\n    # academic institutions, university & research institutions, service among university, \n    # worldwide research centres\n    category = 'Innovation activities'\n    changelist = ['academic institutions', 'worldwide research centers', 'university and research institutions', 'customer service among university']\n    for c in changelist:\n        gold_dict[c][0] = category\n\n    # Deal with the singular and plural cases in keywords\n    cortext3 = open(\"../input/cortext/Cortext3_min_delac_flex_utf8.txt\", \"r\")\n    lines = cortext3.readlines()\n    lefts, rights = [], []\n    for line in lines:\n        left, right = line.split(',')\n        if left != right.split('.')[0]:\n            lefts.append(left)\n            rights.append(right.split('.')[0])\n\n    tmp_gold_dict = gold_dict.copy()\n    for key, value in tmp_gold_dict.items():\n        if key in lefts:\n            index = lefts.index(key)\n            right = rights[index]\n            gold_dict[right] = gold_dict[key]\n        if key in rights:\n            indices = [i for i, word in enumerate(rights) if word == key]\n            for index in indices:\n                left = lefts[index]\n                gold_dict[left] = gold_dict[key]\n\n    tmp_dict = {}\n    for k in gold_dict.keys():\n        tmp_dict[k] = []\n    for k, v in gold_dict.items():\n        tmp_dict[k] = list(set(v))\n    gold_dict = tmp_dict\n\n    return gold_dict\n\ndef tags_to_keywords(sample, words):\n    indices = [i for i, l in enumerate(sample) if l != 'O']\n    keywords, key_cats = [], []\n    for j, id in enumerate(indices):\n        if j == 0:\n            start = end = id\n            continue\n        if j == len(indices):\n            pos = pos_tag(words[start:end+1])\n            if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n                keywords.append(' '.join(words[start:end+1]))\n                key_cats.append((' '.join(words[start:end+1]), sample[start:end+1]))\n            continue\n        if end+1 == id:\n            end = id\n        else:\n            try: # the index is out of the words\n                pos = pos_tag(words[start:end+1])\n                if (pos[-1][1] == 'NN' or pos[-1][1] == 'NNS' or pos[-1][1] == 'NNP' or pos[-1][1] == 'NNPS') and pos[0][1] != 'CC':\n                    keywords.append(' '.join(words[start:end+1]))\n                    key_cats.append((' '.join(words[start:end+1]), sample[start:end+1]))\n                start = end = id\n            except:\n                continue\n    return list(set(keywords)), key_cats\n\ndef get_index_positions(list_of_elems, element):\n    ''' Returns the indexes of all occurrences of give element in\n    the list- listOfElements '''\n    index_pos_list = []\n    index_pos = 0\n    while True:\n        try:\n            # Search for item in list from indexPos to the end of list\n            index_pos = list_of_elems.index(element, index_pos)\n            # Add the index position in list\n            index_pos_list.append(index_pos)\n            index_pos += 1\n        except ValueError as e:\n            break\n    return index_pos_list\n\ndef get_distribution(inputs, name):\n    true_res = {}\n    for k, v in tag2cat.items():\n        indexlist = get_index_positions(inputs, k)\n        left, right = 0, 0\n        new_labels = []\n        for i in indexlist:\n            if left == 0:\n                left = i\n                right = i\n                continue\n            if right+1 == i:\n                right = i\n            else:\n                new_labels.append((left, right))\n                left = i\n                right = i\n        true_res[v] = new_labels\n    print(f\"The distribution of {name} is:\")\n    for k,v in true_res.items():\n        print(f\"There are {len(v)} occurances in {k}\")\n    return true_res\n\ndef get_confusion_matrix(true_labels, true_predictions):\n    # strict mode\n    tp_dict = {}\n    for true_k, true_v in true_labels.items():\n        true_total = len(true_v)\n        tmp_dict = {}\n        tmp_total = 0\n        for pred_k, pred_v in true_predictions.items():\n            num = 0\n            for v in pred_v:\n                if v in true_v:\n                    num += 1\n            tmp_dict[pred_k] = num\n            tmp_total += num\n        tmp_dict[\"O\"] = true_total - tmp_total\n        tp_dict[true_k] = tmp_dict\n    tp_dict\n    tmp_dict = {}\n    for pred_k, pred_v in true_predictions.items():\n        true_total = len(pred_v)\n        num = 0\n        for tp_k, tp_v in tp_dict.items():\n            num += tp_v[pred_k]\n        tmp_dict[pred_k] = true_total - num\n    tmp_dict[\"O\"] = 0\n    tp_dict[\"O\"] = tmp_dict\n    return pd.DataFrame(tp_dict).T\n\ndef intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\ndef get_recall(Labels, Predictions, ignore=True):\n    recalls = []\n    for i, lbs in enumerate(Labels):\n        preds = Predictions[i]\n        if not lbs:\n            if not ignore:\n                recalls.append(1.0 if not preds else 0.0)\n            continue\n        recalls.append(np.nan_to_num(len(intersection(preds, lbs)) / len(lbs)))\n    return np.mean(recalls)\n\ndef get_precision(Labels, Predictions, ignore=True):\n    precisions = []\n    for i, preds in enumerate(Predictions):\n        lbs = Labels[i]\n        if not preds:\n            if not ignore:\n                precisions.append(1.0 if not preds else 0.0)\n            continue\n        precisions.append(np.nan_to_num(len(intersection(preds, lbs)) / len(preds)))\n    return np.mean(precisions)\n\ndef get_f1score(Labels, Predictions, ignore=True):\n    precision = get_precision(Labels, Predictions, ignore)\n    recall = get_recall(Labels, Predictions, ignore)\n    return np.nan_to_num(2 * precision * recall / (precision + recall))\n\ndef lengthOfTokens(pair):\n    return len(pair.split())\n\ndef tokenize_example(example, max_length=300):\n    return example.split(' ')[:max_length]\n\ndef split_tags(example, max_length=300):\n    return example.split(',')[:max_length]\n\ndef argmax(x):\n    # return the argmax as a python int\n    _, idx = torch.max(x, 1)\n    return idx.item()\n\ndef log_sum_exp(x):\n    #return log sum exp of a tensor\n    m = torch.max(x, -1)[0]\n    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(-1)), -1))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T09:27:50.042521Z","iopub.execute_input":"2022-09-08T09:27:50.043066Z","iopub.status.idle":"2022-09-08T09:27:52.238063Z","shell.execute_reply.started":"2022-09-08T09:27:50.042940Z","shell.execute_reply":"2022-09-08T09:27:52.237039Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]}]}